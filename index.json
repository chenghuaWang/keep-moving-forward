[{"content":"在编写算子和框架的时候，经常需要分析程序的性能瓶颈。我们需要一个合适的指标和指导方法，Roofline model给出了一个简洁的定量分析方法。Roofline模型引入了一种基于Operational Intensity的定量分析方法，它定义了在计算平台上可实现的理论最大计算效率。该模型还提供了一个公式，用以计算在特定计算环境中可能达到的最高理论性能。\n我们先来看下roofline model的直观表示：\nRoofline ModelRoofline: An Insightful Visual Performance Model for Floating-Point Programs and Multicore Architectures\n我们先来看下图中指标的定义：\n计算强度 $I$ : 每Byte内存在交换后用于进行了多少浮点运算，即FLOPs/Bytes。模型的计算强度$I$越大，对于内存带宽的压力越小，内存的使用效率越高。\n算力的大小决定了屋顶的高度，带宽决定了斜率。\n在达到屋顶的转折点之前都是Memory Bound，在之后是Compute Bound。在Compute Bound的区域，不管计算强度$I$有多大，它的计算性能都由机器的实际最大计算性能所限制。\nNote:\nroofline model讲的是程序理论上可以达到的最好性能，而不是实际达到的性能。如cache大小的限制，网络限制，未必能达到roofline模型定义的边界。 ","permalink":"https://chenghuawang.github.io/keep-moving-forward/tech/roofline_model/","summary":"在编写算子和框架的时候，经常需要分析程序的性能瓶颈。我们需要一个合适的指标和指导方法，Roofline model给出了一个简洁的定量分析方法。Roofline模型引入了一种基于Operational Intensity的定量分析方法，它定义了在计算平台上可实现的理论最大计算效率。该模型还提供了一个公式，用以计算在特定计算环境中可能达到的最高理论性能。\n我们先来看下roofline model的直观表示：\nRoofline ModelRoofline: An Insightful Visual Performance Model for Floating-Point Programs and Multicore Architectures\n我们先来看下图中指标的定义：\n计算强度 $I$ : 每Byte内存在交换后用于进行了多少浮点运算，即FLOPs/Bytes。模型的计算强度$I$越大，对于内存带宽的压力越小，内存的使用效率越高。\n算力的大小决定了屋顶的高度，带宽决定了斜率。\n在达到屋顶的转折点之前都是Memory Bound，在之后是Compute Bound。在Compute Bound的区域，不管计算强度$I$有多大，它的计算性能都由机器的实际最大计算性能所限制。\nNote:\nroofline model讲的是程序理论上可以达到的最好性能，而不是实际达到的性能。如cache大小的限制，网络限制，未必能达到roofline模型定义的边界。 ","title":"Roofline Model"},{"content":"0x01 前言 XNNPACK是一个由Google维护的算子库，在TensorFlowLite，ExecuTorch，ONNX RT等众多知名框架中使用。笔者最近在做mllm的xnnpack后端适配工作，因xnnpack缺少文档，在此记录。\nxnnpack的test中展示了大部分xnnpack的API和使用方式，读者在碰到API使用问题的时候不妨去test文件夹下面找找答案；xnnpack遵循标准的doxygen注释，也较好的说明了函数和class的使用方法。\n在xnn中使用的是静态图构建的方法，在开始构建静态图之前，需要初始化xnn：\nxnn_initialize(nullptr /* allocator */) 使用如下API可以构建出一张subgraph，接下来的所有操作都在更改这张子图，\nxnn_subgraph_t subgraph_ = nullptr; auto status = xnn_create_subgraph(external_nums, 0, \u0026amp;subgraph_); 0x02 定义 Tensor 定义未经过量化的Tensor使用的API是：\nuint32_t uuid; status = xnn_define_tensor_value( /*subgraph*/..., /*dtype*/..., dims.size(), dims.data(), /*data=*/..., /*external id*/XNN_INVALID_VALUE_ID, /*flag*/0, /*id*/\u0026amp;uuid); 这里需要特殊解释的是external_id、flag和uuid三个值：\nexternal_id 是对于EXternal Inputs和Outputs才需要设置的，对于xnnpack内部管理的Tensor，不需要设置这个值，给出默认的XNN_INVALID_VALUE_ID就行。external_id的作用是让xnnpack可以从runtime中传入的external_values中索引到需要的Tensor值。详细解释如下： 在xnnpack中，每个Tensor都会有一个uuid，对于xnnpack自己管理的Tensor，uuid在定义的时候会由xnnpack自己生成。还记得在xnn_create_subgraph创建的时候需要传入external_nums吗？这里的external_nums就是用户侧预留的uuid。比如external_nums是3的时候，xnn_define_tensor_value就会从4开始计数给新创建的Tensor。而前3个Tensor，即external Tensor的uuid(external_id)就是1，2，3。\nflag flag是用来标识这个Tensor是不是External Inputs，Outputs或者是其他类型的Tensor。比如inputs tensor的flag是flags = XNN_VALUE_FLAG_EXTERNAL_INPUT;, outputs 是 flags = XNN_VALUE_FLAG_EXTERNAL_OUTPUT;\nuuid 是每个Tensor的全局索引标识\n题外话：\n在mllm中，Tensor的define过程如下：\nvoid defineXpTensor(XnnpackBackend *xpb, Tensor *t, XpTensorType ttype) { if (t-\u0026gt;uuid() != XNN_INVALID_VALUE_ID) return; auto xp_dtype = XnnpackBackend::mllmDType2XnnDType(t-\u0026gt;dtype()); xnn_status status; std::vector\u0026lt;size_t\u0026gt; dims; for (auto d : t-\u0026gt;shape()) dims.push_back(d); uint32_t flags; uint32_t external_id = XNN_INVALID_VALUE_ID; switch (ttype) { case XpTensorType::Normal: flags = XNN_INVALID_VALUE_ID; break; case XpTensorType::ExternalInput: flags = XNN_VALUE_FLAG_EXTERNAL_INPUT; external_id = xpb-\u0026gt;getNewEXternalId(); break; case XpTensorType::ExternalOutput: flags = XNN_VALUE_FLAG_EXTERNAL_OUTPUT; external_id = xpb-\u0026gt;getNewEXternalId(); break; } switch (xp_dtype) { case xnn_datatype_fp32: { status = xnn_define_tensor_value( xpb-\u0026gt;getXnnSubgraph(), xp_dtype, dims.size(), dims.data(), /*data=*/nullptr, external_id, flags, \u0026amp;t-\u0026gt;uuid()); } default: break; } switch (ttype) { case XpTensorType::Normal: break; case XpTensorType::ExternalInput: case XpTensorType::ExternalOutput: xpb-\u0026gt;registerExternalValue(t-\u0026gt;uuid(), xnn_external_value{.id = t-\u0026gt;uuid(), .data = t-\u0026gt;rawHostPtr()}); xpb-\u0026gt;registerUuidTensor(t-\u0026gt;uuid(), t); break; } if (status != xnn_status_success) { Log::error(\u0026#34;xnnpack backend defineXpTensor Error\u0026#34;); exit(-1); } } 0x03 定义Op 这里笔者以Fully Connect Op为例，因为这个算子包含了Weight，可以更好的展示Weight Tensor是怎么定义的。\nweight的定义不同于0x02中所述，因为weight有固定的数据，在mllm中是这样调用的：\nstatus = xnn_define_tensor_value( xpb-\u0026gt;getXnnSubgraph(), xp_dtype, dims.size(), dims.data(), /*data=*/t-\u0026gt;rawHostPtr(), XNN_INVALID_VALUE_ID, 0, \u0026amp;t-\u0026gt;uuid()); 可以发现，我们直接传入了一个data ptr，这个data ptr指向的就是weight的数据。\nOp的定义过程很简单，对于FP32的Fully Connect Op：\nauto status = xnn_define_fully_connected( xpb-\u0026gt;getXnnSubgraph(), std::numeric_limits\u0026lt;float\u0026gt;::min(), std::numeric_limits\u0026lt;float\u0026gt;::max(), inputs[0]-\u0026gt;uuid(), weight_params_.uuid(), bias_ ? bias_params_.uuid() : XNN_INVALID_VALUE_ID, outputs[0]-\u0026gt;uuid(), 0); 0x04 执行 执行的过程有以下几步：\ncreate model 通过之前定义好的subgraph来prealloc external inputs和outputs的Memory，xnnpack中提供了xnn_tensor_get_size和xnn_allocate_zero_simd_memory来帮助读者实现这一点。\ncreate runtime 读者需要事先创建好pthread线程池。\nxnn_create_runtime_v4(model_.get(), nullptr, nullptr, threadpool_, flags, \u0026amp;runtime_);\nreshape runtime xnn_reshape_runtime(runtime_);\nsetup runtime 在setup的时候需要传入external_values。\nxnn_setup_runtime_v2(runtime_, external_values_.size(), external_values_.data());\ninvoke xnn_invoke_runtime(runtime_)\n题外话：\n在mllm中，所有的执行过程被放在XpDispatch中：\nErrorCode XpDispatch::execute(vector\u0026lt;shared_ptr\u0026lt;Tensor\u0026gt;\u0026gt; inputs, vector\u0026lt;shared_ptr\u0026lt;Tensor\u0026gt;\u0026gt; outputs) { auto xnnbk = (XnnpackBackend *)(this-\u0026gt;backend()); // recreate runtime auto m_rt = xnnbk-\u0026gt;recreateModelRuntime(thread_count_); // create Model m_rt-\u0026gt;createModel(xnnbk-\u0026gt;getXnnSubgraph()); // create runtime m_rt-\u0026gt;createRuntime(0); // reshape m_rt-\u0026gt;reshapeRuntime(); // setup m_rt-\u0026gt;setupRuntime(); // run if (!m_rt-\u0026gt;invoke()) { return ErrorCode::NO_EXECUTION; } // update all output\u0026#39;s ptr xnnbk-\u0026gt;assignPtrToTensor(); return ErrorCode::MLLM_NO_ERROR; } ","permalink":"https://chenghuawang.github.io/keep-moving-forward/tech/xnnpack_guide/","summary":"0x01 前言 XNNPACK是一个由Google维护的算子库，在TensorFlowLite，ExecuTorch，ONNX RT等众多知名框架中使用。笔者最近在做mllm的xnnpack后端适配工作，因xnnpack缺少文档，在此记录。\nxnnpack的test中展示了大部分xnnpack的API和使用方式，读者在碰到API使用问题的时候不妨去test文件夹下面找找答案；xnnpack遵循标准的doxygen注释，也较好的说明了函数和class的使用方法。\n在xnn中使用的是静态图构建的方法，在开始构建静态图之前，需要初始化xnn：\nxnn_initialize(nullptr /* allocator */) 使用如下API可以构建出一张subgraph，接下来的所有操作都在更改这张子图，\nxnn_subgraph_t subgraph_ = nullptr; auto status = xnn_create_subgraph(external_nums, 0, \u0026amp;subgraph_); 0x02 定义 Tensor 定义未经过量化的Tensor使用的API是：\nuint32_t uuid; status = xnn_define_tensor_value( /*subgraph*/..., /*dtype*/..., dims.size(), dims.data(), /*data=*/..., /*external id*/XNN_INVALID_VALUE_ID, /*flag*/0, /*id*/\u0026amp;uuid); 这里需要特殊解释的是external_id、flag和uuid三个值：\nexternal_id 是对于EXternal Inputs和Outputs才需要设置的，对于xnnpack内部管理的Tensor，不需要设置这个值，给出默认的XNN_INVALID_VALUE_ID就行。external_id的作用是让xnnpack可以从runtime中传入的external_values中索引到需要的Tensor值。详细解释如下： 在xnnpack中，每个Tensor都会有一个uuid，对于xnnpack自己管理的Tensor，uuid在定义的时候会由xnnpack自己生成。还记得在xnn_create_subgraph创建的时候需要传入external_nums吗？这里的external_nums就是用户侧预留的uuid。比如external_nums是3的时候，xnn_define_tensor_value就会从4开始计数给新创建的Tensor。而前3个Tensor，即external Tensor的uuid(external_id)就是1，2，3。\nflag flag是用来标识这个Tensor是不是External Inputs，Outputs或者是其他类型的Tensor。比如inputs tensor的flag是flags = XNN_VALUE_FLAG_EXTERNAL_INPUT;, outputs 是 flags = XNN_VALUE_FLAG_EXTERNAL_OUTPUT;\nuuid 是每个Tensor的全局索引标识\n题外话：\n在mllm中，Tensor的define过程如下：\nvoid defineXpTensor(XnnpackBackend *xpb, Tensor *t, XpTensorType ttype) { if (t-\u0026gt;uuid() !","title":"Xnnpack 使用指南"},{"content":"本文主要总结两篇文章：Deja Vu 和 Apple 的 LLM in a flash。这两篇文章的内容都是端侧推理加速的尝试，他们主要使用了大致的思路\u0026ndash;利用MLP的稀疏性，各自的工程实现各有一些创新。\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time LLM in a flash: Efficient Large Language Model Inference with Limited Memory 端侧推理有着比较大的应用前景，随着端侧设备的算力跟进，端侧设备已经具有了运行7B模型的能力。在端侧运行小参数模型可以极大的减少云端的压力，从而减少运营成本。相比于云端的大模型，端侧大模型处理复杂问题能力不足，所以端侧和云端应该是相辅相成的。轻量级任务给端侧，需要长逻辑理解的任务交给云端。\n端侧和云的协同工作，也是一个很好的研究方向。\n0x01 Deja Vu 1. 问题分析和动机 作者通过分析OPT-175B模型的上下文稀疏性发现对于大部分的Transformer Layer，他们的稀疏性都在85%左右。上下文稀疏性就是：对于特定的输入，仅有一小部分的模型参数对最终结果有着重要的影响。 如图1-3所示：\nFig 1. Contextual SparsityDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nFig 3. Contextual sparsity in Attention HeadDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nFig 3. Contextual sparsity in MLP BlockDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\n在论文中，作者通过以下的步骤来计算上下文稀疏性：\n首先，对模型进行一次前向传播，记录下在计算过程中哪些注意力头和MLP神经元产生了较大的输出范数（output norms）。 然后，对每个输入样本进行第二次前向传播，但这次只使用第一次传播中记录的参数子集来计算输出。 通过比较两次前向传播的结果，作者们验证了即使只使用一部分参数，模型的输出也能保持与完整模型相当的准确性。这意味着对于特定的输入，模型展现出了上下文稀疏性。 相比较于静态分析的剪枝方法，基于上下文稀疏性来进行剪枝可以获得更好的Acc:\nFig 4. Accuracy-Efficiency Trade-offsDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\n上下文稀疏化根据当前输入动态调整激活的参数，实现了对模型的精准剪枝。这种适应性的方法比固定不变的剪枝策略更加灵活高效，因为它能够智能地选择对特定输入最为关键的参数，从而在牺牲极少模型性能的前提下，有效提升推理速度和计算效率。\n作者在实验的过程中还发现：Attention Head的每一个Head的输出对的分布是不同的，有的Head中Token对应的Attention Score很高，有的则很小。 作者假设，注意力机制是在做mean shift聚类。每一次过Attention Block就是在完成一次聚类迭代。作者将Attention Score打印了出来，见图5。\nFig 5. visualize the attention scores of three different heads for an exemplary sentence.Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\n作者们还观察到了层与层之间数据变化的特性。他们发现，在激活层中，尽管数据会经历线性和非线性变换，但相邻层之间的嵌入向量或激活向量仍保持高度的余弦相似性。 以OPT-175B模型为例，除了第一层外，任意两个连续层之间的相似度接近0.99，这表明在模型的传递过程中，输入的嵌入表示在层间的改变是渐进的，且在更深层的网络中这种特性更为显著。\n此外，作者们还探讨了导致这种高相似性的原因，指出这是由于Transformer中的残差连接造成的。残差连接允许信息在层与层之间直接传递，减少了因层内复杂变换导致的信息丢失。作者们还提供了数学上的分析，比如证明了在特定条件下，模型的层间变换具有收缩性质，这有助于维持信息的稳定性，从而使得层间的嵌入向量保持高度相似。这些发现对于理解和改进深度学习模型的效率和性能具有重要意义。\n2. 有效神经元预测算法 作者使用了一个较小的额外的神经网络分类器来预测每个层的上下文稀疏性。这个分类器预测每个Attention Head和MLP的参数在给定输入下的活跃度，然后做动态的剪枝。\nDeja Vu在预测的时候使用的是异步的策略，让分类器和模型的计算并行执行。之所以可以并行执行，是因为“在激活层中，尽管数据会经历线性和非线性变换，但相邻层之间的嵌入向量或激活向量仍保持高度的余弦相似性。 ” 这个重要结。因为在做计算的时候，是tiling的，所以我们可以通过活跃度选择性的加载我们需要的tile到内存里面去，从而减少内存的IO。\n本文的结果也很惊艳：\nFig 6. Accuracy Trend for DEJAVU-OPT-175B.Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nFig 7. Average per-token latency (ms).Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nsparsity percentage在70%左右的时候Acc些许下降，相比于FasterTransformer，有一倍latency下降。\n0x00 LLM in a Flash 一个7B半精度LLM，放在DRAM中需要14GB。纵使4bit量化，也要2.5GB+的DRAM，这对于端侧设备来说是一个不小的负担。这篇文章就沿用了Deja Vu的思路，并在工程上做了很多有意思的工作。本文的动机其实很常见，那就是：DRAM放不下，那就从Flash中取就行了，类似于SWAP？但是对于LLM的任务，对于DRAM带宽的要求很高，更不用说Flash Memory了。文中给出了各个部件的带宽，见图8-9。\nFig 8. Bandwidth in a unified memory architectureLLM in a flash: Efficient Large Language Model Inference with Limited Memory\nFig 9. Random read throughput of flash memoryLLM in a flash: Efficient Large Language Model Inference with Limited Memory\n这篇文章使用的方法和Deja Vu非常的相似，但是使用的是类似于LoRA的分类器。\nFig 10. Low Rank PredictorLLM in a flash: Efficient Large Language Model Inference with Limited Memory\n本文主要的难点是：\n如何快速准确的找出哪些模型的参数是重要的 如何减少Flash Memory的负载压力，或者说如何减少Flash Memory和DRAM之间的数据传输。实际上第一点也是减少数据传输的手段。 1. Selective Persistence Strategy LLM的参数主要是两个部分，Attention部分的参数和MLP部分的参数，其中MLP部分的参数占比非常的大。Apple在这里的方案是将Attention和Embedding层常驻在内存中，MLP部分的参数使用LoRA预测器（应该是灵感来源于Deja Vu）来进行预测。\n2. Sliding Window 每一次LLM前向推理的时候，都需要过LoRA，加载需要的参数，但是这个参数加载的过程是随机读取的，对于Flash来说并不友好。比如第一个token在前向的第一个MLP需要{0, 1, 7, 9}位置的参数，第二个token在前向推理的时候要{2, 3, 7, 9}位置的参数。那么我们其实可以keep参数{7, 9}来避免二次读取。这就是LRU的思想，Apple的方案在这里借鉴了LRU的方法。Sliding Window的核心思想就是LRU：保留处理过去k个token时的激活神经元所对应的参数在DRAM中，并在处理当前token时只对部分多余的参数进行删除和对缺少的参数进行加载。\nFig 11. Sliding windowLLM in a flash: Efficient Large Language Model Inference with Limited Memory\n","permalink":"https://chenghuawang.github.io/keep-moving-forward/tech/edgellms-algorithms-part-1/","summary":"本文主要总结两篇文章：Deja Vu 和 Apple 的 LLM in a flash。这两篇文章的内容都是端侧推理加速的尝试，他们主要使用了大致的思路\u0026ndash;利用MLP的稀疏性，各自的工程实现各有一些创新。\nDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time LLM in a flash: Efficient Large Language Model Inference with Limited Memory 端侧推理有着比较大的应用前景，随着端侧设备的算力跟进，端侧设备已经具有了运行7B模型的能力。在端侧运行小参数模型可以极大的减少云端的压力，从而减少运营成本。相比于云端的大模型，端侧大模型处理复杂问题能力不足，所以端侧和云端应该是相辅相成的。轻量级任务给端侧，需要长逻辑理解的任务交给云端。\n端侧和云的协同工作，也是一个很好的研究方向。\n0x01 Deja Vu 1. 问题分析和动机 作者通过分析OPT-175B模型的上下文稀疏性发现对于大部分的Transformer Layer，他们的稀疏性都在85%左右。上下文稀疏性就是：对于特定的输入，仅有一小部分的模型参数对最终结果有着重要的影响。 如图1-3所示：\nFig 1. Contextual SparsityDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\nFig 3. Contextual sparsity in Attention HeadDeja Vu: Contextual Sparsity for Efficient LLMs at Inference Time","title":"【施工中】端侧大模型推理-算法-Part1: Deja Vu, LLM in a Flash"},{"content":"GEMM/GEMV in MLLM Slides\n在本文中我以mllm的实现为例。mllm中的大部分混合精度的矩阵乘法是从llama.cpp中更改过来的。我们先来看下Q8_0和Q4_0代表什么。Huggingface的Doc中给出了一张表，大家可以去看一下：GGUF Quantization Type，我在这里也截图给出来\nFig 1. Q8_0和Q4_0含义GGUF Quantization Type FROM Huggingface\n对于量化操作不是很熟悉的读者可以看下我之前的blog: [Fundamental] 模型量化\n在mllm中，Q8_0和Q4_0的实现是这样的：\ntypedef struct { mllm_fp16_t d; // delta int8_t qs[QK8_0]; // quants QK8_0 = 32 } block_q8_0; // QK4_0 = 32 typedef struct { mllm_fp16_t d; // delta uint8_t qs[QK4_0 / 2]; // nibbles / quants } block_q4_0; 而Q4_0x4实际上就是将4个Q4_0打包成一组，这样在GEMM的时候可以利用起指令并行性。\n我们首先来看下GEMV问题定义，然后再推广到GEMM上。我们有两个矩阵，分别是A($1 \\times nr$), B($nc \\times nr$)，矩阵乘法后的结果是C$1 \\times nc$。一个不是非常恰当的图例如下图所示：\nFig 2. Q8_0和Q4_0x4的GEMV 为了更好的理解怎么分块，我们先来看下Q4_0x4的数据排布是怎么样的：Q4_0x4实际上是在$nc$的方向上以4分块，在$nr$的方向上以32分块，最终得到的block形状如下图所示：\nFig 3. Q8_0和Q4_0x4的GEMV Tiled 我们在$nc$的方向上以4分块，在$nr$的方向上以32分块，将gemv拆解成一个更小的子问题。\nFig 4. GEMV Vec 如图所示，Tiled A在每一词迭代，是取出x, x+16的位置的数据和Q4_0x4一行中的元素做点乘。在遍历完成后再reduce。Q4_0x4的每一行都是采用了优化的存储方法，这种存储方式在很多的量化算法中都有体现。该方式存储的数据可以很容易的使用左移和Mask等速度更快的操作来获得我们想要的信息。\n现在我们来看下汇编，在看汇编中，主要注意vector寄存器的排布，我在下文中给出了排布示意图\n\u0026#34;movi v31.16b, #0x4\\n\u0026#34; // for sshl. to get high bits. \u0026#34;movi v30.16b, #0xf0\\n\u0026#34; // for mask. to get low bits. \u0026#34;add %x[b_ptr], %x[b_ptr], #0x8\\n\u0026#34; // to qs \u0026#34;1:\u0026#34; // Column loop \u0026#34;add x22, %x[a_ptr], #0x2\\n\u0026#34; // to qs \u0026#34;movi v29.16b, #0x0\\n\u0026#34; // acc is on register v29(16x8bits). Set to 0. \u0026#34;mov x21, %x[nb]\\n\u0026#34; // move num of blocks to register x21 \u0026#34;2:\u0026#34; // Block loop \u0026#34;ldr q28, [%x[b_ptr], #0x0]\\n\u0026#34; // load 128 bits from b matrix \u0026#34;ldr q27, [x22, #0x0]\\n\u0026#34; // load 128 bits from a matrix \u0026#34;movi v26.4s, #0x0\\n\u0026#34; // acc is on register v26(4x32bits). Set to 0. \u0026#34;sub x20, x22, #0x2\\n\u0026#34; // to get scalar \u0026#34;ldr q25, [x22, #0x10]\\n\u0026#34; // load 128 bits to q25. offsets is 16B \u0026#34;ldr q24, [%x[b_ptr], #0x10]\\n\u0026#34; // load 128 bits to q24. offsets is 16B \u0026#34;sub x21, x21, #0x1\\n\u0026#34; // nb = nb - 1 \u0026#34;add x22, x22, #0x22\\n\u0026#34; // a_ptr = aptr + 34B \u0026#34;ldr q23, [%x[b_ptr], #0x20]\\n\u0026#34; // load 128 bits to q23. offset is 32 \u0026#34;ldr q22, [%x[b_ptr], #0x30]\\n\u0026#34; // load 128 bits to q22. offset is 48 \u0026#34;ld1r { v21.8h }, [x20]\\n\u0026#34; // scalar 4x16bit \u0026#34;ldr q20, [%x[b_ptr], #-0x8]\\n\u0026#34; // scalar 1x16bit \u0026#34;sshl v16.16b, v28.16b, v31.16b\\n\u0026#34; // get high bits in q4_0x4 \u0026#34;and v28.16b, v28.16b, v30.16b\\n\u0026#34; // get low bits in q4_0x4 \u0026#34;sshl v19.16b, v24.16b, v31.16b\\n\u0026#34; // get high bits in q4_0x4 \u0026#34;and v24.16b, v24.16b, v30.16b\\n\u0026#34; // get low bits in q4_0x4 \u0026#34;add %x[b_ptr], %x[b_ptr], #0x48\\n\u0026#34; // b_ptr = b_ptr + 72 \u0026#34;sshl v18.16b, v23.16b, v31.16b\\n\u0026#34; // get high bits in q4_0x4 \u0026#34;and v23.16b, v23.16b, v30.16b\\n\u0026#34; // get low bits in q4_0x4 \u0026#34;.inst 0x4f9be21a // sdot v26.4s, v16.16b, v27.4b[0]\\n\u0026#34; \u0026#34;sshl v17.16b, v22.16b, v31.16b\\n\u0026#34; // get high bits in q4_0x4 \u0026#34;and v22.16b, v22.16b, v30.16b\\n\u0026#34; // get low bits in q4_0x4 \u0026#34;fcvtl v21.4s, v21.4h\\n\u0026#34; // cvt 8x16b to 4x32b, scalar of a matrix \u0026#34;fcvtl v16.4s, v20.4h\\n\u0026#34; // cvt 8x16b to 4x32b, scalar of b matrix. reuse v16 register \u0026#34;.inst 0x4f99e39a // sdot v26.4s, v28.16b, v25.4b[0]\\n\u0026#34; \u0026#34;fmul v16.4s, v16.4s, v21.4s\\n\u0026#34; // v16 = v16 * v21, scalar a * scalar b \u0026#34;.inst 0x4fbbe27a // sdot v26.4s, v19.16b, v27.4b[1]\\n\u0026#34; // v19(8 bits) + v27(32bit, 1B) to v26(32bit) \u0026#34;.inst 0x4fb9e31a // sdot v26.4s, v24.16b, v25.4b[1]\\n\u0026#34; \u0026#34;.inst 0x4f9bea5a // sdot v26.4s, v18.16b, v27.4b[2]\\n\u0026#34; \u0026#34;.inst 0x4f99eafa // sdot v26.4s, v23.16b, v25.4b[2]\\n\u0026#34; \u0026#34;.inst 0x4fbbea3a // sdot v26.4s, v17.16b, v27.4b[3]\\n\u0026#34; \u0026#34;.inst 0x4fb9eada // sdot v26.4s, v22.16b, v25.4b[3]\\n\u0026#34; \u0026#34;scvtf v26.4s, v26.4s, #0x4\\n\u0026#34; // cvt int to float. the #0x4 is scale factor \u0026#34;fmla v29.4s, v26.4s, v16.4s\\n\u0026#34; // v29 = v26 * v16 + v29 \u0026#34;cbnz x21, 2b\\n\u0026#34; // is x21 is not zero, jmp to label 2. num block loop. \u0026#34;sub %x[nc], %x[nc], #0x4\\n\u0026#34; // sub col by 4 \u0026#34;str q29, [%x[res_ptr], #0x0]\\n\u0026#34; // store value to res_ptr \u0026#34;add %x[res_ptr], %x[res_ptr], #0x10\\n\u0026#34; // res_ptr move 16B. 4xf32. \u0026#34;cbnz %x[nc], 1b\\n\u0026#34; // if nc is not zero, jump to label 1. num col loop. Fig 5. GEMV Register 有了GEMV，GEMM的实现就相对轻松的多了，GEMM在GEMV的分块基础上，还在$n$维度上进行了大小为4的分块，然后对每个row为4的分块做GEMV，并且通过strip-mining来获得指令并行。\nFig 6. GEMM ","permalink":"https://chenghuawang.github.io/keep-moving-forward/tech/q8_0_q4_0_4_gemm_in_llamacpp/","summary":"GEMM/GEMV in MLLM Slides\n在本文中我以mllm的实现为例。mllm中的大部分混合精度的矩阵乘法是从llama.cpp中更改过来的。我们先来看下Q8_0和Q4_0代表什么。Huggingface的Doc中给出了一张表，大家可以去看一下：GGUF Quantization Type，我在这里也截图给出来\nFig 1. Q8_0和Q4_0含义GGUF Quantization Type FROM Huggingface\n对于量化操作不是很熟悉的读者可以看下我之前的blog: [Fundamental] 模型量化\n在mllm中，Q8_0和Q4_0的实现是这样的：\ntypedef struct { mllm_fp16_t d; // delta int8_t qs[QK8_0]; // quants QK8_0 = 32 } block_q8_0; // QK4_0 = 32 typedef struct { mllm_fp16_t d; // delta uint8_t qs[QK4_0 / 2]; // nibbles / quants } block_q4_0; 而Q4_0x4实际上就是将4个Q4_0打包成一组，这样在GEMM的时候可以利用起指令并行性。\n我们首先来看下GEMV问题定义，然后再推广到GEMM上。我们有两个矩阵，分别是A($1 \\times nr$), B($nc \\times nr$)，矩阵乘法后的结果是C$1 \\times nc$。一个不是非常恰当的图例如下图所示：\nFig 2. Q8_0和Q4_0x4的GEMV 为了更好的理解怎么分块，我们先来看下Q4_0x4的数据排布是怎么样的：Q4_0x4实际上是在$nc$的方向上以4分块，在$nr$的方向上以32分块，最终得到的block形状如下图所示：\nFig 3. Q8_0和Q4_0x4的GEMV Tiled 我们在$nc$的方向上以4分块，在$nr$的方向上以32分块，将gemv拆解成一个更小的子问题。","title":"Q8_0 @ Q4_0_4 GEMM/GEMV in llama.cpp"},{"content":"0x00 前沿和阅读材料 FlashDecoding系列的文章是对FA在推理场景下的改进，目前包含两篇文章：\nFlash-Decoding for long-context inference, Torch团队的Blog FlashDecoding++: Faster Large Language Model Inference on GPUs 我们知道，在FA2中特别对Seq方向做了并行化，但是在推理的时候Seq=1。此时，并不能占用满GPU的全部的SM，导致性能损失，FlashDecoding就是对此的优化。\n0x01 FlashDecoding 在解码过程中，生成的每个新Token都需要考虑之前的所有Token，以便进行注意力计算。\n在训练的时候，Attention这一算子已使用FlashAttentionV2算法进行了优化，其瓶颈在于读写中间结果（例如 Q @ K^T）的内存带宽不足。但是，这些优化并不能直接应用于推理，因为推理的时候不在是内存带宽的瓶颈。在训练过程中，FlashAttention 会在Batch和Seq两个维度上进行并行处理。在推理过程中，Seq=1，这意味着，如果Batch大小小于 GPU 上的SM数量（A100 为 108），则这个Attention操作只会使用 GPU 的一小部分！在使用长上下文时尤其如此。当Batch大小为 1 时，FlashAttention 将使用不到 1%的 GPU！\nFig 1. Regular AttentionFlash-Decoding for long-context inference\n为此我们不难想到可以实用Split-K的方法来使得Attention在推理的时候也有很好的并行性。如下图所示：\nFig 1. Split-K AttentionFlash-Decoding for long-context inference\n非常的好理解，但是这里需要注意的是，在最后的Reduce Op这里还是要做Online Softmax的，所以在SRAM里面保存的东西是比原来多的，除了Output，还有exp Sum和Max。\n0x02 FlashDecoding++ flashdecoding++不是meta官方出品的。\nFA中，求解Max需要遍历迭代，之后的子块依赖于之前的子块。Safe-softmax的计算公式中，需要先求每行x的最大值，然后减去这个max(x)之后，再做softmax以防止数值溢出。FlashDecoding++提出的创新点就是，我们可以实用一个先验的$\\phi$来作为max值，只要它能让数值稳定就可以了。从Safe Softmax的公式上来看，无论是$\\phi$还是max(x)，他们的结果是一致的，我们需要追求的是数值上的稳定与否。\nFlashDecoding++认为一个合理的先验值 $\\phi$，可以直接从数据集中进行统计获得。对于不同的模型，这个先验值也是不一样的。在实现的时候，FlashDecoding++还使用了Fallback的思路，当出现数值溢出的时候，使用传统的FlashDecoding。\n那为什么FalshDecoding++能异步，而FlashDecoding不行呢？\n在FalshDecoding Split-K分成的几个区间内，还是使用的FA2的方法来计算，但是FA2的一次迭代是依赖于上一次迭代的结果的，也就是需要rescale。但是FlashDecoding++不需要，它大致上是这样的：\n$$\\begin{aligned} \u0026\\ell^{(1)}=\\mathrm{rowsum}\\Big(e^{\\mathbf{S}^{(1)}-\\phi}\\Big)\\in\\mathbb{R}^{B_{r}} \\\\ \u0026\\tilde{\\mathbf{O}}^{(1)}=e^{\\mathbf{S}^{(1)}-\\phi}\\mathbf{V}^{(1)}\\in\\mathbb{R}^{B_{r}\\times d} \\\\ \u0026\\ell^{(2)}=\\mathrm{rowsum}\\left(e^{\\mathbf{S}^{(2)}-\\phi}\\right) \\\\ \u0026\\tilde{\\mathbf{O}}^{(2)}=e^{s^{(2)}-\\phi}\\mathbf{V}^{(2)} \\\\ \u0026\\mathbf{O}^{(2)}=\\mathrm{diag}\\left(\\ell^{(1)}+\\ell^{(2)}\\right)^{-1}(\\tilde{\\mathbf{O}}^{(1)}+\\tilde{\\mathbf{O}}^{(2)})=\\mathbf{O} \\end{aligned}$$ ","permalink":"https://chenghuawang.github.io/keep-moving-forward/tech/fundamental_flashdecoding_series/","summary":"0x00 前沿和阅读材料 FlashDecoding系列的文章是对FA在推理场景下的改进，目前包含两篇文章：\nFlash-Decoding for long-context inference, Torch团队的Blog FlashDecoding++: Faster Large Language Model Inference on GPUs 我们知道，在FA2中特别对Seq方向做了并行化，但是在推理的时候Seq=1。此时，并不能占用满GPU的全部的SM，导致性能损失，FlashDecoding就是对此的优化。\n0x01 FlashDecoding 在解码过程中，生成的每个新Token都需要考虑之前的所有Token，以便进行注意力计算。\n在训练的时候，Attention这一算子已使用FlashAttentionV2算法进行了优化，其瓶颈在于读写中间结果（例如 Q @ K^T）的内存带宽不足。但是，这些优化并不能直接应用于推理，因为推理的时候不在是内存带宽的瓶颈。在训练过程中，FlashAttention 会在Batch和Seq两个维度上进行并行处理。在推理过程中，Seq=1，这意味着，如果Batch大小小于 GPU 上的SM数量（A100 为 108），则这个Attention操作只会使用 GPU 的一小部分！在使用长上下文时尤其如此。当Batch大小为 1 时，FlashAttention 将使用不到 1%的 GPU！\nFig 1. Regular AttentionFlash-Decoding for long-context inference\n为此我们不难想到可以实用Split-K的方法来使得Attention在推理的时候也有很好的并行性。如下图所示：\nFig 1. Split-K AttentionFlash-Decoding for long-context inference\n非常的好理解，但是这里需要注意的是，在最后的Reduce Op这里还是要做Online Softmax的，所以在SRAM里面保存的东西是比原来多的，除了Output，还有exp Sum和Max。\n0x02 FlashDecoding++ flashdecoding++不是meta官方出品的。\nFA中，求解Max需要遍历迭代，之后的子块依赖于之前的子块。Safe-softmax的计算公式中，需要先求每行x的最大值，然后减去这个max(x)之后，再做softmax以防止数值溢出。FlashDecoding++提出的创新点就是，我们可以实用一个先验的$\\phi$来作为max值，只要它能让数值稳定就可以了。从Safe Softmax的公式上来看，无论是$\\phi$还是max(x)，他们的结果是一致的，我们需要追求的是数值上的稳定与否。\nFlashDecoding++认为一个合理的先验值 $\\phi$，可以直接从数据集中进行统计获得。对于不同的模型，这个先验值也是不一样的。在实现的时候，FlashDecoding++还使用了Fallback的思路，当出现数值溢出的时候，使用传统的FlashDecoding。\n那为什么FalshDecoding++能异步，而FlashDecoding不行呢？\n在FalshDecoding Split-K分成的几个区间内，还是使用的FA2的方法来计算，但是FA2的一次迭代是依赖于上一次迭代的结果的，也就是需要rescale。但是FlashDecoding++不需要，它大致上是这样的：\n$$\\begin{aligned} \u0026\\ell^{(1)}=\\mathrm{rowsum}\\Big(e^{\\mathbf{S}^{(1)}-\\phi}\\Big)\\in\\mathbb{R}^{B_{r}} \\\\ \u0026\\tilde{\\mathbf{O}}^{(1)}=e^{\\mathbf{S}^{(1)}-\\phi}\\mathbf{V}^{(1)}\\in\\mathbb{R}^{B_{r}\\times d} \\\\ \u0026\\ell^{(2)}=\\mathrm{rowsum}\\left(e^{\\mathbf{S}^{(2)}-\\phi}\\right) \\\\ \u0026\\tilde{\\mathbf{O}}^{(2)}=e^{s^{(2)}-\\phi}\\mathbf{V}^{(2)} \\\\ \u0026\\mathbf{O}^{(2)}=\\mathrm{diag}\\left(\\ell^{(1)}+\\ell^{(2)}\\right)^{-1}(\\tilde{\\mathbf{O}}^{(1)}+\\tilde{\\mathbf{O}}^{(2)})=\\mathbf{O} \\end{aligned}$$ ","title":"[Fundamental] FlashDecoding Series"},{"content":"0x00 Materials 本文是对LM量化的学习笔记，其中有不少内容是摘自业内的前辈们的文章，在此一并感谢。所参考的资料、摘录的文章来源在下面列出：\n经典论文：\n2021-02, VS-Quant: Per-Vector Scaled Quantization for Accurate Low-Precision Neural Network Inference [Steve Dai, et al.] 2022-08, FP8 Quantization: The Power of the Exponent，高通 2022-08, LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale 2022-09, FP8 Formats for Deep Learning，Arm \u0026amp; Intel \u0026amp; Nvidia 2022-11, SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models 2023-10, LLM-FP4: 4-Bit Floating-Point Quantized Transformers 解析：\n量化那些事之FP8与LLM-FP4 [Transformer 101系列] LLM模型量化世界观(上) [Transformer 101系列] LLM模型量化世界观(下) 课程：\nTinyML and Efficient Deep Learning Computing from MIT-Han-Lab，看量化和剪枝两章。 0x01 前言 数据类型 FP32，FP16，FP8 and BF16, \u0026hellip; 这里复习下最基础的计算机组成里面的知识。浮点数可以被符号(Sign)、指数（exponent）、尾数（mantissa）三部分来表示。忘记了可以去看IEEE754 wikipedia去复习下计算方法。\nFig 1. FP32, FP16, BF16,etc...https://www.servethehome.com/nvidia-h100-hopper-details-at-hc34-as-it-waits-for-next-gen-cpus/nvidia-h100-hopper-fp8-tensor-core/\n这里主要是强调表示方法，比如FP8(E5M2、E4M3)，E表示指数、M表示尾数。我们接下来看一下4bit的精度，因为4bit的精度可以更容易说明subnormal numbers。\n下面的公式的公式表示了FP的计算方式，$p$是指数，$b$是bias。\n$$ f = (-1)^s \\times 2^{p-b} \\times (1 + \\frac{d_1}{2^1} + \\frac{d_2}{2^2} + \\dots + \\frac{d_m}{s^m}) $$\n我们首先看E2M2的FP4，FP4不保留符号位，使用完整的4bit来表示数据：\n指数 尾数 指数实值$e=2^{p-b}$这里使用$b=0$ 尾数实值$m = (1 + \\frac{d_1}{2^1} + \\frac{d_2}{2^2} + \\dots + \\frac{d_m}{s^m})$ 00 00 $1$ $1$ 01 01 $2$ $\\frac{5}{4}$ 10 10 $4$ $\\frac{6}{4}$ 11 11 $8$ $\\frac{7}{4}$ 将4个指数和4个尾数相乘，我们可以得到16个数：\n$$ \\frac{4}{4}, \\frac{5}{4}, \\frac{6}{4},\\frac{7}{4},\\frac{4}{2},\\frac{5}{2},\\frac{6}{2},\\frac{7}{2},4,5,6,7,8,10,12,14 $$\n我们发现，上述的16个数字是没办法表示0的，这肯定不是我们想要的表示思路。之所以不表示0，是因为如果我们使用下面的式子：\n$$ f = 2^{p-b} \\times (0 + \\frac{d_1}{2^1} + \\frac{d_2}{2^2} + \\dots + \\frac{d_m}{s^m}) $$\n那么如果尾数是0(即二进制的00)，无论指数是什么情况，最终解析出来的fp4数字都是0，也就是说我们为了引入0浪费了3个数据表示，这种情况在只能表示16个数字的fp4中显然是不允许存在的。因此，fp4规则引入了不同的处理方法，即subnormal numbers(Fig 2 中的 x x x x 四个点)。\n当指数是0的时候，强行令指数是1，subnormal numbers使用下述公式来计算：\n$$f = 2^{1-b} \\times (0 + \\frac{d_1}{2^1} + \\frac{d_2}{2^2} + \\dots + \\frac{d_m}{s^m})$$\nFig 2. FP4数据分布 from LLM-FP42023-10, LLM-FP4: 4-Bit Floating-Point Quantized Transformers\n如果不使用subnormal numbers，$b=-2$，那么16个数的分布是：\n$$\\frac{4}{16},\\frac{5}{16},\\frac{6}{16},\\frac{7}{16},\\frac{4}{8},\\frac{5}{8},\\frac{6}{8},\\frac{7}{8},\\frac{4}{4},\\frac{5}{4},\\frac{6}{4},\\frac{7}{4},\\frac{4}{2},\\frac{5}{2},\\frac{6}{2},\\frac{7}{2}$$\n使用subnormal numbers，$b=-2$后是：\n$$\\frac{0}{8},\\frac{1}{8},\\frac{2}{8},\\frac{3}{8},\\frac{4}{8},\\frac{5}{8},\\frac{6}{8},\\frac{7}{8},\\frac{4}{4},\\frac{5}{4},\\frac{6}{4},\\frac{7}{4},\\frac{4}{2},\\frac{5}{2},\\frac{6}{2},\\frac{7}{2}$$\nInt32、Int16、Int8 and Int4 \u0026hellip; 整型的编码就十分简单了，没有什么可以讲的。与FP不同的是，整型的每个数是均匀分布的。\nFig 3. 整型 vs 浮点https://arxiv.org/pdf/2305.12356\n0x02 常见量化计算方法 Linear Quantization Storage: Int\nComputation: Int，实际上Int/Float应该是都可以的？\n线性量化就是将一组整型映射到实际数值的仿射变换，可以用如下公式来表示：\n$$r=S(q-Z)$$\n其中$r$表示量化前的数值，$S$表示缩放因子，$q$表示量化后的数值，$Z$表示零点偏移量。如果不对零点进行偏移就是对称量化，但是可能会损失很多的精度；如果对零点进行偏移就是非对称量化，会有更好的量化效果。量化的方式如下图所示：\nFig 4. Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference [Jacob et al., CVPR 2018]TinyML and Efficient Deep Learning Computing\n接下来的问题就是我们怎么求解出$S,Z$:\n$$r_{max} = S(q_{max} - Z)$$\n$$r_{min} = S(q_{min}-Z)$$\n两个式子，两个变量，可以解，解得：\n$$S=\\frac{r_{max}-r_{min}}{q_{max}-q_{min}}$$\n$$Z = \\text{Round}(\\frac{q_{min}}{S})$$\n此时我们来考虑一下量化后的矩阵怎么做矩阵乘法，这在Transformer中是最频繁的操作。对于矩阵乘法：\n$$\\bf{Y} = \\bf{W}\\bf{X}$$\n$$S_{\\mathbf{Y}}\\left(\\mathbf{q_{Y}}-Z_{\\mathbf{Y}}\\right)=S_{\\mathbf{W}}\\left(\\mathbf{q_{W}}-Z_{\\mathbf{W}}\\right)\\cdot S_{\\mathbf{X}}\\left(\\mathbf{q_{X}}-Z_{\\mathbf{X}}\\right)$$\n$$\\mathbf{q_{Y}}=\\frac{S_{\\mathbf{W}}S_{\\mathbf{X}}}{S_{\\mathbf{Y}}}\\left(\\mathbf{q_{W}}-Z_{\\mathbf{W}}\\right)\\left(\\mathbf{q_{X}}-Z_{\\mathbf{X}}\\right)+Z_{\\mathbf{Y}}$$\n$$\\mathbf{q_{Y}}=\\frac{S_{\\mathbf{W}}S_{\\mathbf{X}}}{S_{\\mathbf{Y}}}\\left(\\mathbf{q_{W}}\\mathbf{q_{X}}-Z_{\\mathbf{W}}\\mathbf{q_{X}}-Z_{\\mathbf{X}}\\mathbf{q_{W}}+Z_{\\mathbf{W}}Z_{\\mathbf{X}}\\right)+Z_{\\mathbf{Y}}$$\n这里的$S_{\\bf{Y}}, Z_{\\bf{Y}}$是在PTQ中使用一个验证集来计算的全局值。比如W4A4，也就是$\\bf{W},\\bf{X}$都使用4bits量化，然后计算的时候使用int32，然后此时就可以运用预先计算好的$S_{\\bf{Y}}, Z_{\\bf{Y}}$了，算完后再计算量化后的$\\bf{Y}$。如下图：\nFig 5. Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference [Jacob et al., CVPR 2018]TinyML and Efficient Deep Learning Computing\n优化 1：Fix-Point Multiplication 在实验中发现，$\\frac{S_{\\mathbf{W}}S_{\\mathbf{X}}}{S_{\\mathbf{Y}}} \\in (0,1)$，所以在实现的时候我们可以实用定点数而非浮点数来实现。\n$$\\frac{S_{\\mathbf{W}}S_{\\mathbf{X}}}{S_{\\mathbf{Y}}} = 2^{-n} M_0, M_0 \\in [0.5,1)$$\n其中，$M_0$是定点数。而定点数可以实用Int来模拟，Int的计算会比浮点数快得多。\n优化 2：$Z_{\\bf{W}} = 0$，对称量化 不难发现，若$Z_{\\bf{w}} = 0$，我们可以省下很大一部分的计算。如果参数矩阵$\\bf{W}$的值近乎于正态分布，也就是说均值为0，那么我们就可以不使用$Z_{\\bf{w}}$。这时候的$S$就非常的直观了:\n$$S = \\frac{\\vert r \\vert _{max}}{2^{N-1}}$$\n其中，$N$是量化的位数。非常好理解，就是要把最大的动态范围的$r$尽数映射到$N$bits量化能够表示的全部空间内部。Pytorch 和 ONNX 的native量化实现就是用的这个方法。优化后，我们得到简化的矩阵乘法公式：\n$$\\mathbf{q_{Y}}=\\frac{S_{\\mathbf{W}}S_{\\mathbf{X}}}{S_{\\mathbf{Y}}}\\left(\\mathbf{q_{W}}\\mathbf{q_{X}}-Z_{\\mathbf{X}}\\mathbf{q_{W}}\\right)+Z_{\\mathbf{Y}}$$\n含有Bias的全连接层 $\\bf{Y} = \\bf{W}\\bf{X} + \\bf{b}$\n$S_{\\mathbf{Y}}\\left(\\mathbf{q_{Y}}-Z_{\\mathbf{Y}}\\right)=S_{\\mathbf{W}}\\left(\\mathbf{q_{W}}-Z_{\\mathbf{W}}\\right)\\cdot S_{\\mathbf{X}}\\left(\\mathbf{q_{X}}-Z_{\\mathbf{X}}\\right) + S_{\\mathbf{b}}\\left(\\mathbf{q_{b}}-Z_{\\mathbf{b}}\\right)$\n我们可以继续使用上述的两种优化方法，令$Z_{\\bf{b}}=0,Z_{\\bf{w}} =0$:\n$$S_{\\mathbf{Y}}\\left(\\mathbf{q_{Y}}-Z_{\\mathbf{Y}}\\right)=S_{\\mathbf{W}}\\left(\\mathbf{q_{W}}\\right)\\cdot S_{\\mathbf{X}}\\left(\\mathbf{q_{X}}-Z_{\\mathbf{X}}\\right) + S_{\\mathbf{b}}\\left(\\mathbf{q_{b}}\\right)$$\n$$S_\\mathbf{Y}\\left(\\mathbf{q_Y-Z_Y}\\right)=S_\\mathbf{W}S_\\mathbf{X}\\left(\\mathbf{q_Wq_X-Z_Xq_W}\\right)+S_\\mathbf{b}\\left(\\mathbf{q_b}\\right)$$\n可以继续损失一点精度，令$S_{\\mathbf{b}}= S_\\mathbf{W}S_\\mathbf{X}$:\n$$S_\\mathbf{Y}\\left(\\mathbf{q_Y-Z_Y}\\right)=S_\\mathbf{W}S_\\mathbf{X}\\left(\\mathbf{q_Wq_X-Z_Xq_W}+\\mathbf{q_b}\\right)$$\n$$\\mathbf{q_{Y}}=\\frac{S_{\\mathbf{W}}S_{\\mathbf{X}}}{S_{\\mathbf{Y}}}\\left(\\mathbf{q_{W}}\\mathbf{q_{X}}+\\bf{q}_b-Z_{\\mathbf{X}}\\mathbf{q_{W}}\\right)+Z_{\\mathbf{Y}}$$ 其中，我们可以预先计算$\\bf{q}_{bias}$:\n$$\\bf{q}_{bias} = \\bf{q}_b - Z_{\\bf{X}}\\bf{q}_{\\bf{W}}$$ $$\\mathbf{q_{Y}}=\\frac{S_{\\mathbf{W}}S_{\\mathbf{X}}}{S_{\\mathbf{Y}}}\\left(\\mathbf{q_{W}}\\mathbf{q_{X}}+\\bf{q}_{bias}\\right)+Z_{\\mathbf{Y}}$$\nFig 6. Bias QuantTinyML and Efficient Deep Learning Computing\n如何计算Activation的$Z,S$ Type 1. 训练时计算 使用Exponential Moving Averages(EMA)。在训练的时候计算每组Activations的$S, Z$，然后使用EMA来累计得到我们最终需要的$S,Z$。\n$$\\hat{r}_{\\max,\\min}^{(t)}=\\alpha\\cdot r_{\\max,\\min}^{(t)}+(1-\\alpha)\\cdot\\hat{r}_{\\max,\\min}^{(t-1)}$$ Type 2. 在训练后，使用一些样本来推理 每一次输入一个Batch给网络，然后计算出Activations的平均$r_{min}, r_{max}$。 使用KL散度等方法来找到Clipping的准确的点。 Fig 7. Activations Values的统计8-bit Inference with TensorRT [Szymon Migacz, 2017]\n上图是每一个Activations Values的统计，横坐标是Activations的值，纵坐标是对应一个Value区间的Activations在整个网络中出现的次数。我们希望找到一个合理的截断点，使得量化损失最小，那么使用KL散度，将量化后的输出和FP32模型的输出做Loss就知道Clip的位置在哪里合适了。\nFig 8. 合适的截断8-bit Inference with TensorRT [Szymon Migacz, 2017]\nClip掉的越多，低精度的数据类型映射到原始的数据精度的区域就更细致，如下图。但是卡掉的Outliers越多也会导致模型的精度下降。\nFig 9. ClippingOptimal Clipping and Magnitude-aware Differentiation for Improved Quantization-aware Training [Sakr et al., ICML 2022]\n如何Round？四舍五入是好方法吗？ 四舍五入的方法肯定不是最优的。因为Tensor中的每一个值是跟周围的值有关系的，我们要考虑他周围的值。\nFig 10. RoundingUp or Down? Adaptive Rounding for Post-Training Quantization [Nagel et al., PMLR 2020]\n我们希望知道每一个值到底是下取整好还是上取整好，我们可以用一个可以学习的Bias来学习得到如何Rounding。如：\n$$\\tilde{w}=\\lfloor\\lfloor w\\rfloor+\\delta\\rceil,\\delta\\in[0,1]$$\n在优化的时候，我们使用下述式子：\n$$\\argmin_{\\mathbf{V}}\\Vert \\mathbf{W}\\mathbf{x}- \\lfloor\\lfloor\\mathbf{W} + \\mathbf{h}(\\mathbf{V})\\mathbf{x}\\Vert_\\mathbf{F}^2+\\lambda f_{\\text{reg}}(\\mathbf{V})$$\n$\\mathbf{x}$是每个Layer的输入，$\\bf{V}$是需要被优化的参数。$\\mathbf{h}()$是一个将值映射到$(0, 1)$的函数，比如rectified sigmoid。\n$f_{\\text{reg}}(\\mathbf{V})$是一个正则化的函数，鼓励$\\bf{h}(\\bf{V})$的结果是二值的。\n$$f_{\\text{reg}}(\\mathbf{V}) = \\sum_{i,j}1- \\vert 2\\bf{h}(\\bf{V}_{i,j}) - 1 \\vert ^ {\\beta}$$\nK-Means Based Fig 11. K-Means CompressionDeep Compression [Han et al., ICLR 2016], from han lab.\nK Means方法比较直白，就是对weight做K-Means聚类，把weight转换成index和lookup table。\n0x03 量化粒度 下面介绍三种PTQ（Post-Training Quantization）中使用的量化粒度。\nFig 12. Pre-Tensor, Per-Channel, Per-Group示意图TinyML and Efficient Deep Learning Computing\nPer-Tensor Quantization $$\\vert r \\vert _{max} = \\vert \\bf{W} \\vert _{max}$$\n缩放系数$S$是在整个Tensor层面上进行的。但是对于Outliers的兼容性不好，我们需要更加细的量化粒度。\nFig 13. MobileNetV2 Tensor Per-Channel RangeData-Free Quantization Through Weight Equalization and Bias Correction [Markus et al., ICCV 2019]\n从图中MobileNetV2的一个Tensor的Per Channel Range中可以看出每一个Channel的动态范围非常不一样，对于一个Tensor使用同一个$S,Z$的粒度太粗了。我们可以考虑对每一个Channel做量化操作，即量化的粒度下放到Channel的程度。\nPer-Channel Quantization 以2bit为例，将一个Tensor的每一个Channel进行缩放。\nFig 14. Per-Channel QuantizationTinyML and Efficient Deep Learning Computing\n$$S = \\frac{\\vert r \\vert _{max}}{q_{max}}$$ Fig 15. Per-Channel QuantizationTinyML and Efficient Deep Learning Computing\nGroup Quantization Group Quantization是一种细粒度更小的量化方法，如下图中仅对一个框定的Vector进行量化。Group Quantization通常和层次化的量化方法一起使用，读者可以进一步看这篇文章来了解层次化的量化方法：2021-02, VS-Quant: Per-Vector Scaled Quantization for Accurate Low-Precision Neural Network Inference [Steve Dai, et al.]。一个简单的描述是：\n$$r = S(q-Z) \\to r = \\gamma \\cdot S_q(q-Z)$$\n其中$\\gamma$是一个浮点数的Per-Tensor缩放系数，$S_q$是一个Integer的Per-Vector的缩放系数\nFig 16. Per-Group QuantizationTinyML and Efficient Deep Learning Computing\n这样的量化方法可以被多层次使用，如下图所示：\nFig 17. VSQVS-Quant: Per-Vector Scaled Quantization for Accurate Low-Precision Neural Network Inference [Steve Dai, et al.]\n上图中，VSQ的处理逻辑是：\n先对四个元素一组的元素做Group Quantization，使用的$S$为UINT4类型的。 然后对整个Channel做Per-Channel的量化，使用的$S$为FP16类型的。 在计算Effective的时候，上图中是忽略了Per-Channel的16个bits的，顾仅仅是4（Per-Group的UINT4） / 16。\nFig 18. MX TypeWith Shared Microexponents, A Little Shifting Goes a Long Way [Bita Rouhani et al.]\n上图中描述的MX量化方法用了共享指数部分的操作，以MX4为例：\n每个元素为S1M2，我们发现Exponent不见了，其实Export是L0 Group 量化的$S$，每2个元素共用一个Exponent。\n在第二层次的Group量化也是这样的，只不过是每4个元素共用一个8bits的Exponent。\n0x04 Quantization-Aware Training Fig 19. QATTinyML and Efficient Deep Learning Computing\n在训练的时候需要保留FP32的副本，因为我们希望梯度被累计。比如梯度传回来+0.1，但是如果不保留FP32的副本，Int值的量化参数会Round掉+0.1导致网络无法收敛。使用FP32后，当梯度传播回来的值累积到一定的程度后，Round后的量化参数会发生变化。所以前向走量化参数，反向走FP32的全量副本。\n0x05 Advance Advance的内容可以看博客，这里会不定期更新链接：\n✅[April-May 2024] 模型量化之 🥕Quarot \u0026amp; SpinQuant ✅[Oct 2023] 模型量化之QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models ✅[April 2024] 模型量化之AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration ","permalink":"https://chenghuawang.github.io/keep-moving-forward/tech/fundamental_quantization/","summary":"RoPE from Fundamental Series","title":"[Fundamental] 模型量化"},{"content":"0. 前言 本问分析的两篇文章是：\n2024-05, SpinQuant: LLM quantization with learned rotations from meta，一作是 Zechun Liu\n2024-04, QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs from ETH Zurich、EPFL、Microsoft Research、IST Austria \u0026amp; NeuralMagic\n这两篇文章以相同的视角去解决问题，并且量化后的模型保持了相当好的性能，应该就是未来模型量化的一个主要的跟进方向。QuaRot和SpinQuant可以算作是同一系列的工作，SpinQuant在QuaRot的基础上做了可学习旋转矩阵的改进。\n1. Computational Invariance Computational Invariance是Quarot和SpinQuant的基础。\nComputational Invariance定理[1]指出，可以使用正交矩阵对Transformer 中的权重和块间激活进行变换，而模型输出不变。这个定理是说，如果$W_{in}$是一个在transformer block(i.e. $W_k,W_q,W_v$等)左边的权重，我们可以左乘上一个正交的矩阵$Q$，为了在最后的结果里消除这个影响，我们可以在输出矩阵(i.e. $W_{out}, W_{down}$)右边乘上$Q_T$。\n尽管 RMSNorm 被应用于两个数据块之间，但只要 RMSNorm 数据块中没有重新缩放（在实际操作中，我们会先将任何重新缩放吸收到相邻的权重矩阵中），这一点还是适用的。从概念上讲，这是因为 RMSNorm 将激活值除以它们的模长，而对激活值应用旋转矩阵$Q$不会影响模长（因为正交矩阵的特性）：\n$$\\text{RMSNorm}(\\boldsymbol{X}) = \\text{RMSNorm}(\\boldsymbol{X\\boldsymbol{Q}^T})\\boldsymbol{Q}$$\n我们这里假设RMSNorm对激活值$\\boldsymbol{X}$的每一行做的操作都是$x_{i} \\larr x_i/ \\Vert x_I \\Vert$。这意味着，将输出矩阵乘以 $Q^T$ 会使线性层输出$XQ^T$，$XQ^T$被归一化，然后传入下一个区块，其输入权重矩阵现在是$QW$，因此该线性层不做任何修改就会输出原始激活。\n2. 🥕Quarot Quarot有两个阶段，一个阶段是在全精度下操作模型权重，并在模型的前向传递中插入两个额外的乘Hadamard矩阵操作；在第二个阶段使用现在的量化方法来量化夹在Hadamard矩阵中的模型权重，因为这些权重被削减了峰度，outliers减少，可以使量化的压力小很多。\nFig 1. QuaRot workflow 1 Fig 2. QuaRot workflow 2 Quarot文中的图片已经非常清晰的描绘了什么时候做旋转以及何时做量化和量化的数据流。SpinQuant和Quarot比较了实验结果，Quarot的实验结果请看第三章节的SpinQuant的表格。\n3. SpinQuant spin把成对的旋转和反旋转操作用到了整个模型中去，包括残差的地方。对于我来说，SpinQuant更加的直接一点。SpinQuant的动机和Quarot相似都是为了解决Outlier问题， Outliers会拉伸量化范围，导致大部分值的在对应量化网格上的有效映射减少；或者需要裁剪Outliers。通过对激活或权重矩阵进行旋转可以帮助消除Outliers。\n作者的文章写的很好，展示了旋转之后的数据分布情况：\nFig 3. 数据分布。LLaMA-2 7B 模型旋转前后的激活分布。旋转前，特定通道中存在异常值。由于大多数硬件都不支持per-channel量化，因此利用旋转去除异常值可以实现精确的token-wise量化或tensor-wise量化。 作者使用的旋转矩阵不同于Quarot，她使用了Cayley SGD 优化器来学到一个旋转矩阵。学习得到的旋转矩阵比Quarot的Hadamard矩阵有更好的效果。作者也探究了不同的旋转矩阵对模型性能的影响：\nFig 4. 不同的旋转矩阵对模型性能影响的分析 作者在文中和Quarot一样使用了两个Stage的方式，第一个Stage是训练学习到旋转矩阵，第二个Stage是将旋转矩阵和权重合并之后再做量化。如下面的流程图所示：\nFig 5. SpinQunat算法流程示意图。(a) 可以在 Transformer 网络中旋转残差流，从而得到旋转前后数值相当的浮点网络。旋转后的激活异常值更少，也更容易量化。(b) 和 (c) 旋转矩阵可以与相应的权重矩阵进行整合，我们进一步定义了 R2、R3 和 R4，以减少块内的异常值。 需要说明的是，图中$q$分支的的$R_3$应该是标注错误了，$R_3^{-1}$才是准确的。\n旋转矩阵是学习出来的，其流程是 WikiText2 校准数据集上的 800 个样本使用 Cayley 优化来更新旋转 100 次迭代。\nFig 6. 实验结果Comparison of the perplexity score on WikiText2 and averaged accuracy on Zero-shot Common Sense Reasoning tasks. 0-shot4 employs ARC-easy, ARC-challenge, PIQA, and WinoGrande tasks, while 0-shot8 adds BoolQ, SIQA, HellaSwag, and OBQA tasks. Results for SmoothQuant, LLM-QAT, GPTQ, and QuaRot were obtained using their publicly released codebase. While OmniQuant, AQLM, AWQ, QuIP, and QuIP#results were quoted from their papers. SpinQuant* and QuaRot* represent using RTN quantization, while SpinQuant and QuaRot denote using GPTQ weight quantization. Mean scores for SpinQuant, GPTQ, and QuaRot are reported from six trials. Full results, including error bars, are in the Appendix.\n结果是非常惊艳的，在W4A4KV4上SpinQuant算法的表现非常的优秀！！！在W4KV4的情况下不损失过多的性能，看得出SpinQuant在Outlier的剔除上有非常大的作用。\nRef:\nSaleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. Slicegpt: Compress large language models by deleting rows and columns. arXiv preprint arXiv:2401.15024, 2024. ","permalink":"https://chenghuawang.github.io/keep-moving-forward/papers/quant_quarot_spinquant/","summary":"0. 前言 本问分析的两篇文章是：\n2024-05, SpinQuant: LLM quantization with learned rotations from meta，一作是 Zechun Liu\n2024-04, QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs from ETH Zurich、EPFL、Microsoft Research、IST Austria \u0026amp; NeuralMagic\n这两篇文章以相同的视角去解决问题，并且量化后的模型保持了相当好的性能，应该就是未来模型量化的一个主要的跟进方向。QuaRot和SpinQuant可以算作是同一系列的工作，SpinQuant在QuaRot的基础上做了可学习旋转矩阵的改进。\n1. Computational Invariance Computational Invariance是Quarot和SpinQuant的基础。\nComputational Invariance定理[1]指出，可以使用正交矩阵对Transformer 中的权重和块间激活进行变换，而模型输出不变。这个定理是说，如果$W_{in}$是一个在transformer block(i.e. $W_k,W_q,W_v$等)左边的权重，我们可以左乘上一个正交的矩阵$Q$，为了在最后的结果里消除这个影响，我们可以在输出矩阵(i.e. $W_{out}, W_{down}$)右边乘上$Q_T$。\n尽管 RMSNorm 被应用于两个数据块之间，但只要 RMSNorm 数据块中没有重新缩放（在实际操作中，我们会先将任何重新缩放吸收到相邻的权重矩阵中），这一点还是适用的。从概念上讲，这是因为 RMSNorm 将激活值除以它们的模长，而对激活值应用旋转矩阵$Q$不会影响模长（因为正交矩阵的特性）：\n$$\\text{RMSNorm}(\\boldsymbol{X}) = \\text{RMSNorm}(\\boldsymbol{X\\boldsymbol{Q}^T})\\boldsymbol{Q}$$\n我们这里假设RMSNorm对激活值$\\boldsymbol{X}$的每一行做的操作都是$x_{i} \\larr x_i/ \\Vert x_I \\Vert$。这意味着，将输出矩阵乘以 $Q^T$ 会使线性层输出$XQ^T$，$XQ^T$被归一化，然后传入下一个区块，其输入权重矩阵现在是$QW$，因此该线性层不做任何修改就会输出原始激活。\n2. 🥕Quarot Quarot有两个阶段，一个阶段是在全精度下操作模型权重，并在模型的前向传递中插入两个额外的乘Hadamard矩阵操作；在第二个阶段使用现在的量化方法来量化夹在Hadamard矩阵中的模型权重，因为这些权重被削减了峰度，outliers减少，可以使量化的压力小很多。\nFig 1. QuaRot workflow 1 Fig 2. QuaRot workflow 2 Quarot文中的图片已经非常清晰的描绘了什么时候做旋转以及何时做量化和量化的数据流。SpinQuant和Quarot比较了实验结果，Quarot的实验结果请看第三章节的SpinQuant的表格。","title":"✅[April-May 2024] 模型量化之 🥕Quarot \u0026 SpinQuant"},{"content":"前言 mllm在最近(2024-08-09, PR112)合并进去了最新的NPU版本，该版本对应的是mllm的NPU相关工作的论文：Empowering 1000 tokens/second on-device LLM prefilling with mllm-NPU。本文将对mllm的NPU工作做解析。本文将首先回顾论文中的主要算法，然后对mllm的实现做自顶向下的分析。NPU相关工作的思路非常的好，文中给出的效果也非常惊艳，但是mllm中的实现实在是有点粗糙了（比如在关键部位传值而非引用，头文件里面无inline的普通函数等），接下来需要进行修整一番。\n算法回顾 Chunk Graph AlgorithmEmpowering 1000 tokens/second on-device LLM prefilling with mllm-NPU\nDynamic Ops是在CPU上运行的（为了精度问题），Static Ops是在NPU上运行的。 QNN目前似乎支持了Dynamic Tensor，但是Chunk的方案对于异构设备应该会带来更多的Overlap上的空间。 $$\\text{Attention} = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$$\nChunk Sharing Graph方案的总计算量是减少的。 我们不妨计算下CPU/NPU上的矩阵乘法的Flops。假设$Q,K,V$的形状都是$(B, H, S, D)$\nW/O Chunk：纯CPU计算\n$QK^T$的Flops是$2\\times S^2DBH$，Softmax后的结果乘上$V$，Flops是一样的。总的Flops是$4\\times S^2 DBH$\nW Chunk：Linear分块的计算量是一样的，就是注意力Block不一样\n假设Chunk Size是$C$。一个Sequence被分割成$\\frac{S}{C}$个Chunk。\n第一个Chunk的计算量是：$4\\times C^2 DBH$Flops\n第二个Chunk的计算量是：$4\\times 2 \\times C^2DBH$Flops\n总的Chunk计算量是：$4 \\times C^2DBH \\times \\sum_{i=1}^{\\frac{S}{C}} i = 2\\times CSDBH +2 \\times S^2DBH$\n再来分析下为什么计算量会变小：\n实际上，将Attention的输入X分为几个Chunk类似于Masked MatMul的 Early Exit。另：在普通手机上运行，Chunk Graph流水线能不能真的并行起来还存疑（可能会有CPU瓶颈+Layout转换瓶颈），笔者没有运行过。\n这么说的原因是在$Q\\times K^T$这一步，因为我们需要给上Mask，那么就有很多的元素是被重复计算的。所以在Flash Attention中有根据分块对应的Mask 来做 Early Exit 的操作。\n在使用了Chunk以后，$Q$的$\\text{Seq}$维度大小变小了，那么每次计算浪费的元素就少了。\n自顶向下分析，以Qwen为例 对于各个环节的数据类型和量化方法，本文不做过多分析，可以看文章中的描述。\n模型实现 Qwen的模型实现分为两个内容，CPU、NPU。我们先来看最主要的CPUNPU Attention部分的代码：\nstd::vector\u0026lt;NetTensor *\u0026gt; Qwen_CPUNPUAttention(Context *c, NetTensor *x, NetTensor *res, int embedding_size, int hidden_size, int head_size, int cache_max, string name, int seq, int chunk) { x = x-\u0026gt;view(1, static_cast\u0026lt;int\u0026gt;(seq / chunk / 32), static_cast\u0026lt;int\u0026gt;(32), hidden_size * head_size); auto *q = _LinearINT8({x}, embedding_size, hidden_size * head_size, true, name + \u0026#34;.q_proj\u0026#34;); auto *k = _LinearINT8({x}, embedding_size, hidden_size * head_size, true, name + \u0026#34;.k_proj\u0026#34;); auto *v = _LinearINT8({x}, embedding_size, hidden_size * head_size, true, name + \u0026#34;.v_proj\u0026#34;); q = q-\u0026gt;view(1, head_size, seq / chunk, hidden_size); k = k-\u0026gt;view(1, head_size, seq / chunk, hidden_size); v = v-\u0026gt;view(1, head_size, seq / chunk, hidden_size); q = _Dequantize({q}, true, (string)name + \u0026#34;.q_proj.dequantize\u0026#34;); k = _Dequantize({k}, true, (string)name + \u0026#34;.k_proj.dequantize\u0026#34;); v = _Dequantize({v}, true, (string)name + \u0026#34;.v_proj.dequantize\u0026#34;); v = _Transpose({v}, {0, 2, 3, 1}, (string)name + \u0026#34;.v_proj.transpose\u0026#34;); auto *m = _MergeOutput({q, k, v, res}, name + \u0026#34;.qkv_merge\u0026#34;); // -------------------- _SubgraphBegin(c, MLLM_CPU); // -------------------- auto s = _SplitInput({m}, true, 4, name + \u0026#34;.qkv_split\u0026#34;); q = s[0]; k = s[1]; v = s[2]; res = s[3]; q = _RoPE({q}, HFHUBROPE, name + \u0026#34;.q_rope\u0026#34;, 1000000, 32768); k = _RoPE({k}, HFHUBROPE, name + \u0026#34;.k_rope\u0026#34;, 1000000, 32768); k = _KVCacheNPU({k}, cache_max, name + \u0026#34;.k_cache\u0026#34;); v = _KVCacheNPU({v}, cache_max, name + \u0026#34;.v_cache\u0026#34;); auto *qk = _Matmul({q, k}, false, true, name + \u0026#34;.qk\u0026#34;); // qk = *qk / std::sqrt(hidden_size); // qk = _Causalmask({qk}, name + \u0026#34;.mask\u0026#34;); qk = _Softmax({qk}, DIMENSION, true, name + \u0026#34;.softmax\u0026#34;); auto *o = _Matmul({qk, v}, false, false, name + \u0026#34;.qkv\u0026#34;); o = _Quantize({o}, true, (string)name + \u0026#34;.o_proj.quantize\u0026#34;); m = _MergeOutput({o, res}, name + \u0026#34;.or_merge\u0026#34;); // -------------------- _SubgraphBegin(c); // -------------------- s = _SplitInput({m}, true, 2, name + \u0026#34;.or_split\u0026#34;); o = s[0]; res = s[1]; o = o-\u0026gt;view(1, static_cast\u0026lt;int\u0026gt;(seq / chunk / 32), static_cast\u0026lt;int\u0026gt;(32), hidden_size * head_size); res = res-\u0026gt;view(-1, 1, -1, hidden_size * head_size); o = _LinearINT8({o}, hidden_size * head_size, embedding_size, false, name + \u0026#34;.o_proj\u0026#34;); o = _Dequantize({o}, true, (string)name + \u0026#34;.o_proj.dequantize\u0026#34;); return {o, res}; } 在代码中，以_SubgraphBegin(c, ...);为分界线完成了论文中算法的主要思路。\n_SubgraphBegin(c, MLLM_CPU);以上的部分是NPU上执行的Shared QKV Linear Layer。SubgraphBegin(c, MLLM_CPU);以下的部分是在CPU上执行的$O = \\text{softmax}(\\text{mask}(\\frac{QK^T}{\\sqrt{d_k}}))V$的部分。之后的_SubgraphBegin(c);后实现了Shared O Linear Layer。\nmllm中的子图的实现方式对于我来说有点奇怪，应该是不能实现sub Graph in sub Graph。是在每次_SubgraphBegin(c);之后，所有新创建的算子都在这张top的子图上。\n其他的图构建方式和上述代码类似，这里就不赘述了。\nContext，Net.Convert，Executor 我们可以在main_qwen_npu.cpp中看到具体的网络执行流程：\n创建Context 创建计算图 Convert计算图到Net 创建Param Loader 创建Executor Context struct Context { vector\u0026lt;NetParameter\u0026gt; sub_param_; vector\u0026lt;BackendType\u0026gt; sub_backend_; BackendType next_backend = MLLM_DEFAULT; vector\u0026lt;NetOp *\u0026gt; net_ops; std::set\u0026lt;NetTensor *\u0026gt; net_tensors; int idx = 0; int active_sub = 0; }; 正如其名，用来记录网络参数、backend类型、算子、tensor等全局的参数。\nNet.Convert 默认的Net::Convert如下代码所示。它的作用是将网络参数转换为特定后端类型（将NetTensor这个Node抽象转换成实际执行的Tensor抽象），并进行拓扑排序，以构建计算图。\nvoid Net::convert(vector\u0026lt;NetParameter\u0026gt; \u0026amp;param, BackendType backend_type, int threadCount) { for (int ii = 0; ii \u0026lt; (int)param.size(); ++ii) { auto \u0026amp;sub_param = param[ii]; vector\u0026lt;string\u0026gt; names = {}; auto net_in_tensor = sub_param.net_inputs; for (const auto \u0026amp;out_t : net_in_tensor) { tensors_[out_t-\u0026gt;name] = std::make_shared\u0026lt;Tensor\u0026gt;(backends_[backend_type].get()); tensors_[out_t-\u0026gt;name]-\u0026gt;setName(out_t-\u0026gt;name); for (auto \u0026amp;tensor_name : tensor_names_) { tensor_name.erase(std::remove(tensor_name.begin(), tensor_name.end(), out_t-\u0026gt;name), tensor_name.end()); } names.push_back(out_t-\u0026gt;name); } for (auto *t:sub_param.net_tensors) { if(t-\u0026gt;in == NULL){ auto *in_tensor = t; tensors_[in_tensor-\u0026gt;name] = std::make_shared\u0026lt;Tensor\u0026gt;(backends_[backend_type].get()); tensors_[in_tensor-\u0026gt;name]-\u0026gt;setName(in_tensor-\u0026gt;name); input_names_.push_back(in_tensor-\u0026gt;name); inputname_graphidx_[in_tensor-\u0026gt;name] = ii; names.push_back(in_tensor-\u0026gt;name); } } tensor_names_.push_back(names); } for (int i = 0; i \u0026lt; (int)param.size(); ++i) { param[i].topologySort(); shared_ptr\u0026lt;Graph\u0026gt; subg_1; subg_1.reset(new Graph( param[i], backends_[backend_type].get(), tensors_, threadCount)); subGraphs_[\u0026#34;G\u0026#34; + std::to_string(i)] = subg_1; } } Executor Executor是一个对图执行逻辑的封装，见3.3章节的run\nQNNPipelineExecutor::run 首先明确，只有在Prefill阶段才会用到QNN。先来看下Chunk Graph是怎么运行起来的，第一个chunk运行完NPU的部分(也就是图中的绿色部分)后进入CPU部分，然后下一个chunk进入NPU部分，以此反复。目前我猜测，主要瓶颈可能还是在CPU上，导致NPU等待，并行度上不去。\nQNN Pipeline run 的代码在QNNExecutor.hpp/.cpp中，函数定义是\nvoid QNNPipelineExecutor::run(Context *ctx, Net *net, vector\u0026lt;shared_ptr\u0026lt;Tensor\u0026gt;\u0026gt; input_tensors) { ... } 我们先看拆分input的代码：\n// input will be split into chunks and execute in pipeline const int chunk_size = 32; int chunk_num = (input_tensors[0]-\u0026gt;sequence() + chunk_size - 1) / chunk_size; // create a new tensor for each chunk vector\u0026lt;vector\u0026lt;shared_ptr\u0026lt;Tensor\u0026gt;\u0026gt;\u0026gt; chunked_tensors_list(chunk_num, vector\u0026lt;shared_ptr\u0026lt;Tensor\u0026gt;\u0026gt;(input_tensors.size())); // split the tensor in chunks for (int i = 0; i \u0026lt; chunk_num; ++i) { // for all inputs in input_tensors auto \u0026amp;chunked_tensors = chunked_tensors_list[i]; for (int j = 0; j \u0026lt; input_tensors.size(); ++j) { chunked_tensors[j] = std::make_shared\u0026lt;Tensor\u0026gt;(); chunked_tensors[j]-\u0026gt;setBackend(net-\u0026gt;backends()[BackendType::MLLM_CPU].get()); chunked_tensors[j]-\u0026gt;reshape(1, 1, chunk_size, 1); chunked_tensors[j]-\u0026gt;setName(net-\u0026gt;inputNames()[j]); // use deepCopyFrom for each chunk to avoid memcpy chunked_tensors[j]-\u0026gt;deepCopyFrom(input_tensors[j].get(), false, {0, 0, i * chunk_size, 0}); } } 作者假设了这里的input_tensors是Seqence长度一致或者是只有一个元素的。代码逻辑简单，就是对每个input_tensors中的tensor拆分成为chunks。\n再来看计算图修改inputs的代码：\nvector\u0026lt;int\u0026gt; flashGid = {}; for (int tid = 0; tid \u0026lt; net-\u0026gt;inputNames().size(); ++tid) { auto input_name = net-\u0026gt;inputNames()[tid]; auto input_tensor = chunked_tensors_list[0][tid]; input_tensor-\u0026gt;setName(input_name); net-\u0026gt;tensors()[input_name] = input_tensor; if (std::find(flashGid.begin(), flashGid.end(), net-\u0026gt;inGmap()[input_name]) == flashGid.end()) { flashGid.push_back(net-\u0026gt;inGmap()[input_name]); } } for (auto Gid : flashGid) { net-\u0026gt;subGraph()[graphNamingRule(Gid)]-\u0026gt;reflashInput(net-\u0026gt;tensors()); } 因为切分了Seq为32一个chunk的tensor，这里把每个子图的inputs重新设置为seq=32的tensor。这里的inGmap函数就是直接传值的，可以改为引用。我们可以看到这里使用的是第一个chunk：chunked_tensors_list[0][tid];，这是因为在下文中，会有在子线程内根据线程id对自己的input进行更新的操作，即下文中(i == 0)的scope内部的操作。\n图改写完成后，就是offload计算图到对应的Backend：\nfor (int i = 0; i \u0026lt; (int)net-\u0026gt;subGraph().size(); ++i) { string name = graphNamingRule(i); auto \u0026amp;g = net-\u0026gt;subGraph()[name]; // cast graph to QNNGraph // the qnn_graph below is where we cast the Graph to QNNGraph auto expectedBackend = ctx-\u0026gt;sub_backend_[i]; if (graphOffloadRule(expectedBackend, i) == MLLM_CPU) { g-\u0026gt;reshape(); g-\u0026gt;setUpTensors(); } else if (graphOffloadRule(expectedBackend, i) == MLLM_QNN) { auto *qnn_graph = dynamic_cast\u0026lt;QNNGraph *\u0026gt;(g.get()); g-\u0026gt;reshape(); qnn_graph-\u0026gt;setUpTensors(name); } else { std::cerr \u0026lt;\u0026lt; \u0026#34;Backend Not Support\u0026#34; \u0026lt;\u0026lt; std::endl; exit(1); } } 接下来就是具体执行的代码了，作者在这里写了个chunkExecutionFunction:\nstd::function\u0026lt;void(int chunk_id)\u0026gt; chunkExecutionFunction = [\u0026amp;](int chunk_id) { for (int i = 0; i \u0026lt; (int)net-\u0026gt;subGraph().size(); ++i) { // make sure chunks execute by order while (true) { chunk_mutex.lock(); if (graph_chunk_index[i] == chunk_id) { graph_chunk_index[i]++; chunk_mutex.unlock(); break; } else { chunk_mutex.unlock(); std::this_thread::yield(); } } // make sure current graph is ready for this chunk // lock the mutex of mutexes at i mutexes[i].lock(); if (i == 0) { // update the input tensor for each chunk for (int tid = 0; tid \u0026lt; net-\u0026gt;inputNames().size(); ++tid) { auto input_name = net-\u0026gt;inputNames()[tid]; auto input_tensor = chunked_tensors_list[chunk_id][tid]; unordered_map\u0026lt;string, shared_ptr\u0026lt;Tensor\u0026gt;\u0026gt; map; map[input_name] = input_tensor; string graphName = graphNamingRule(i); net-\u0026gt;subGraph()[graphName]-\u0026gt;reflashInput(map); } } auto expectedBackend = ctx-\u0026gt;sub_backend_[i]; string name = graphNamingRule(i); if (graphOffloadRule(expectedBackend, i) == MLLM_CPU) { // execute only one cpu graph at a time cpu_mutex.lock(); #ifdef DEBUGPRINT std::cout \u0026lt;\u0026lt; \u0026#34;chunk:\u0026#34; \u0026lt;\u0026lt; chunk_id \u0026lt;\u0026lt; \u0026#34; execute cpu graph \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; #endif auto \u0026amp;g = net-\u0026gt;subGraph()[name]; if (chunk_id != 0) { // cpu graph should reshape and setup for every chunk forward for KVCache op g-\u0026gt;reshape(); g-\u0026gt;setUpTensors(); } // only get the result at the last graph if (i == net-\u0026gt;subGraph().size() - 1) { chunked_result_list = g-\u0026gt;forward(); } else { g-\u0026gt;forward(); } // execute only one cpu graph at a time cpu_mutex.unlock(); } else if (graphOffloadRule(expectedBackend, i) == MLLM_QNN) { #ifdef DEBUGPRINT std::cout \u0026lt;\u0026lt; \u0026#34;chunk:\u0026#34; \u0026lt;\u0026lt; chunk_id \u0026lt;\u0026lt; \u0026#34; execute qnn graph \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; #endif auto \u0026amp;g = net-\u0026gt;subGraph()[name]; auto *qnn_graph = dynamic_cast\u0026lt;QNNGraph *\u0026gt;(g.get()); qnn_graph-\u0026gt;forward(name); // only get the result at the last graph if (i == net-\u0026gt;subGraph().size() - 1) { chunked_result_list = qnn_graph-\u0026gt;forward(name); } else { qnn_graph-\u0026gt;forward(name); } } else { std::cerr \u0026lt;\u0026lt; \u0026#34;Backend Not Support\u0026#34; \u0026lt;\u0026lt; std::endl; exit(1); } PRINT_MEMORY_USAGE((string(\u0026#34;execute graph: \u0026#34;) + std::to_string(i)).c_str()); // if it is the last graph, move the result to the final result if (i == (int)net-\u0026gt;subGraph().size() - 1) { result_.resize(chunked_result_list.size()); if (chunk_id == 0) { // reshape the result tensor when first chunk is executed for (int tid = 0; tid \u0026lt; chunked_result_list.size(); ++tid) { result_[tid] = std::make_shared\u0026lt;Tensor\u0026gt;(); result_[tid]-\u0026gt;setBackend(net-\u0026gt;backends()[BackendType::MLLM_CPU].get()); result_[tid]-\u0026gt;reshape(chunked_result_list[tid]-\u0026gt;batch(), chunked_result_list[tid]-\u0026gt;head(), chunk_size * chunk_num, chunked_result_list[tid]-\u0026gt;dimension()); result_[tid]-\u0026gt;alloc(); } } // move the result to the final result for (int tid = 0; tid \u0026lt; chunked_result_list.size(); ++tid) { auto \u0026amp;result_tensor = chunked_result_list[tid]; memcpy(result_[tid]-\u0026gt;ptrAt\u0026lt;float\u0026gt;(0, 0, chunk_size * chunk_id, 0), result_tensor-\u0026gt;hostPtr\u0026lt;float\u0026gt;(), result_tensor-\u0026gt;count() * sizeof(float)); } } // unlock the mutex of mutexes at i mutexes[i].unlock(); } }; 代码中，开头的while循环是检查现在的chunk是否可以被执行，如果不可以被执行则yield这个线程。接下来如果是第一个子图的话，就是首先更新当前子图需要的chunk inputs。接下来就分别按照需求在CPU/NPU上进行运算。如果是最后的一个子图，就将运算的结果写在一个最终的result中，这个result是没有chunk的大小，写入的时候使用offset来指定写入的位置。\n然后会有一个ThreadPool来执行这几个chunck子图：\n// wrap the thread pool execution in a function and await the thread pool to finish std::function executeFunction = [\u0026amp;]() { // use thread pool to manage the threads ThreadPool thread_pool(4); for (int i = 0; i \u0026lt; chunk_num; ++i) { thread_pool.enqueue(std::bind(chunkExecutionFunction, i)); } }; executeFunction(); QNN Backend QNN的使用还不熟悉，先学习先。\n改进 易用性 可以改进前端实现和模型执行的流程，对于子图，更希望是torch中的形式，一个Module可以视为一个子图，使用to_device()等函数来标明运行设备。笔者最近在实现一个Eager+Lazy+Static合一的计算图构建模式，可能可以借鉴进去。\n子图Dispatch是手动的，有完整的计算图，还是用自动的Dispatch算法吧，通用性会更好点。\n代码中有多处shared_ptr和裸指针混用问题，继承问题可以用enable_shared_from_this\u0026lt;T\u0026gt;和static_ptr_cast来解决的。\n性能问题 我严重怀疑存在CPU瓶颈+Layout/精度转换瓶颈。导致Chunk之间是没法完全并行起来的。这是最主要的问题。\n最好使用细粒度的Thread Dispatch方法，OpenMP的线程可能与当前Chunk Thread Pool中的线程干扰。\nNPU构建图的时候需要不少的Memory，导致峰值内存需求大，可能会使用Swapping memory。\n","permalink":"https://chenghuawang.github.io/keep-moving-forward/tech/mllm-qwen-2/","summary":"以Qwen0.5B为例解析mllm的基本实现，NPU Backend","title":"mllm框架浅析(二)-QNN-Backend"},{"content":"0x00 参考资料 本文是对RoPE的学习笔记，其中有不少内容是摘自业内的前辈们的文章，在此一并感谢。所参考的资料、摘录的文章来源在下面列出：\n苏老师的Blog，Transformer升级之路：2、博采众长的旋转式位置编码\nReformer，Github repo\n视屏讲解，【解密旋转位置编码：数学基础、代码实现与绝对编码一体化探索】 0x01 为什么需要位置编码 在自然语言处理中，Attention机制是一种用于建立输入序列中不同元素间关联的方法，它允许模型在处理序列数据时，能够关注到当前元素之外的其他元素。然而，原始的Attention机制存在一个问题：它没有考虑序列中元素的顺序信息。 在处理序列数据时，元素的顺序是非常重要的，因为不同的顺序可能会表达完全不同的意义。例如，在句子中，\u0026ldquo;I saw the man with the telescope\u0026rdquo; 和 \u0026ldquo;The man saw I with the telescope\u0026rdquo; 虽然使用了相同的词汇，但意义却大相径庭，这正是因为词序不同。\n为了解决这个问题，引入了位置编码（Positional Encoding）。位置编码是一种向量，它与原始的输入序列中的每个元素相乘，为模型提供关于每个元素在序列中位置的信息。这样，即使在计算Attention权重时，模型也能够区分不同位置的元素，从而更好地理解序列数据中的顺序信息。\n0x02 绝对位置编码 可学习的绝对位置编码 BERT模型中引入了可学习的绝对位置编码（position embeddings），其目的是在模型中为每个位置提供序列中单词的位置信息。这种位置编码是模型参数的一部分，可以在训练过程中自动学习得到最优的表示。具体来说，BERT使用了一个可训练的嵌入层来生成位置编码，这使得模型能够捕捉到序列中单词的顺序信息，从而更好地理解句子结构和语义。\n但是，可学习的位置编码受长度限制，无法应用在长文本。\nSinusoidal位置编码 Sinusoidal位置编码的基本思想是利用正弦和余弦函数的周期性来编码位置信息。具体来说，对于序列中的每个元素，位置编码会生成一个与位置相关的向量：\n$$PE_{(p, 2i)} = \\sin\\left(\\frac{p}{10000^{(i/d)}}\\right)$$\n$$PE_{(p, 2i+1)} = \\cos\\left(\\frac{p}{10000^{(i/d)}}\\right)$$\n其中，$p$表示位置，$i$表示维度信息。\n0x03 RoPE 这里的公式借用苏老师文章中的用法。假设我们需要对$\\boldsymbol{q},\\boldsymbol{k}$添加上位置信息$m,n$，我们可以假设有$\\boldsymbol{f}(\\cdot, \\text{pos})$这样的操作可以做到这一点：\n$$\\tilde{\\boldsymbol{q}}_m=\\boldsymbol{f}(\\boldsymbol{q},m),\\quad\\tilde{\\boldsymbol{k}}_n=\\boldsymbol{f}(\\boldsymbol{k},n)$$\n在Attention操作中，$q,k^*$会做内积，我们希望在做内积的时候可以体现出相对位置关系，也就是下面的式子展示的：\n$$\\langle \\boldsymbol{f}(\\boldsymbol{q},m), \\boldsymbol{f}(\\boldsymbol{k},n) \\rangle = \\boldsymbol{g}(\\boldsymbol{q},\\boldsymbol{k}, m-n)$$\n其中$m-n$就是相对位置关系。\n$\\langle \\boldsymbol{x}, \\boldsymbol{y} \\rangle$表示希尔伯特空间的内积，即高维度欧几里得空间。\n那么接下来的问题就是如何找到这样的$\\boldsymbol{f}$以满足我们上面的假设。为了简化问题，我们不妨假设\n$$\\boldsymbol{f}(\\boldsymbol{q},0) = q, \\boldsymbol{f}(\\boldsymbol{k},0)=k$$。\n我们可以借用复数的概念来求解这个问题，在此，我们先考虑二维情况下。\n在复数中有$\\langle \\boldsymbol{q} ,\\boldsymbol{k} \\rangle = \\text{Re}[\\boldsymbol{q} \\boldsymbol{k}^*]$，即\n$$\\text{Re}[ \\boldsymbol{f}(\\boldsymbol{q},m)\\boldsymbol{f}^*(\\boldsymbol{k},n)]= \\boldsymbol{g}(\\boldsymbol{q},\\boldsymbol{k}, m-n)$$\n为了简化问题，我们假设存在复数$\\boldsymbol{g}(\\boldsymbol{q},\\boldsymbol{k}, m-n)$使得：\n$$\\boldsymbol{f}(\\boldsymbol{q},m)\\boldsymbol{f}^*(\\boldsymbol{k},n)= \\boldsymbol{g}(\\boldsymbol{q},\\boldsymbol{k}, m-n)$$\n我们使用复数的指数形式可以得到：\n$$\\boldsymbol{f}(\\boldsymbol{q},m) = R_f(\\boldsymbol{q},m)e^{i \\Theta_f(\\boldsymbol{q},m)}$$\n$$\\boldsymbol{f}(\\boldsymbol{k},n) = R_f(\\boldsymbol{k},n)e^{i \\Theta_f(\\boldsymbol{k},m)}$$\n$$\\boldsymbol{g}(\\boldsymbol{q},\\boldsymbol{k}, m-n) = R_g(\\boldsymbol{q},\\boldsymbol{k},m-n)e^{i \\Theta_g(\\boldsymbol{q},\\boldsymbol{k},m-n)}$$\n带入之前的方程后可以得到下述方程组：\n$$R_f(\\boldsymbol{q},m)R_f(\\boldsymbol{k},n)=R_g(\\boldsymbol{q},\\boldsymbol{k},m-n)$$\n$$\\Theta_f(\\boldsymbol{q},m) - \\Theta_f(\\boldsymbol{k},n) = \\Theta_g(\\boldsymbol{q},\\boldsymbol{k},m-n)$$\n对于上述两个方程，我们带入$m=n$，由于一开始的假设$\\boldsymbol{f}(\\boldsymbol{q},0) = q, \\boldsymbol{f}(\\boldsymbol{k},0)=k$，我们可以得到下面的方程:\n$$R_f(\\boldsymbol{q},m)R_f(\\boldsymbol{k},m)=R_g(\\boldsymbol{q},\\boldsymbol{k},0)=R_f(\\boldsymbol{q},0)R_f(\\boldsymbol{k},0)=\\Vert \\boldsymbol{q} \\Vert \\Vert \\boldsymbol{k} \\Vert$$\n$$\\Theta_f(\\boldsymbol{q},m) - \\Theta_f(\\boldsymbol{k},m) = \\Theta_g(\\boldsymbol{q},\\boldsymbol{k},0)=\\Theta_f(\\boldsymbol{q},0) - \\Theta_f(\\boldsymbol{k},0)=\\Theta_f(\\boldsymbol{q}) - \\Theta_f(\\boldsymbol{k})$$\n现在，对于第一个式子，我们简单的假设\n$$R_f(\\boldsymbol{q},m)=\\Vert \\boldsymbol{q} \\Vert,R_f(\\boldsymbol{k},n)=\\Vert \\boldsymbol{k} \\Vert$$\n对于第二个式子，经过变换得到：\n$$\\Theta_f(\\boldsymbol{q},m) - \\Theta_f(\\boldsymbol{q}) = 0,\\Theta_f(\\boldsymbol{k},m) - \\Theta_f(\\boldsymbol{k}) = 0$$\n所以，$\\Theta_f(\\boldsymbol{q},m) - \\Theta_f(\\boldsymbol{q})$是一个与$\\boldsymbol{q}$无关，与$\\boldsymbol{m}$相关的一个函数，我们设它为$\\varphi(m)$，即:\n$$\\Theta_f(\\boldsymbol{q},m) = \\Theta_f(\\boldsymbol{q}) + \\varphi(m)$$\n此时，\n$$\\varphi(m) - \\varphi(m-1) = \\Theta(\\boldsymbol{q},\\boldsymbol{k},1)+\\Theta(\\boldsymbol{k}) - \\Theta(\\boldsymbol{q})$$\n即${\\varphi(m)}$是等差数列，设右端是$\\theta$，那么$\\varphi(m) = \\theta m$\n此时就得到了二维情况下的RoPE：\n$$\\boldsymbol{f}(\\boldsymbol{q}, m) = \\boldsymbol{q}e^{im\\theta}$$\n这实际上就是对于$\\boldsymbol{q}$的旋转公式：\n$$\\boldsymbol{f}(\\boldsymbol{q}, m) = \\begin{pmatrix} \\cos m\\theta \u0026 -\\sin m \\theta \\\\ \\sin m \\theta \u0026 \\cos m \\theta \\end{pmatrix} \\begin{pmatrix} q_0 \\\\ q_1 \\end{pmatrix}$$ $\\text{Re}$表示复数的Real部分。 $Z = \\text{a} + \\text{b}i=r(\\cos(\\Theta) + i\\sin(\\Theta)) = re^{i\\Theta}$，复数形式变换\n由于内积满足线性叠加性，因此任意偶数维的RoPE，我们都可以表示为二维情形的拼接\n$$\\underbrace{\\begin{pmatrix}\\cos m\\theta_0\u0026-\\sin m\\theta_0\u00260\u00260\u0026\\cdots\u00260\u00260\\\\\\sin m\\theta_0\u0026\\cos m\\theta_0\u00260\u00260\u0026\\cdots\u00260\u00260\\\\0\u00260\u0026\\cos m\\theta_1\u0026-\\sin m\\theta_1\u0026\\cdots\u00260\u00260\\\\0\u00260\u0026\\sin m\\theta_1\u0026\\cos m\\theta_1\u0026\\cdots\u00260\u00260\\\\\\vdots\u0026\\vdots\u0026\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\u0026\\vdots\\\\0\u00260\u00260\u00260\u0026\\cdots\u0026\\cos m\\theta_{d/2-1}\u0026-\\sin m\\theta_{d/2-1}\\\\0\u00260\u00260\u00260\u0026\\cdots\u0026\\sin m\\theta_{d/2-1}\u0026\\cos m\\theta_{d/2-1}\\end{pmatrix}}_{\\mathbf{W}_m}\\begin{pmatrix}q_0\\\\q_1\\\\q_2\\\\q_3\\\\\\vdots\\\\q_{d-2}\\\\q_{d-1}\\end{pmatrix}$$ 可以看到，RoPE形式上和Sinusoidal位置编码有点相似，只不过Sinusoidal位置编码是加性的，而RoPE可以视为乘性的。\n对于远程衰减，在$\\theta_i$的选择上，RoPEs同样沿用了Sinusoidal位置编码的方案，即:\n$$\\theta_i = 10000^{-\\frac{2i}{d}}$$\n远程衰减： 对于$q,k$，我们希望距离近的$q,k$有较大的相关性，距离远的$q,k$相关性小。\n0x04 思考 既然是旋转，会出现旋转角度重复的现象吗？ 在任意的第$k$个子空间上，只要$\\theta_k$中不包含$\\pi$，那么旋转角度序列${ i \\theta_k }$就不会有角度重复。\n$\\theta_i$中10000的设置会影响外推的性能吗？ 是会的，较小的值会造成外推性能严重下降。可以把旋转角度的函数图画出来，或者设置一个$q,k$算一下就明了了。\n使用$\\text{base}=10000$，跑出下图的结果：\nFig 1. $\\text{Base}=10000$ 而$\\text{base}=50000$和$\\text{base}=10000$比，远程衰减的效果有降低：\nFig 2. $\\text{Base}=50000$ 目前很多大长度外推的模型都是通过调大base来提升模型的输入长度。\n二维的子空间可以是任意的吗 二维的子空间可以是任意的，只要是成对的就可以了，无需按照文章中所说的来。像HF Llama中的那样，就是half分割的形式。\n0x05 代码实现 参考的是HF的Llama RoPE实现。\n在实现的过程中，我们需要避开旋转矩阵的相乘，因为旋转矩阵是非常稀疏的。在论文中，作者使用的是：\nFig 3. 原文子空间的选择方法${0, 1, 2, 3}$ -\u0026gt; { 0, 1 }, { 2, 3 }\n但是实际上，如HF Llama，其使用的是：\nFig 4. HF Llama 子空间的选择方法${0, 1, 2, 3}$ -\u0026gt; { 0, 2 }, { 1, 3 }\n两者都是合理的实现，只是子空间的划分不同。对于half划分，在HF llama的实现中是这样的：\ndef rotate_half(x): \u0026#34;\u0026#34;\u0026#34;Rotates half the hidden dims of the input.\u0026#34;\u0026#34;\u0026#34; x1 = x[..., : x.shape[-1] // 2] x2 = x[..., x.shape[-1] // 2 :] return torch.cat((-x2, x1), dim=-1) 在乘上$\\cos,\\sin$后是：\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1): \u0026#34;\u0026#34;\u0026#34;Applies Rotary Position Embedding to the query and key tensors. Args: q (`torch.Tensor`): The query tensor. k (`torch.Tensor`): The key tensor. cos (`torch.Tensor`): The cosine part of the rotary embedding. sin (`torch.Tensor`): The sine part of the rotary embedding. position_ids (`torch.Tensor`, *optional*): Deprecated and unused. unsqueeze_dim (`int`, *optional*, defaults to 1): The \u0026#39;unsqueeze_dim\u0026#39; argument specifies the dimension along which to unsqueeze cos[position_ids] and sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2. Returns: `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding. \u0026#34;\u0026#34;\u0026#34; cos = cos.unsqueeze(unsqueeze_dim) sin = sin.unsqueeze(unsqueeze_dim) q_embed = (q * cos) + (rotate_half(q) * sin) k_embed = (k * cos) + (rotate_half(k) * sin) return q_embed, k_embed 在$\\cos,\\sin$的生成上就不使用Llama的代码了，它封装的太多了，我自己写了一个：\nbase = 1e5 d = D / 2 B = base ** (1/d) theta_beta = 1.0 / (B ** torch.arrange(0, d)) theta_0 = q.outer(theta_beta) theta = torch.concat([theta_0, theta_0], dim=-1) cos = theta.cos() sin = theta.sin() ","permalink":"https://chenghuawang.github.io/keep-moving-forward/tech/fundamental_rope/","summary":"RoPE from Fundamental Series","title":"[Fundamental] 旋转位置编码(RoPE)"},{"content":"0x00 Materials 本文是对Flash Attention的学习笔记，其中有不少内容是摘自业内的前辈们的文章，在此一并感谢。所参考的资料、摘录的文章来源在下面列出：\nFrom Online Softmax to FlashAttention(CSE599m, ML for ML System) ,本文的行文逻辑也是按照这篇文章来的。强烈安利CSE599m给入门ML System的新人。\nFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\nFlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\n【BBuf的CUDA笔记】十四，OpenAI Triton入门笔记三 FusedAttention\n[Attention优化][2w字]🔥原理\u0026amp;图解: 从Online-Softmax到FlashAttention V1/V2/V3\n0x01 问题定义 $$ \\text{Attention} = \\text{Softmax}(\\frac{QK^T}{\\sqrt{d_k}})V $$\n$$ Q,K,V \\in \\mathbb{R}^{N\\times D} $$\n其中$N$表示Sequence Length,$D$表示Dimension。我们先来考虑最简单的计算方式：\n$$ S = QK^T $$\n$$ P = \\text{Softmax}(S) $$\n$$ O = PV $$\n在这个Naive的计算方式中，$P,S \\in \\mathbb R^{N\\times N}$，这意味着为了计算P，我们需要多保存一个$N\\times N$的矩阵，这个情况下内存的需求是$O(N^2)$的，很容易爆显存；且为了计算$S和P$势必需要从HBM中进行大量的读写操作，IO的访问次数是$O(N^2 + ND)$复杂度的。随着现在的Context Length需求越来越大，在$N$变大的时候，是很容易爆显存的。总结问题，主要有：\nSequence Length($N$)越大，传统的Attention计算方法很容易爆显存。 传统的Attention计算方式对HBM的访问复杂度是平方级别的，越长的$N$，耗时越长。 HBM / Shared Mem IO BandwidthFrom FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n考虑到HBM，Shared Memory的速度差异，我们希望能够减少HBM Access而将更多的IO Access操作放在Shared Memory中。\n0x02 Online Softmax 我们再来看下Safe Softmax的逻辑:\n$$ S_i = \\frac{e^{x_i - M}}{\\sum_{i=0}^N e^{x_i - M}}, M = \\max{X},X \\in \\mathbb{R}^{N} $$ 。我们先用一个非常naive的思路来实现这个Softmax，这里使用From Online Softmax to FlashAttention文章中的伪代码来解释：\nOnline Softmax 伪代码From From Online Softmax to FlashAttention(CSE599m, ML for ML System)\n在这个简单的例子中，我们使用了三个循环来进行计算，这要求我们对$[1; N]$进行三次迭代。而Self-Attention中，因为SRAM放不下那么多的数据，所以我们需要三次访问$Q$和 $K$（并且重新计算），这在$I/O$效率上是不利的。\n那么，有没有一种方法可以合并一些Pass，就像是我们经常在Kernel Fusion中做的那样呢？初看似乎困难，因为公式(8)依赖于公式(7)所得到的计算结果，但是，使用一些变换，可以允许我们以重计算一部分数据为代价来合并公式(7, 8)。\n现在，我们来推导下公式，\n$$ \\begin{aligned} d_{i}^{\\prime}\u0026amp; =\\sum_{j=1}^ie^{x_j-m_i} \\\\ \u0026amp;= \\left(\\sum_{j=1}^{i-1} e^{x_j-m_i}\\right)+e^{x_i-m_i} \\\\ \u0026amp;= \\left(\\sum_{j=1}^{i-1} e^{x_j-m_{i-1}}\\right)e^{m_{i-1}-m_i}+e^{x_i-m_i} \\\\ \u0026amp;= d_{i-1}\u0026rsquo; e^{m_{i-1}-m_i}+e^{x_i-m_i} \\end{aligned} $$\n我们可以得到一个递推的公式，其中$d_N^{\\prime}$为最后我们需要的加和，即$\\sum_{i=0}^{N}e^{x_i-m_N}$。在这个递推公式中，我们使用新的$m$来修正之前的$d_i^{\\prime}$，之前错误的$m$可以通过幂相乘的计算规则消去。总的计算流程被缩减为2个Pass，如下图所示：\nOnline Softmax 2 passes 伪代码From From Online Softmax to FlashAttention(CSE599m, ML for ML System)\n但是，这个计算方式还是有两个Pass，我们能不能将所有的计算Fuse到一个Pass中去呢？\n在Online Softmax中很难做到这一点，因为$a_i$所需要的$m_N,d_N^{\\prime}$依赖于全局更新。而$a_i$是一个无法全局更新的变量，除非在第一个Pass中再嵌套一个循环，这样违背了我们简化计算的初衷。但是，将问题放在Self-Attention的计算的时候，就变得不一样了。\n我们在这里再理解下，为什么2Pass的Online Safe Softmax是重要的，在Self-Attention的计算中，我们有下面2个主要的问题：\n需要提前计算好$QK^T$，保存在全局显存中，需要$O(N^2)$的显存，容易爆显存。 在算法中Online计算，每次循环中去加载一部分$Q,K$到片上内存，计算得到部分的$QK^T$。 总的来说，Online Softmax解决的是显存不足的问题，但是因为有两个Pass，还是存在HBM R/W次数较多，有Memory Bound，所以我们需要消除这个瓶颈。虽然现在我们需要对每一个$d_i^{\\prime}$做Scale，但是考虑到目前显卡并不是Compute Bound，这多余的计算是可以暂时不去考虑的。\n0x03 FA1 虽然在Online Softmax中，我们没有办法得到一个1 Pass的算法，但是在Self-Attention中，我们需要的是计算出$O=A\\times V$，而不是$A$，这有什么不同呢？我们来推导下公式，不过首先，我们先来看一下原始的Self-Attention是怎么求解的：\n原始的Self-Attention 伪代码From From Online Softmax to FlashAttention(CSE599m, ML for ML System)\n这张未打码流程图仍然是从CSE 599m中借用的。可以看到，在第一个Pass中，就是0x02章节中提及的Online Softmax；在第二个Pass中，$o_i$的计算可能稍有点难以理解，可以画张图。实际上就是遍历$a_i$就是$\\text{Attention}$矩阵的一行，拿每一行的每个值$a_i$去乘$V$矩阵的每一行，就是行乘列操作。这个操作可以同时把$O$矩阵的一行给算出来。\n$$o_i^{\\prime}:=\\left(\\sum_{j=1}^i\\frac{e^{x_j-m_i}}{d_i^{\\prime}}V[j,:]\\right)$$\n上面的公式就是把Pass2内部的计算整合在了一起，和0x02章节的推导一样，我们也去尝试做递推：\n$$ \\begin{aligned} o_i^{\\prime}\u0026 =\\sum_{j=1}^i\\frac{e^{x_j-m_i}}{d_i'}V[j,:] \\\\ \u0026= \\left(\\sum_{j=1}^{i-1}\\frac{e^{x_j-m_i}}{d_i'}V[j,:] \\right)+\\frac{e^{x_i-m_i}}{d_i'}V[i,: ] \\\\ \u0026= \\left(\\sum_{j=1}^{i-1}\\frac{e^{x_j-m_{i-1}}}{d_{i-1}^{\\prime}}\\frac{e^{x_j-m_i}}{e^{x_j-m_{i-1}}}\\frac{d_{i-1}^{\\prime}}{d_i^{\\prime}}V[j,.]\\right)+\\frac{e^{x_i-m_i}}{d_i^{\\prime}}V[i,.] \\\\ \u0026= \\left(\\sum_{j=1}^{i-1}\\frac{e^{x_j-m_{i-1}}}{d_{i-1}^{\\prime}}V[j,:]\\right)\\frac{d_{i-1}^{\\prime}}{d_i^{\\prime}}e^{m_{i-1}-m_i}+\\frac{e^{x_i-m_i}}{d_i^{\\prime}}V[i,:] \\\\ \u0026=\\begin{array}{c}\\boldsymbol{o}_{i-1}^{\\prime}\\frac{d_{i-1}^{\\prime}e^{m_{i-1}-m_i}}{d_i^{\\prime}}+\\frac{e^{x_i-m_i}}{d_i^{\\prime}}V[i,:]\\end{array} \\end{aligned} $$ 可以推导出和Online Softmax相似的形式，至此，我们推导出了FA算法。\nFA1 伪代码From From Online Softmax to FlashAttention(CSE599m, ML for ML System)\n可以看出，在FA算法中，$Q,K,V$都可以分块载入，我们可以进一步得到FA的Tiling方法：\nFA1 TiledFrom From Online Softmax to FlashAttention(CSE599m, ML for ML System)\n在这种改进的Tiling技术中，K矩阵被划分为多个较小的区块，同样的方法也适用于Q矩阵。这些较小的区块可以被加载到SRAM中，以便于进行高效的计算。一旦这些区块被加载，就可以在kernel内部完成整个注意力机制的计算过程。从算法的角度来看，现在只需要一次性加载Q、K、V矩阵，就能在内核中完成所有的注意力计算。这种优化方法将原始3-pass Self Attention转变为1-pass FlashAttention，不仅节省了存储中间矩阵所需的显存，还减少了对Q和K矩阵的HBM R/W的次数。\n最终，FA的算法可以被下面的伪代码来表示：\nFA1 tiled 伪代码From From Online Softmax to FlashAttention(CSE599m, ML for ML System)\n此时，我们再看FA的算法流程图，就不感觉陌生了。和上文中的推导思路一致：\nFA1 原文 伪代码From FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n在第6行，FA载入$K,V$分块，然后在第8行遍历完成所有的$Q$（这里有个显而易见的问题，$Q$的遍历放在最外面会好很多）。我们在这里再探讨下为什么分块$B_c=\\lceil \\frac{M}{4d} \\rceil, B_r=\\min (\\lceil \\frac{M}{4d} \\rceil, d)$。\n这样设置的目的是，为了确保SRAM能够放下所有$Q, K, V$的小块，其中$M$就是系统可用的SRAM上限。那么，对于每一个$Q$的分块$Q_i,O_i$以及$K, V$的分块$K_i, V_i$需要的共享内存为：\n$$ \\begin{gathered} SRAM(Q_{i})=B_{r}\\times d=\\min\\left(\\left\\lceil\\frac{M}{4d}\\right\\rceil,d\\right)\\times d\u0026lt;\\lceil\\frac{M}{4}\\rceil \\\\ SRAM(O_i)=B_r\\times d=\\min\\left(\\left\\lceil\\frac{M}{4d}\\right\\rceil,d\\right)\\times d\u0026lt;\\lceil\\frac{M}{4}\\rceil \\\\ SRAM(K_{j},V_{j})=2\\times B_{c}\\times d=2\\times\\left\\lceil\\frac{M}{4d}\\right\\rceil\\times d\u0026lt;\\lceil\\frac{M}{2}\\rceil \\end{gathered} $$\n在这个情况下，SRAM基本上可以被占满。FA1原始论文中说道，Block Size 越大，HBM Accesses 越低，在256附近基本就是效率最优的转折点。\nFA1 Block Size 实验From FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n文中的实验条件是A100GPU，GPT-2 medium (seq. length 1024, head dim. 64, 16 heads, batch size 64)\n0x04 FA2 在0x03章节中我们提到：然后在第8行遍历完成所有的$Q$（这里有个显而易见的问题，$Q$的遍历放在最外面会好很多），这点就是FA2优化的很重要的一点。\nFA2一共做了主要的几种优化：\n优化了Scale的时机，使得除法的次数被大大减少\nForward优化了循环的顺序，使得HBM Access更加的高效。Backward没有\nForward/Backward均增加了Seq维度的并行\nWarp的分配更加的合理，避免Split-K(不是很理解？)\n优化了Scale的时机，使得除法的次数被大大减少 虽然一般来说，非matmul运算FLOPs要比matmul低，但是非matmul计算使用的是CUDA Cores，而矩阵计算可以利用Tensor Cores加速。基于Tensor Cores的matmul运算吞吐是不使用Tensor Cores的非matmul运算吞吐的16x。\n与FA1相比，FA2的主要不同点是计算每一次的$\\boldsymbol{O}^{(n)}$的逻辑，这里以$\\boldsymbol{O}^{(1)},\\boldsymbol{O}^{(2)}$为例来说明，在FA2中：\n$$ \\begin{gathered} \\tilde{\\mathbf{o}}^{(1)} =e^{s^{(1)}-m^{(1)}}\\mathbf{V}^{(1)}\\in\\mathbb{R}^{B_{r}\\times d} \\\\ \\tilde{\\mathrm{o}}^{(2)} =e^{s^{(1)}-m}\\mathbf{V}^{(1)}+e^{s^{(2)}-m}\\mathbf{V}^{(2)} \\\\ \\mathrm{o}^{(2)} =\\mathrm{diag}\\left(\\ell^{(2)}\\right)^{-1}\\tilde{\\mathbf{O}}^{(2)}=\\mathbf{O} \\end{gathered} $$\n其中，$\\tilde{\\mathrm{o}}^{(2)} =e^{s^{(1)}-m}\\mathbf{V}^{(1)}+e^{s^{(2)}-m}\\mathbf{V}^{(2)}$在计算的时候，$e^{s^{(1)}-m}\\mathbf{V}^{(1)}$这一项是对$\\tilde{\\mathbf{o}}^{(1)}$做了缩放，缩放因子是$e^{m^{(1)} - m}$。也就是：\n$$\\tilde{\\mathrm{o}}^{(2)} = e^{m^{(1)} - m} \\tilde{\\mathbf{o}}^{(1)} +e^{s^{(2)}-m}\\mathbf{V}^{(2)}$$\n相比于原来的FA1，我们首先计算Softmax的分子部分，在最后才算上分母。这样减少了每次迭代而必须的分母缩放。而原本的FA1的计算过程如下式所示：\n$$ \\mathbf{O}_{i}\\leftarrow\\mathrm{diag}\\left(\\ell_{i}^{\\mathrm{new}}\\right)^{-1}\\left(\\mathrm{diag}(\\ell_{i})e^{{m_{i}-m_{i}^{\\mathrm{new}}}}\\mathbf{O}_{i}+e^{{\\tilde{m}_{ij}-m_{i}^{\\mathrm{new}}}}\\mathbf{\\tilde{P}}_{ij}\\mathbf{V}_{j}\\right) $$ FA2的计算中，先不在每个block的每次迭代计算中执行全部的rescale操作，而是最后执行一次rescale。每次计算可以减少一次除法运算。\nFA2 伪代码From From Online Softmax to FlashAttention(CSE599m, ML for ML System)\n可以看到在原文的伪代码中，在$T_c$循环结束后，才去做了分母上的计算。\n第十行的$\\text{diag}^{-1}$是错的，把$^{-1}$去掉。\n优化了循环的顺序，增加了Seq维度的并行 FA1的两重循环中，是先外层循环load K, V，然后内层循环再load Q。这就会导致内层循环，每次计算的只是Qi的一部分，每次内循环的迭代都需要对Oi进行全局内存的读写。而且，一个显而易见的事实就是，在Attention的计算中，不同query的Attention计算是完全独立的。也就是说，如果外部循环是先load Q，那么就可以把不同的query块的Attention分配不同thread block进行计算，这些thread block之间是不需要通信的。没错，在FA2中，正是这样做的，对于forward pass，算法调换了循环的顺序，先load Q，再load K, V。\nFA2增加seqlen并行，提高了occupancy，并且对于forward pass，Q*K^T在【行】方向的seqlen上天然可以并行，thread block之间不需要额外的通信。\nWarp的分配更加的合理，避免Split-K 摘自 FlashAttention核心逻辑以及V1 V2差异总结\nWarp Split-KFrom From Online Softmax to FlashAttention(CSE599m, ML for ML System)\n首先看fwd，相比V1，V2改进了Warp Partition：4个warp会从smem的K/V tile load同样的数据做mma计算，但是load 不同Q，把V1 sliced-K sliced-V 改成了v2 sliced-Q，V1的做法是需要warp之间产生同步通信的，因为在计算QK结果乘V的时候，如图所示需要跨warp reduction得到O的结果，而且fwd的目的是沿着行方向计算softmax，行方向信息最后要汇总的，这也需要跨warp不同。V2就不需要了，这样可以减少同步开销。\n0x05 Causal Mask怎么用？ 摘自 [Attention优化][2w字]🔥原理\u0026amp;图解: 从Online-Softmax到FlashAttention V1/V2/V3\n非常简单的Early Exit逻辑：\n情况0: 全Early Exit。全0的mask可以直接返回0，无需$Q\\times K^T$，无需causal mask。\n情况1: 部分Early Exit。全1的mask，只需$\\text{Softmax}(Q\\times K^T)$，无需causal mask。\n情况3: 无法Early Exit。0-1混合的causal mask，需QxK^T，需要causal mask，然后$\\text{Softmax}(\\text{Mask}(Q \\times K^T))$。\nMasked 示意图[Attention优化][2w字]🔥原理\u0026amp;图解: 从Online-Softmax到FlashAttention V1/V2/V3\n0x06 MHA/GQA/MQA 在FlashAttention中，也支持MQA和GQA。对于MQA和GQA的情形，FlashAttention采用Indexing的方式，而不是直接复制多份KV Head的内容到显存然后再进行计算。Indexing，即通过传入KV/KV Head索引到Kernel中，然后计算内存地址，直接从内存中读取KV。\n0x07 IO复杂度分析 因为FA主要是优化IO Acces，所以我们分析下FA的IO复杂度。我们假设Sequence的长度是$N$，每个头的维度是$d$，SRAM的大小是$M,d \\le M \\le Nd$。\n使用原始的Self Attention算法的IO复杂度是$\\Theta(Nd + N^2)$，FA1的IO复杂度是$\\Theta(N^2d^2M^{-1})$，考虑到$d$一般是64-128，而$M$一般是100KB，所以FA1的访存次数小于原始的做法。\nMemory Accesses和d的平方成正比关系，当d越大，FA的Memory Accesses会增长剧烈。比如对于N=2K, M=192KB, 当d=256时，依然满足 FA IO Acesses \u0026lt; Naive Attention，但是当d=512时，这个结论就会反过来，变成是 FA IO Acesses \u0026gt; Naive Attention IO Acesses，并且由于FA本身的FLOPS就是比Naive Attention高的，于是，此时无论是IO还是FLOPS，FA都会比Naive Attention高，无论是访存还是计算量都没有优势，唯一剩下的优势，应该就只剩节省显存了（不需要保存中间的S和P矩阵，O(N^2)的内存复杂度）\n0x08 Triton代码 先再来复习下Block是怎么切块的，这里的图摘自BBuf的 笔记图解大模型计算加速系列：Flash Attention V2，从原理到并行计算。\nBlock切块方向图解大模型计算加速系列：Flash Attention V2，从原理到并行计算\n增加了Seq维度的并行以后：\nSeq维度切块方向图解大模型计算加速系列：Flash Attention V2，从原理到并行计算\n与V1不同的是，我们在Q的seq_len维度上也做了切分，将其分成四份，即num_m_block = 4。所以现在我们共有1_2_4 = 8个block在跑。这些block之间的运算也是独立的， 因为：\nhead的计算是独立的，所以红色block和蓝色block互不干扰 采用Q做外循环，KV做内循环时，行与行之间的block是独立的，因此不同行的block互相不干扰。 每个block从Q上加载对应位置的切块，同时从KV上加载head0的切块，计算出自己所维护的那部分O，然后写入O的对应位置。\n我们使用OpenAI Triton的FA2 Tutorial代码来分析。\n下面的代码是每一个子Block中的最内层的代码，其中q是最外层循环的子块；K_block_ptr、V_block_ptr是$K$、$V$的子块，需要一次for循环完整的遍历。\n@triton.jit def _attn_fwd_inner(acc, l_i, m_i, q, # K_block_ptr, V_block_ptr, # start_m, qk_scale, # BLOCK_M: tl.constexpr, HEAD_DIM: tl.constexpr, BLOCK_N: tl.constexpr, # STAGE: tl.constexpr, offs_m: tl.constexpr, offs_n: tl.constexpr, # N_CTX: tl.constexpr, fp8_v: tl.constexpr): # range of values handled by this stage # 根据STAGE的值，函数定义了处理的键（K）和值（V）的范围。 # 不同的STAGE对应不同的处理范围，支持因果（causal）和非因果（non-causal）的自注意力。 if STAGE == 1: # 使用 Mask lo, hi = 0, start_m * BLOCK_M elif STAGE == 2: # 使用 Mask lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M lo = tl.multiple_of(lo, BLOCK_M) # causal = False，不使用 Mask else: lo, hi = 0, N_CTX # tl.advance 根据步长调整K_block_ptr的指向 K_block_ptr = tl.advance(K_block_ptr, (0, lo)) V_block_ptr = tl.advance(V_block_ptr, (lo, 0)) # 对K,V Block做完整的遍历 for start_n in range(lo, hi, BLOCK_N): start_n = tl.multiple_of(start_n, BLOCK_N) # -- compute qk ---- # 加载 K Block k = tl.load(K_block_ptr) # 伪代码 line8: q x k qk = tl.dot(q, k) if STAGE == 2: # Mask mask = offs_m[:, None] \u0026gt;= (start_n + offs_n[None, :]) # Mask 区域加上 -INF qk = qk * qk_scale + tl.where(mask, 0, -1.0e6) # 伪代码 line 9: Safe online softmax 的 max m_ij = tl.maximum(m_i, tl.max(qk, 1)) # 伪代码 line 9: s - m qk -= m_ij[:, None] else: # 伪代码 line 9: Safe online softmax 的 max，和伪代码的区别是这里有 qk_scale，稍后解释 m_ij = tl.maximum(m_i, tl.max(qk, 1) * qk_scale) # 伪代码 line 9: s - m. 和伪代码的区别是这里有 qk_scale，稍后解释 qk = qk * qk_scale - m_ij[:, None] # 伪代码 line 9: p = exp(s-m) p = tl.math.exp2(qk) # 伪代码 line 9: rowsum(p) l_ij = tl.sum(p, 1) # -- update m_i and l_i # 伪代码 line 10 alpha = tl.math.exp2(m_i - m_ij) l_i = l_i * alpha + l_ij # -- update output accumulator -- # 伪代码 line 10: 这里的 acc 是伪代码中的 O_i acc = acc * alpha[:, None] # update acc v = tl.load(V_block_ptr) if fp8_v: p = p.to(tl.float8e5) else: p = p.to(tl.float16) # 伪代码 line 10. acc = tl.dot(p, v, acc) # update m_i and l_i m_i = m_ij # 更新下一轮的 K,V Block的指针 V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0)) K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N)) return acc, l_i, m_i 下面我们来看一下调用这个子块函数的函数。\n@triton.autotune(list(filter(keep, configs)), key=[\u0026#34;N_CTX\u0026#34;, \u0026#34;HEAD_DIM\u0026#34;]) @triton.jit def _attn_fwd(Q, K, V, sm_scale, M, Out, # stride_qz, stride_qh, stride_qm, stride_qk, # stride_kz, stride_kh, stride_kn, stride_kk, # stride_vz, stride_vh, stride_vk, stride_vn, # stride_oz, stride_oh, stride_om, stride_on, # Z, H, N_CTX, # HEAD_DIM: tl.constexpr, # BLOCK_M: tl.constexpr, # BLOCK_N: tl.constexpr, # STAGE: tl.constexpr # ): tl.static_assert(BLOCK_N \u0026lt;= HEAD_DIM) # 输入参数里的Z和H分别表示batch size和注意力头数 # q.shape is [Batch, Head, Seq, Dim] # 启动的时候 [grid] 是 # grid = lambda args: (triton.cdiv(q.shape[2], args[\u0026#34;BLOCK_M\u0026#34;]), q.shape[0] * q.shape[1], 1) # start_m表示当前kernel program 实例对应的seq维度的偏移，而off_hz表示的是batch*heads维度的偏移。 start_m = tl.program_id(0) # seq off_hz = tl.program_id(1) # batch * heads # 这两行计算了两个偏移量off_z和off_h，它们分别代表在batch（或heads）中的位置。 off_z = off_hz // H # 表示在哪个 Batch off_h = off_hz % H # 表示在哪个 Head # 计算用于定位Q、K和V张量中当前处理块的偏移量。这是基于先前计算的偏移量和提供的步长参数。 qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh # block pointers # 使用tl.make_block_ptr创建一个指向Q张量当前处理块的指针。这个函数调用指定了基础地址、形状、步长、偏移量和块形状等，以及如何在内存中访问这个数据块。 # N_CTX 是q.shape[2]，表示的是序列长度，BLOCK_DMODEL是Lk，表示的是每个注意力头的隐藏层维度大小 # 下面几个make_block_ptr创建的张量类似，分别是对K，V以及输出O创建指向当前处理块的指针 Q_block_ptr = tl.make_block_ptr( base=Q + qvk_offset, shape=(N_CTX, HEAD_DIM), strides=(stride_qm, stride_qk), offsets=(start_m * BLOCK_M, 0), block_shape=(BLOCK_M, HEAD_DIM), order=(1, 0), ) v_order: tl.constexpr = (0, 1) if V.dtype.element_ty == tl.float8e5 else (1, 0) V_block_ptr = tl.make_block_ptr( base=V + qvk_offset, shape=(N_CTX, HEAD_DIM), strides=(stride_vk, stride_vn), offsets=(0, 0), block_shape=(BLOCK_N, HEAD_DIM), order=v_order, ) K_block_ptr = tl.make_block_ptr( base=K + qvk_offset, shape=(HEAD_DIM, N_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0), block_shape=(HEAD_DIM, BLOCK_N), order=(0, 1), ) O_block_ptr = tl.make_block_ptr( base=Out + qvk_offset, shape=(N_CTX, HEAD_DIM), strides=(stride_om, stride_on), offsets=(start_m * BLOCK_M, 0), block_shape=(BLOCK_M, HEAD_DIM), order=(1, 0), ) # initialize offsets # 计算M维度（seq维度）上每个线程应处理的元素的起始偏移量。 offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M) # 计算N维度（batch*heads维度）上每个线程应处理的元素的偏移量。 offs_n = tl.arange(0, BLOCK_N) # initialize pointer to m and l # 初始化m向量，m用于存储每个m维度上的最大logit，初始化为负无穷大。 m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\u0026#34;inf\u0026#34;) # 初始化l向量，l用于累计softmax的分母，初始化为1。 l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0 # 初始化累加器，用于累积注意力加权和。注意这里的shape是(BLOCK_M, BLOCK_DMODEL) acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32) # load scales qk_scale = sm_scale qk_scale *= 1.44269504 # 1/log(2) # load q: it will stay in SRAM throughout # 将Q矩阵的当前块加载到SRAM中，此数据在整个计算过程中保持不变。 q = tl.load(Q_block_ptr) # stage 1: off-band # For causal = True, STAGE = 3 and _attn_fwd_inner gets 1 as its STAGE # For causal = False, STAGE = 1, and _attn_fwd_inner gets 3 as its STAGE if STAGE \u0026amp; 1: acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, # start_m, qk_scale, # BLOCK_M, HEAD_DIM, BLOCK_N, # 4 - STAGE, offs_m, offs_n, N_CTX, V.dtype.element_ty == tl.float8e5 # ) # stage 2: on-band if STAGE \u0026amp; 2: # barrier makes it easier for compielr to schedule the # two loops independently acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, # start_m, qk_scale, # BLOCK_M, HEAD_DIM, BLOCK_N, # 2, offs_m, offs_n, N_CTX, V.dtype.element_ty == tl.float8e5 # ) # epilogue m_i += tl.math.log2(l_i) acc = acc / l_i[:, None] m_ptrs = M + off_hz * N_CTX + offs_m tl.store(m_ptrs, m_i) tl.store(O_block_ptr, acc.to(Out.type.element_ty)) 需要特别注意的是这段代码最后的epilogue部分就对应了FlashAttention V2伪代码中的12行以后的内容，根据softmax的分母部分较正输出。此外，Triton的实现里面考虑了一些paper里面没有的东西比如qk_scale，causal mask，对Q*K的结果S应用了减掉m，使得整个实现看起来要复杂不少，但整体的算法逻辑和并行设置和paper还是一致的。\n最后在Attention中使用这个函数\nclass _attention(torch.autograd.Function): @staticmethod def forward(ctx, q, k, v, causal, sm_scale): # shape constraints HEAD_DIM_Q, HEAD_DIM_K = q.shape[-1], k.shape[-1] # when v is in float8_e5m2 it is transposed. HEAD_DIM_V = v.shape[-1] assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V assert HEAD_DIM_K in {16, 32, 64, 128, 256} o = torch.empty_like(q) stage = 3 if causal else 1 extra_kern_args = {} # Tuning for AMD target if is_hip(): waves_per_eu = 3 if HEAD_DIM_K \u0026lt;= 64 else 2 extra_kern_args = {\u0026#34;waves_per_eu\u0026#34;: waves_per_eu, \u0026#34;allow_flush_denorm\u0026#34;: True} # q.shape is [Batch, Head, Seq, Dim] grid = lambda args: (triton.cdiv(q.shape[2], args[\u0026#34;BLOCK_M\u0026#34;]), q.shape[0] * q.shape[1], 1) M = torch.empty((q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32) # Launch Kernel. _attn_fwd[grid]( q, k, v, sm_scale, M, o, # q.stride(0), q.stride(1), q.stride(2), q.stride(3), # k.stride(0), k.stride(1), k.stride(2), k.stride(3), # v.stride(0), v.stride(1), v.stride(2), v.stride(3), # o.stride(0), o.stride(1), o.stride(2), o.stride(3), # q.shape[0], q.shape[1], # N_CTX=q.shape[2], # HEAD_DIM=HEAD_DIM_K, # STAGE=stage, # **extra_kern_args) ctx.save_for_backward(q, k, v, o, M) ctx.grid = grid ctx.sm_scale = sm_scale ctx.HEAD_DIM = HEAD_DIM_K ctx.causal = causal return o 0x09 CUDA代码 0x0A FA 3 0x0B 思考 CPU上使用这个靠谱吗？CPU上并行度较低，用这个没有必要，但是可以考虑分块和Mask混合的MatMul来减少计算量，也就是Early Exit。 ","permalink":"https://chenghuawang.github.io/keep-moving-forward/tech/fundamental_from_online_softmax_to_flash_attentionv3/","summary":"Flash Attention from Fundamental Series","title":"[Fundamental] From Online Softmax to Flash Attention V3"},{"content":" 笔者最近在做一些mllm相关的工作，书写此文对mllm框架进行梳理总结，定有不少纰漏，请读者立即指出，谢谢。mllm目前在做一些其他工作，这篇文章的书写时间为发布时间。在mllm的其他工作合并进主仓库后，本文还会进一步的跟进。读者请注意本文的时效性。\n1. 简介 mllm是一款适用于移动设备和边缘设备的快速、轻量的多模态LLM推理引擎。\n完全的C/C++实现，无第三方依赖 针对fuyu-8B等多模态LLM进行了优化 支持ARM NEON和X86 AVX2向量指令 支持4 bits和6 bits整数量化 本文将更多的以工程的视角来解析mllm框架，在行文过程中，本文会将mllm与其他框架的设计方法做对比。接下来，本文将会用项目组织结构、框架执行流程、自定义Op/Layer、Tokenizer和如何支持新模型五个章节来详细描述mllm框架的各项特性和总体结构。读者可以把该文章做mllm的使用文档。在最后，本文将会指出mllm的不足之处和可以尝试跟进的工作。\n在开始正式解析mllm之前，读者可以先clone下mllm的代码库，以便于跟进分析流程。mllm不依赖于git submodule，项目配置起来很方便，目前mllm可以在linux上使用Clang/GCC编译器进行编译。目前mllm支持的目标设备体系结构是X86和Arm。\ngit clone https://github.com/UbiquitousLearning/mllm mllm团队将所有LLM相关的vocab文件都放在了git仓库中（这个其实可以移动到HuggingFace的仓库上），LLM量化后的模型文件都存储在HuggingFace上，读者可以在https://huggingface.co/mllmTeam上找到mllm提供的模型文件。\n2. 框架执行流程 2.1 以两层Linear层运行为例 首先，考虑下面的代码，定义了两个Linear Layers，并且输入$X$通过两个Linear Layers来得到输出：\nclass TwoLinear final : public Module { public: TwoLinear() = default; TwoLinear() { linear1 = Linear(in_f, out_f, /*bias*/true, \u0026#34;linear1\u0026#34;); linear2 = Linear(out_f, out_f, /*bias*/true, \u0026#34;linear2\u0026#34;); } std::vector\u0026lt;Tensor\u0026gt; Forward(std::vector\u0026lt;Tensor\u0026gt; inputs, std::vector\u0026lt;std::any\u0026gt; args) override { x = inputs[0]; x = linear1(x); x = linear2(x); return x; } private: Layer linear1; Layer linear2; } TwoLinear tl; 2.1.1 加载参数 读者可以使用 tl.load(path)来加载参数。那么mllm是如何实现参数加载的呢？在load函数中，mllm会创建一个ParamLoader，这个ParamLoader是Static的，在全局可以访问。然后mllm会设置另一个全局参数doLoad为True，进而进入推理流程operator()(tmps, tmpt);。在推理流程中，要是执行层发现doLoad为True，那么就执行每个算子内定义好的load指令，而不是执行每个算子的原本逻辑。 load的执行在Layer.hpp文件的INIT_OP()中。\n2.1.2Module的Operator()是如何调用Forward函数的？ 对于常见的INPUT_TENSOR类型的Tensor，mllm首先会设置这个Tensor的类型为TENSOR_STATIC_INIT，进行一遍Forward推理；第一遍Forward推理完毕以后再把Tensor的类型设为TENSOR_STATIC_READY，然后进行第二遍Forward推理。\nif (inputs[0].ttype() == TensorType::INPUT_TENSOR) { for (auto \u0026amp;input : inputs) { input.setTtype(TensorType::NORMAL_TENSOR); input.status() = TENSOR_STATIC_INIT; if(input.batch() == 0){ Tensor::gph_[input.name()] = input; } } tensor_status = TENSOR_STATIC_INIT; Forward(inputs, anyArgs); for (auto \u0026amp;input : inputs) { input.status() = TENSOR_STATIC_READY; } tensor_status = TENSOR_STATIC_READY; return Forward(inputs, anyArgs); } 第一次Forward推理的目的是调用Op定义的Reshape和SetUp函数，Reshape函数会推理出这一次模型推理的过程中每个Tensor的形状大小。SetUp函数会对Op需要输出的Tensor做内存的申请。 第二次Forward推理才是真正的计算。\n2.1.3 Linear层的执行 每个Layer在实现的时候都会重载operator()，比如linear layer的operator()函数如下：\nTensor \u0026amp;operator()(Tensor \u0026amp;input) { return _1I1O_OP(input); } 其中，_1I1O_OP表示的意思是，这是需要使用1个输入和1个输出的函数来处理这个算子。mllm还提供了许多类似于_1I1O_OP的函数来处理不同的算子。\n2.2 总结 大体来说，mllm使用了类似于状态机的参数来设置了当前推理过程的运行状态。每一次都是通过Forward函数来进行全模型的遍历，在Op的执行过程中，用这些设定的参数来区分每次Op需要表现的行为。\n3. 如何编写Op与自定义Layer 3.1 新增对应Backend的Op文件 mllm提供了src/backends/new_op.py实用工具来帮助创建Op Class。该文件会帮助读者创建下述基本函数：\nErrorCode reshape(vector\u0026lt;shared_ptr\u0026lt;Tensor\u0026gt;\u0026gt; inputs, vector\u0026lt;shared_ptr\u0026lt;Tensor\u0026gt;\u0026gt; outputs) override; ErrorCode execute(vector\u0026lt;shared_ptr\u0026lt;Tensor\u0026gt;\u0026gt; inputs, vector\u0026lt;shared_ptr\u0026lt;Tensor\u0026gt;\u0026gt; outputs) override; ErrorCode load(AbstructLoader \u0026amp;loader) override; ErrorCode free(vector\u0026lt;shared_ptr\u0026lt;Tensor\u0026gt;\u0026gt; inputs, vector\u0026lt;shared_ptr\u0026lt;Tensor\u0026gt;\u0026gt; outputs) override; ErrorCode setUp(vector\u0026lt;shared_ptr\u0026lt;Tensor\u0026gt;\u0026gt; inputs, vector\u0026lt;shared_ptr\u0026lt;Tensor\u0026gt;\u0026gt; outputs) override; 3.2 Op参数自定义 比如对于CPU上的LinearOp，需要in_features、out_features和has_bias三个参数。那么可以在3.1自动生成的class中加入：\nclass CPULinear final : public Op { ... private: int in_features_; int out_features_; bool support_bias_; int thread_count = 4; Tensor weight_; Tensor bias_; }; 在CPULinearCreator中加入：\nclass CPULinearCreator : public CPUBackend::Creator { public: virtual Op *create(OpParam op_param, Backend *bn, string name, int threadCount) const { int in_features = op_param[\u0026#34;in_features\u0026#34;]; int out_features = op_param[\u0026#34;out_features\u0026#34;]; int bias = op_param[\u0026#34;bias\u0026#34;]; return new CPULinear(bn, name, in_features, out_features, (bool)bias, threadCount); } }; 请注意，OpParam是一个string-float map。\n3.3 重载函数 读者需要自行实现reshape，execute，load，free函数，视情况重载setUp函数。 以Linear Op为例，reshape函数就会通过in_features_变量来检查输入的Tensor的维度是否正确，然后对output Tensor做outputs[0]-\u0026gt;reshape(inputs[0]-\u0026gt;batch(), inputs[0]-\u0026gt;head(), inputs[0]-\u0026gt;sequence(), out_features_)\n在load函数中，实现Weight和Bias的加载。\n在execute函数中，具体实现矩阵乘法等计算操作。\n在free函数中释放Weight和Bias。\n3.4 Op是如何被注册和创建的？ 在定义完成Op后，读者还需要把该Op注册到相应的Backend中，以及将Op抽象成Layer。\n3.4.1 在Backend中注册Op 以CPU Backend为例，读者需要再CPUBackend文件中加入addCreator(LINEAR, (CPUBackend::Creator *)(new CPULinearCreator()));\n如果这是一个新的算子，读者还需要在OpDefined文件中加入新Op的Enum项。\n3.4.2 在Layer.hpp中加入对应的Op Layer 如Linear Layer:\nclass Linear final : public Layer { public: explicit Linear(int in_features, int out_features, bool bias, std::string name) { param_[\u0026#34;in_features\u0026#34;] = in_features; param_[\u0026#34;out_features\u0026#34;] = out_features; param_[\u0026#34;bias\u0026#34;] = (float)bias; init(std::move(name), OpType::LINEAR); } Tensor \u0026amp;operator()(Tensor \u0026amp;input) { return _1I1O_OP(input); } }; 其中，在构造函数中的**init()**函数并没有创建这个Linear算子。它只是负责给这个Linear指派了Backend。 真正的算子创建还是在INIT_OP()函数中。在这个函数中，它会通过backend_-\u0026gt;opCreate(param_, name_);来创建算子。\n4. Tokenizer mllm提供了基础的Tokenizer支持，目前支持BPE和Unigram两种分词算法。\n5. 如何对新模型进行支持 在mllm中，对模型组件（model、Tokenizer、Configuration）的定义和HuggingFace Transformer库中的定义方法基本一致。以支持QWen0.5B模型为例，需要编写三个文件：\nconfiguration_qwen.cpp modeling_qwen.cpp tokenization_qwen.cpp 其中configuration_qwen.cpp定义了Qwen LLM的各类参数，如Head数量，hidden dim等。modeling_qwen.cpp定义了Qwen LLM网络。tokenization_qwen.cpp包含了将句子转化为Token的预处理行为。\n5.1 生成mllm支持的vocab和模型参数 5.1.1 模型转换 使用mllm提供的Converter实用工具来进行转换：\ncd tools/convertor pip install -r ./requirements.txt # for one file pytorch model python convert.py --input_model=model.pth --output_model=model.mllm --type=torch # for multi-file pytorch model python convert.py --input_model=pytorch_model.bin.index.json --output_model=model.mllm --type=torch # for one file safetensor model python convert.py --input_model=model.bin --output_model=model.mllm --type=safetensor # for multi-file safetensor model python convert.py --input_model=model.safetensors.index.json --output_model=model.mllm --type=safetensor 5.1.2 Vocab转换 使用mllm提供的Converter实用工具来进行转换：\ncd tools/convertor python vocab.py --input_file=tokenizer.json --output_file=vocab.mllm --type=Unigram 5.1.3 量化 mllm提供了量化工具，该工具支持4 bits和6 bits整数量化，你可以使用下述指令来对模型参数进行量化\ncd bin ./quantize model.mllm model_q4_0.mllm Q4_K 5.2 Configuration 设置文件里面主要实现两个类，一个是QWenNameConfig，一个是QWenConfig，其中QWenNameConfig包含QWenConfig。在一个mllm模型参数文件中，模型参数是以key-value对的形式统一起来的。QWenNameConfig的目的就是给出每个参数的名称，以便于mllm框架索引到正确的模型参数。\nclass QWenNameConfig : public TransformerNameConfig { public: /** * @brief QWen2 following the hugging face naming method * * @param type RoPEType */ void init(RoPEType type = RoPEType::HFHUBROPE) { switch (type) { case RoPEType::HFHUBROPE: { blk_name = \u0026#34;model.layers.\u0026#34;; _attn_base_name = \u0026#34;self_attn.\u0026#34;; _ffn_base_name = \u0026#34;mlp.\u0026#34;; _q_proj_name = \u0026#34;q_proj\u0026#34;; _k_proj_name = \u0026#34;k_proj\u0026#34;; _v_proj_name = \u0026#34;v_proj\u0026#34;; _o_proj_name = \u0026#34;o_proj\u0026#34;; _gate_proj_name = \u0026#34;gate_proj\u0026#34;; _up_proj_name = \u0026#34;up_proj\u0026#34;; _down_proj_name = \u0026#34;down_proj\u0026#34;; _attn_norm_name = \u0026#34;input_layernorm\u0026#34;; _ffn_norm_name = \u0026#34;post_attention_layernorm\u0026#34;; token_embd_name = \u0026#34;model.embed_tokens\u0026#34;; post_norm_name = \u0026#34;model.norm\u0026#34;; lm_head_name = \u0026#34;lm_head\u0026#34;; break; } ... } } std::string blk_name; std::string token_embd_name; std::string post_norm_name; std::string lm_head_name; std::string _gate_proj_name; }; 在QWenConfig中则主要定义各层的超参数，如rope的theta值、中间层维度大小等，如下面的代码所示：\nstruct QWenConfig { explicit QWenConfig(int token_limit, string billions = \u0026#34;0.5B\u0026#34;, RoPEType type = RoPEType::HFHUBROPE) : cache_limit(token_limit) { ... }; float attention_dropout = 0.0; int bos_token_id = 151643; int eos_token_id = 151643; std::string hidden_act = \u0026#34;silu\u0026#34;; int hidden_size = 1024; float initializer_range = 0.02; int intermediate_size = 2816; int max_position_embeddings = 32768; int max_window_layers = 21; std::string model_type = \u0026#34;qwen2\u0026#34;; int num_attention_heads = 16; int num_hidden_layers = 24; int num_key_value_heads = 16; double rms_norm_eps = 1e-6; float rope_theta = 1000000.0; int sliding_window = 32768; int vocab_size = 151936; bool tie_embedding_words = false; int cache_limit; RoPEType RoPE_type = RoPEType::HFHUBROPE; QWenNameConfig names_config; }; 5.3 Tokenization Tokenization是一个非常客制化的步骤，每个LLM的Tokenization方法都不尽相同。以QWen为例子，QWen使用了BBPE方法，那么读者在支持QWen模型的时候，就要给出实现了BBPE的Tokenizer。mllm内部已经实现一个BPE算法，读者可以复用该实现来实现自己的Tokenizer。\n5.4 Modeling 使用mllm框架提供的算子来实现模型是非常简单和便利的，熟悉Pytorch的读者可以快速的上手mllm。本文在这里默认读者对llama/qwen/mistral等常见LLM的模型有着基本的了解。在下文中，本文以Attention模块为例来演示如何使用mllm来搭建模型。 首先，所有的class需要继承Module父类。Module父类提供了Forward函数，读者需要重载该函数来实现相应的计算流程。\nclass QWenAttention final : public Module ... 5.4.1 创建该Module需要使用的Layers class QWenAttention final : public Module { public： QWenAttention() = default; QWenAttention(const QWenConfig \u0026amp;config, const QWenNameConfig \u0026amp;names, const string \u0026amp;base_name) { hidden_size = config.hidden_size; num_heads = config.num_attention_heads; head_dim = config.hidden_size / num_heads; num_key_value_heads = config.num_key_value_heads; num_key_value_groups = num_heads / num_key_value_heads; // init layers q_proj = Linear(hidden_size, num_heads * head_dim, true, base_name + names._q_proj_name); k_proj = Linear(hidden_size, num_key_value_heads * head_dim, true, base_name + names._k_proj_name); v_proj = Linear(hidden_size, num_key_value_heads * head_dim, true, base_name + names._v_proj_name); o_proj = Linear(num_heads * head_dim, hidden_size, false, base_name + names._o_proj_name); q_rope = RoPE(config.RoPE_type, config.rope_theta, config.max_position_embeddings, base_name + \u0026#34;q_rope\u0026#34;); k_rope = RoPE(config.RoPE_type, config.rope_theta, config.max_position_embeddings, base_name + \u0026#34;k_rope\u0026#34;); k_cache = KVCache(num_key_value_groups, config.cache_limit, base_name + \u0026#34;k_cache\u0026#34;); v_cache = KVCache(num_key_value_groups, config.cache_limit, base_name + \u0026#34;v_cache\u0026#34;); mask = Causalmask(base_name + \u0026#34;mask\u0026#34;); softmax = Softmax(DIMENSION, base_name + \u0026#34;softmax\u0026#34;); } private: int hidden_size; int num_heads; int head_dim; int num_key_value_heads; int num_key_value_groups; Layer q_proj; Layer k_proj; Layer v_proj; Layer o_proj; Layer q_rope; Layer k_rope; Layer k_cache; Layer v_cache; Layer mask; Layer softmax; } 细心的读者可能已经发现了，在QWenAttention的构造函数中，创建每个Layer的时候都在最后一个参数上传递了Layer名称（std::string type），这是因为mllm依赖于Layer的名称来寻找该Layer所需要的参数。\n5.4.2 重载Forward前向推理函数 创建完了所有我们需要的Layers以后，就可以编写Forward函数来定义Attention模块的计算流程，Forward函数接收一个Tensor Array和一个std::any Array，返回Tensor Array：\nstd::vector\u0026lt;Tensor\u0026gt; Forward(std::vector\u0026lt;Tensor\u0026gt; inputs, std::vector\u0026lt;std::any\u0026gt; args) override { auto query_states = q_proj(inputs[0]); auto key_states = k_proj(inputs[1]); auto value_states = v_proj(inputs[2]); // [batch, heads, sequence, dims] query_states = query_states.view(-1, num_heads, -1, head_dim); key_states = key_states.view(-1, num_key_value_heads, -1, head_dim); value_states = value_states.view(-1, num_key_value_heads, -1, head_dim); // embedding query_states = q_rope(query_states); key_states = k_rope(key_states); // kv cache key_states = k_cache(key_states); value_states = v_cache(value_states); // attention weight auto atten_weight = Tensor::mm(query_states, key_states.transpose(Chl::SEQUENCE, Chl::DIMENSION)) / std::sqrt(head_dim); atten_weight = mask(atten_weight); atten_weight = softmax(atten_weight); // attention output auto atten_output = Tensor::mm(atten_weight, value_states); atten_output = atten_output.view(-1, 1, -1, head_dim * num_heads); atten_output = o_proj(atten_output); return {atten_output}; } 5.5 运行 完整的Qwen模型定义代码可以在附录1中找到。读者可以像Torch一样调用定义好的模型：首先，创建模型：\nQWenConfig config(tokens_limit, \u0026#34;0.5B\u0026#34;, RoPEType::HFHUBROPE); auto model = QWenForCausalLM(config); model.load(model_path); moduleclass重载了()operator，读者可以使用model({input_tensor})来进行推理。\n6. mllm框架的不足 这里写的有点mean，本人专业知识浅薄，在学术上是依托答辩，对mllm的理解更是不到位，大家轻喷。\n6.1 Benchmark 缺少算子的Benchmark 本文认为，mllm在实现的时候极力的避免使用第三方的库，因为mllm需要迁移到移动设备上，一些三方库可能不能正常工作。但是手工实现的Kernel还是需要一个Benchmark来和目标平台上提供的算子库来进行性能比较的。就mllm目前提供的MatMul Kernel来看，似乎缺少Pack优化和/micro Kernel的优化？\n缺少prefill/decode的Benchmark mllm的issues中也有人提到过这个问题。作为具有LLM推理能力的引擎，应当测一下这两个基本能力。\n6.2 对于移动端LLM推理的特定优化 KV Cache量化 IIRC，在OPPO的Transformer-Lite[2]中，用到了KV Cache量化的小技巧。这对移动设备有限的内存来说可能会更加友好，当然还需要考量量化带来的CPU负载问题。\n动态形状推理/内存复用/KV Cache搬移优化 目前mllm是没有做内存复用的，可以考虑使用符号推理方法来做动态形状的支持进而便于求解下一轮的内存使用情况。或许可以考虑一下PageAttention[3]的Tensor管理方法或者[2]中的KV Cache规划方法来进一步减少内存的搬移。\n异构算力 可以考虑把形状推理（CPU）和计算（GPU/NPU）并行执行起来。或者是6.2.4中提到的内容与计算并行起来。\n对模型参数的Lazy Fetch和Pre Fetch 目前，mllm会把参数一次性的读入内存？考虑到移动设备的内存有限，可以在合适的时机提前从外存上预取而不是全数载入。\n6.3 易用性 模型结构需要手动编写且无法保存 目前，mllm的模型结构还是需要在C++文件中进行显示的手动定义。或许可以考虑创建自己的计算图和算子描述方式，使用flatbuffers来存储计算图。\n如果要很好的使用所有的算力，可能还是需要完善的计算图机制，这样便于优化分析。 尝试引入三方易用的库如icu等来弥补C++ utf-8处理能力的不足。 Ref：\n[1] mllm, https://github.com/UbiquitousLearning/mllm\n[2] transformer-lite, https://arxiv.org/abs/2403.20041\n[3] PageAttention, https://arxiv.org/abs/2309.06180\nA1. Qwen模型定义 #ifndef MODELING_QWEN_HPP #define MODELING_QWEN_HPP #include \u0026#34;Backend.hpp\u0026#34; #include \u0026#34;Layer.hpp\u0026#34; #include \u0026#34;Module.hpp\u0026#34; #include \u0026#34;Tensor.hpp\u0026#34; #include \u0026#34;configuration_qwen.hpp\u0026#34; #include \u0026lt;cmath\u0026gt; using namespace mllm; // Copied from GemmaMLP with Gemma-\u0026gt;Qwen and using silu class QWenMLP final : public Module { public: QWenMLP() = default; QWenMLP(int hidden_size, int intermediate_size, const QWenNameConfig \u0026amp;names, const std::string \u0026amp;base_name) { gate_proj = Linear(hidden_size, intermediate_size, false, base_name + names._gate_proj_name); silu = SiLU(base_name + \u0026#34;act\u0026#34;); up_proj = Linear(hidden_size, intermediate_size, false, base_name + names._up_proj_name); down_proj = Linear(intermediate_size, hidden_size, false, base_name + names._down_proj_name); } std::vector\u0026lt;Tensor\u0026gt; Forward(std::vector\u0026lt;Tensor\u0026gt; inputs, std::vector\u0026lt;std::any\u0026gt; args) override { auto x = gate_proj(inputs[0]); x = silu(x); auto y = up_proj(inputs[0]); x = x * y; x = down_proj(x); return {x}; } private: Layer gate_proj; Layer up_proj; Layer down_proj; Layer silu; }; // Copied from GemmaAttention with Gemma-\u0026gt;Qwen and using SWA class QWenAttention final : public Module { public: QWenAttention() = default; QWenAttention(const QWenConfig \u0026amp;config, const QWenNameConfig \u0026amp;names, const string \u0026amp;base_name) { hidden_size = config.hidden_size; num_heads = config.num_attention_heads; head_dim = config.hidden_size / num_heads; num_key_value_heads = config.num_key_value_heads; num_key_value_groups = num_heads / num_key_value_heads; // init layers q_proj = Linear(hidden_size, num_heads * head_dim, true, base_name + names._q_proj_name); k_proj = Linear(hidden_size, num_key_value_heads * head_dim, true, base_name + names._k_proj_name); v_proj = Linear(hidden_size, num_key_value_heads * head_dim, true, base_name + names._v_proj_name); o_proj = Linear(num_heads * head_dim, hidden_size, false, base_name + names._o_proj_name); q_rope = RoPE(config.RoPE_type, config.rope_theta, config.max_position_embeddings, base_name + \u0026#34;q_rope\u0026#34;); k_rope = RoPE(config.RoPE_type, config.rope_theta, config.max_position_embeddings, base_name + \u0026#34;k_rope\u0026#34;); k_cache = KVCache(num_key_value_groups, config.cache_limit, base_name + \u0026#34;k_cache\u0026#34;); v_cache = KVCache(num_key_value_groups, config.cache_limit, base_name + \u0026#34;v_cache\u0026#34;); // mask = SlidingWindowMask(config.sliding_window, base_name + \u0026#34;mask\u0026#34;); mask = Causalmask(base_name + \u0026#34;mask\u0026#34;); softmax = Softmax(DIMENSION, base_name + \u0026#34;softmax\u0026#34;); } std::vector\u0026lt;Tensor\u0026gt; Forward(std::vector\u0026lt;Tensor\u0026gt; inputs, std::vector\u0026lt;std::any\u0026gt; args) override { auto query_states = q_proj(inputs[0]); auto key_states = k_proj(inputs[1]); auto value_states = v_proj(inputs[2]); // [batch, heads, sequence, dims] query_states = query_states.view(-1, num_heads, -1, head_dim); key_states = key_states.view(-1, num_key_value_heads, -1, head_dim); value_states = value_states.view(-1, num_key_value_heads, -1, head_dim); // embedding query_states = q_rope(query_states); key_states = k_rope(key_states); // kv cache key_states = k_cache(key_states); value_states = v_cache(value_states); // attention weight auto atten_weight = Tensor::mm(query_states, key_states.transpose(Chl::SEQUENCE, Chl::DIMENSION)) / std::sqrt(head_dim); atten_weight = mask(atten_weight); atten_weight = softmax(atten_weight); // attention output auto atten_output = Tensor::mm(atten_weight, value_states); atten_output = atten_output.view(-1, 1, -1, head_dim * num_heads); atten_output = o_proj(atten_output); return {atten_output}; } private: int hidden_size; int num_heads; int head_dim; int num_key_value_heads; int num_key_value_groups; Layer q_proj; Layer k_proj; Layer v_proj; Layer o_proj; Layer q_rope; Layer k_rope; Layer k_cache; Layer v_cache; Layer mask; Layer softmax; }; // Copied from GemmaDecoder with Gemma-\u0026gt;Qwen and set RmsNorm(without add_unit_offset) class QWenDecoder final : public Module { public: QWenDecoder() = default; QWenDecoder(const QWenConfig \u0026amp;config, const QWenNameConfig \u0026amp;names, const string \u0026amp;base_name) { self_atten = QWenAttention(config, names, base_name + names._attn_base_name); mlp = QWenMLP(config.hidden_size, config.intermediate_size, names, base_name + names._ffn_base_name); input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps, base_name + names._attn_norm_name); post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps, base_name + names._ffn_norm_name); } std::vector\u0026lt;Tensor\u0026gt; Forward(std::vector\u0026lt;Tensor\u0026gt; inputs, std::vector\u0026lt;std::any\u0026gt; args) override { auto x = input_layernorm(inputs[0]); x = self_atten({x, x, x})[0]; auto tmp = x + inputs[0]; x = post_attention_layernorm(tmp); x = mlp({x})[0]; x = x + tmp; return {x}; } private: QWenAttention self_atten; QWenMLP mlp; Layer input_layernorm; Layer post_attention_layernorm; }; // Copied from GemmaModel with Gemma-\u0026gt;Qwen and set RmsNorm(without add_unit_offset) class QWenModel final : public Module { public: QWenModel() = default; QWenModel(const QWenConfig \u0026amp;config, const QWenNameConfig \u0026amp;names, const string \u0026amp;base_name) { blocks = List\u0026lt;QWenDecoder\u0026gt;(config.num_hidden_layers, config, names, base_name); norm = RMSNorm(config.hidden_size, config.rms_norm_eps, names.post_norm_name); } std::vector\u0026lt;Tensor\u0026gt; Forward(std::vector\u0026lt;Tensor\u0026gt; inputs, std::vector\u0026lt;std::any\u0026gt; args) override { auto x = inputs[0]; for (auto \u0026amp;block : blocks) { x = block({x})[0]; } x = norm(x); return {x}; } private: std::vector\u0026lt;QWenDecoder\u0026gt; blocks; Layer norm; }; class QWenForCausalLM final : public Module { public: QWenForCausalLM(QWenConfig \u0026amp;config) { auto names = config.names_config; hidden_size = config.hidden_size; tie_embedding_words = config.tie_embedding_words; embedding = Embedding(config.vocab_size, config.hidden_size, names.token_embd_name); model = QWenModel(config, names, names.blk_name); // FIXME Qwen-0.5 use tied embedding // Others use nn.Linear() if (tie_embedding_words) { lm_head = Parameter(1, config.vocab_size, 1, config.hidden_size, names.token_embd_name + \u0026#34;.weight\u0026#34;); } } std::vector\u0026lt;Tensor\u0026gt; Forward(std::vector\u0026lt;Tensor\u0026gt; inputs, std::vector\u0026lt;std::any\u0026gt; args) override { auto x = embedding(inputs[0]); // go through model auto outputs = model({x})[0]; if (tie_embedding_words) { outputs = Tensor::mm(outputs, lm_head().transpose(Chl::SEQUENCE, Chl::DIMENSION)); } return {outputs}; } private: int hidden_size; bool tie_embedding_words; Layer embedding; Parameter lm_head; QWenModel model; }; #endif //! MODELING_QWEN_HPP ","permalink":"https://chenghuawang.github.io/keep-moving-forward/tech/mllm-qwen/","summary":"以Qwen0.5B为例解析mllm的基本实现，CPU Backend","title":"mllm框架浅析(一)-以QWen0.5B为例"},{"content":"http://arxiv.org/abs/2310.16795\nMLSys 2024\n1.背景和动机 为了解决大型模型的高推理成本问题，MoE架构被提出。MoE通过稀疏路由的方式，将输入分配给多个专家（experts）中的一小部分，以实现更快的推理速度和更高的模型质量。但这种架构也带来了巨大的参数量，例如SwitchTransformer-c2048模型就有1.6万亿参数。MoE模型的参数量巨大，需要数TB级的存储空间，这使得它们在实际部署时面临内存和成本的挑战，尤其是在需要大规模并行计算的场合。\n为了降低MoE模型的内存和存储需求，同时保持模型性能，模型压缩成为了一个重要的研究方向。传统的压缩技术，如量化和稀疏性，虽然在一定程度上有效，但对于参数量达到万亿级别的模型来说，仍然不足以实现高效的压缩。\n本文提出了QMoE，一种新的压缩和执行框架，旨在实现对万亿参数MoE模型的高效压缩和推理。QMoE通过设计一种可扩展的算法，将模型压缩到每个参数不到1比特的大小，并与定制的GPU解码内核协同设计，以实现端到端的高效压缩推理，且运行时开销相对较小。\nFig 1. 量化结果http://arxiv.org/abs/2310.16795\n作者首先考虑了Huffman和LZW两种常用于文件压缩的方法。但是Huffman方法的解码依赖于上文已经被解析的参数，并行性低；且变长的编码方式在实现上和存储的时候也是较为困难的。作者总结出了MoE量化的4个难点：\n现有的压缩方法，如量化和稀疏性，通常只能在不显著损失精度的情况下将模型的精度降低到每个参数3或4比特，或者达到大约50%的稀疏度。然而，要使万亿参数的MoE模型实用化，需要比16位精度高出10到20倍的压缩率，即平均每个参数少于1比特。 将现有的压缩方法应用于比大型dense模型大一个数量级的MoE模型时，会遇到内存、性能和可靠性方面的障碍。MoE模型由于其稀疏性，需要处理的内存和数据量巨大。即量化过程需要的内存太大，且可能会出现因为corner case导致量化失败的问题。 实现每个参数少于1比特的压缩率需要一个非平凡的自定义压缩格式，并且这种格式需要配备在GPU等加速器上高效的解码算法，以避免在压缩模型上进行推理时出现重大的处理延迟（比如要避免Huffman方法的同步）。 为了应对上述挑战，需要在系统级别进行设计和优化，包括优化激活卸载、使用列表缓冲区来支持样本访问、延迟权重获取以减少内存占用、专家分组以提高GPU利用率，以及进行鲁棒性修改以处理在压缩具有数万个层的模型时可能遇到的罕见corner case。 2. 算法 2.1 使用GPTQ量化 Fig 2. 使用GPTQ量化流程http://arxiv.org/abs/2310.16795\n具体来说，我们维护一个大型缓冲区$B$，并按以下方式更新 Transformer 块的Dense部分：\n从CPU到GPU抓取一个 \u0026ldquo;样本\u0026rdquo; $X$，其中包含数百个Token 通过相应的Dense Layer，得到结果$Y$ 计算并存储$Y$中标记的专家分配 将$Y$送回CPU并覆盖$B$中的$X$ 并且对于稀疏部分，分别对专家进行循环：\n从CPU到GPU获取$B$中所有被分配给专家$E$的单独Token，记作$X_{E}$ 使用它们来生成压缩后的专家$E^{\u0026rsquo;}$（例如，使用GPTQ算法） 通过$E^{\u0026rsquo;}$模块以获得$Y_{E^{\u0026rsquo;}}$ 将$Y_{E^{\u0026rsquo;}}$发送回CPU，并在B中覆盖$X_{E}$ 作者在这里还引入了List Buffering、Lazy Weight Fetching和Expert Grouping技巧\n2.1.1 List Buffering 为了有效地支持对Dense模型的访问，以及对专家tokens的完全向量化查询，我们将$B$存储为列表缓冲数据结构。这可以被看作是一个包含所有tokens隐藏状态的巨大连续缓冲区，以及分隔符索引，这些索引标志着各个样本之间的边界。下图展示了这种存储格式。这种数据结构对效率至关重要；对于大量样本计数，通过掩码迭代样本并获取相关tokens的方法是很慢的，而作者提出的方法则有大幅度改进。\nFig 3. list bufferinghttp://arxiv.org/abs/2310.16795\n2.1.2 Lazy Weight Fetching 由于1.6万亿参数模型的权重占用了超过3TB的存储空间，它们甚至无法存储在CPU的RAM中。因此，我们按需直接从磁盘存储中懒加载它们。按照推理的流程，我们需要将所有的参数从磁盘搬移到内存中完整的一整次。\n2.1.3 Experts Grouping 此外，为了避免GPU的利用率不足，作者将多个专家组合在一起，并应用GPTQ算法的联合批处理变体。\n2.2 字典生成 对于量化后得到的Ternary Pair ${w_{min}, 0, w_{max}}$，在很多的情况下，是0居多的，也就是说是稀疏的，那么对于稀疏矩阵可以用CSR等方法来存储。但是使用传统的稀疏矩阵存储方法压缩比还是不够，作者团队使用了一种更加偏向于文件压缩的思路来进行量化后的参数压缩，这个方法就使用到了字典查找的方法。字典查找的方法还是比较通俗易懂的，以下面的例子来举例：\n对于“001002003\u0026hellip;”我们可以统计该串里面的子串的出现频率，比如001，002，003出现的频率高，那么我们可以将他们编码成 A,B,C然后仅需要三个char的空间“ABC”就可以表示一个压缩后的文件。\n使用字典来压缩就是本文为什么可以做到平均每个weight小于1bit量化的来源。\n一般来说，假设三元权重矩阵（用 0、1、2 表示）的值接近独立分布，其分布如下:\n$$ P(0)=p_{0}, P(1)=P(2)=\\frac{1-p_{0}}{2} $$\nFig 4. 字典生成算法http://arxiv.org/abs/2310.16795\n然后，作者用上图中的算法来生成字典。字典的Key是16bit的类型，字典的Value是一个长度在14以下的非空对数组(即：[(t1, t2), (t0, t1), \u0026hellip;])。本文在生成字典的时候做了两个限制：1. 字典的Key是16位的 2. 字典的Value最多是14个Pair。\n该算法的每次循环，会找出队列中出现概率最高的Value，然后对其Value Concat上一个新的Pair，这样做的好处是：可以让Cache Locality特性发挥的很好，我们会在2.3小节中看到。 作者一共生成了有$2^{16}$个条目的字典。\n字典中的Value是像下图所示排布的：\nFig 5. Value的排布http://arxiv.org/abs/2310.16795\n一个字典中的Value包含2个int32类型，之所以不用int64是出于GPU访存冲突的考量。每个int32的4个低位表示该Value有多少个有效的pairs。每2个2bits表示一个pair，每个2bits表示一个weight。\n2.3 Decode 在Decode阶段，我们需要知道当前wrap和thread的ID，然后用shift操作符提取到对应字典中的元素。核心伪代码如下：\nint enc = w_comp_block[warp][j]; int wx14 = dec[2 * enc + (lane / 14)]; int ter = (wx14 \u0026gt;\u0026gt; (4 + 2 * (lane % 14))) \u0026amp; 0x3; float w = deq[ter][thread]; 31 res += w * x_shared[idx + lane]; idx += 2 * (wx14 \u0026amp; 0xf); 其中w_comp_block是已经压缩过的weight；dec是一维字典；deq是预先提取出来的三元组；x_shared是输入X。作者在这里融合了解码和点乘的操作。\n3. 总结与思考 QMoE有着非常好的效果，但是现在的MoE模型普遍不是开源的，并且有特定的Infra设施。QMoE的进一步验证和实施还需要更多的业内工程跟进。\n不理解的地方：生成字典的Algorithm1为什么可以生成所有情况的Value呢？会有不同的组合情况产生吗？\n","permalink":"https://chenghuawang.github.io/keep-moving-forward/papers/mlsys2024-qmoe/","summary":"http://arxiv.org/abs/2310.16795\nMLSys 2024\n1.背景和动机 为了解决大型模型的高推理成本问题，MoE架构被提出。MoE通过稀疏路由的方式，将输入分配给多个专家（experts）中的一小部分，以实现更快的推理速度和更高的模型质量。但这种架构也带来了巨大的参数量，例如SwitchTransformer-c2048模型就有1.6万亿参数。MoE模型的参数量巨大，需要数TB级的存储空间，这使得它们在实际部署时面临内存和成本的挑战，尤其是在需要大规模并行计算的场合。\n为了降低MoE模型的内存和存储需求，同时保持模型性能，模型压缩成为了一个重要的研究方向。传统的压缩技术，如量化和稀疏性，虽然在一定程度上有效，但对于参数量达到万亿级别的模型来说，仍然不足以实现高效的压缩。\n本文提出了QMoE，一种新的压缩和执行框架，旨在实现对万亿参数MoE模型的高效压缩和推理。QMoE通过设计一种可扩展的算法，将模型压缩到每个参数不到1比特的大小，并与定制的GPU解码内核协同设计，以实现端到端的高效压缩推理，且运行时开销相对较小。\nFig 1. 量化结果http://arxiv.org/abs/2310.16795\n作者首先考虑了Huffman和LZW两种常用于文件压缩的方法。但是Huffman方法的解码依赖于上文已经被解析的参数，并行性低；且变长的编码方式在实现上和存储的时候也是较为困难的。作者总结出了MoE量化的4个难点：\n现有的压缩方法，如量化和稀疏性，通常只能在不显著损失精度的情况下将模型的精度降低到每个参数3或4比特，或者达到大约50%的稀疏度。然而，要使万亿参数的MoE模型实用化，需要比16位精度高出10到20倍的压缩率，即平均每个参数少于1比特。 将现有的压缩方法应用于比大型dense模型大一个数量级的MoE模型时，会遇到内存、性能和可靠性方面的障碍。MoE模型由于其稀疏性，需要处理的内存和数据量巨大。即量化过程需要的内存太大，且可能会出现因为corner case导致量化失败的问题。 实现每个参数少于1比特的压缩率需要一个非平凡的自定义压缩格式，并且这种格式需要配备在GPU等加速器上高效的解码算法，以避免在压缩模型上进行推理时出现重大的处理延迟（比如要避免Huffman方法的同步）。 为了应对上述挑战，需要在系统级别进行设计和优化，包括优化激活卸载、使用列表缓冲区来支持样本访问、延迟权重获取以减少内存占用、专家分组以提高GPU利用率，以及进行鲁棒性修改以处理在压缩具有数万个层的模型时可能遇到的罕见corner case。 2. 算法 2.1 使用GPTQ量化 Fig 2. 使用GPTQ量化流程http://arxiv.org/abs/2310.16795\n具体来说，我们维护一个大型缓冲区$B$，并按以下方式更新 Transformer 块的Dense部分：\n从CPU到GPU抓取一个 \u0026ldquo;样本\u0026rdquo; $X$，其中包含数百个Token 通过相应的Dense Layer，得到结果$Y$ 计算并存储$Y$中标记的专家分配 将$Y$送回CPU并覆盖$B$中的$X$ 并且对于稀疏部分，分别对专家进行循环：\n从CPU到GPU获取$B$中所有被分配给专家$E$的单独Token，记作$X_{E}$ 使用它们来生成压缩后的专家$E^{\u0026rsquo;}$（例如，使用GPTQ算法） 通过$E^{\u0026rsquo;}$模块以获得$Y_{E^{\u0026rsquo;}}$ 将$Y_{E^{\u0026rsquo;}}$发送回CPU，并在B中覆盖$X_{E}$ 作者在这里还引入了List Buffering、Lazy Weight Fetching和Expert Grouping技巧\n2.1.1 List Buffering 为了有效地支持对Dense模型的访问，以及对专家tokens的完全向量化查询，我们将$B$存储为列表缓冲数据结构。这可以被看作是一个包含所有tokens隐藏状态的巨大连续缓冲区，以及分隔符索引，这些索引标志着各个样本之间的边界。下图展示了这种存储格式。这种数据结构对效率至关重要；对于大量样本计数，通过掩码迭代样本并获取相关tokens的方法是很慢的，而作者提出的方法则有大幅度改进。\nFig 3. list bufferinghttp://arxiv.org/abs/2310.16795\n2.1.2 Lazy Weight Fetching 由于1.6万亿参数模型的权重占用了超过3TB的存储空间，它们甚至无法存储在CPU的RAM中。因此，我们按需直接从磁盘存储中懒加载它们。按照推理的流程，我们需要将所有的参数从磁盘搬移到内存中完整的一整次。\n2.1.3 Experts Grouping 此外，为了避免GPU的利用率不足，作者将多个专家组合在一起，并应用GPTQ算法的联合批处理变体。\n2.2 字典生成 对于量化后得到的Ternary Pair ${w_{min}, 0, w_{max}}$，在很多的情况下，是0居多的，也就是说是稀疏的，那么对于稀疏矩阵可以用CSR等方法来存储。但是使用传统的稀疏矩阵存储方法压缩比还是不够，作者团队使用了一种更加偏向于文件压缩的思路来进行量化后的参数压缩，这个方法就使用到了字典查找的方法。字典查找的方法还是比较通俗易懂的，以下面的例子来举例：\n对于“001002003\u0026hellip;”我们可以统计该串里面的子串的出现频率，比如001，002，003出现的频率高，那么我们可以将他们编码成 A,B,C然后仅需要三个char的空间“ABC”就可以表示一个压缩后的文件。","title":"✅[Oct 2023] 模型量化之QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models"},{"content":"背景和动机 以KV Cache为启发，探索了对time-to-first-token (TTFT) Latency的优化。类似于KV Cache，Prompt Cache(PC)推理加速的核心思想是复用注意力的中间状态(Attention States)。然而与KV Cache不同的是，PC是在不同的prompt之间进行复用。\n在大部分的LLM任务中，prompt有重叠(overlapping)的现象，这些重叠的prompt可以被存储起来，进而在接下来的LLM处理阶段可以像KV Cache一样，提取出来直接使用。在TTFT的推理过程中，免去计算不同prompt中重叠部分的注意力状态，从而缩短TTFT的生成时间。\n与KV Cache不同的点是：\n相同的文本段可能出现在不同prompt的不同位置，如何对它们的Attention States进行复用。因为不同位置的文本段的Position Encoding进去的值是不一样的。在KV Cache中不需要考虑这一点，因为cache是从前往后线性增长的，但Prompt所在的位置是不确定的。 如何从不同的prompt中识别出已经缓存过的文本。 算法 实验经验 一段prompt的Position值不连续没有关系。只要这一段prompt本身的Position值是连续的就行。意思是部分连续对于LLM就够了，不一定要完全连续。请注意：这是一个实验性验证的结论。\nPrompt Schema Fig 1. Prompt Schema 作者团队定义了一个Prompt Markup Language(PML)。上图中的例子有：可以复用的module和不能复用的填充部分，填充部分需要用Param指出，并给出长度。Prompt Attention States中的红色部分是可以被复用的区域。\nFig 2. 原始LLM/KV Cache/Prompt Cache 我们来对比下普通的自回归LLM、使用了KV Cache的LLM和使用了Prompt Cache的LLM。普通的LLM每次都要通过输入的Prompt来预测出下一个Token，Prompt是全量的计算。使用了KV Cache的LLM，每次Token预测不用全量计算了，可以使用上次Attention的中间结果。而使用了Prompt Cache的LLM，在后期预测Token的过程和原来的KV Cache没有什么区别。主要区别是在一开始的Prompt输入的阶段，Prompt Cache中常用的Prompt Attention States可以被利用起来，这会极大的缩减第一个Token输出的时间。 Prompt Schema有很多的细节，这里只讲大致的思路，具体的请看文章和代码仓库。\n我对module怎么复用不是很理解，应该是通过将文本内容进行sha256编码来对其进行识别。\n本文主要是对首Token输出时间的优化，对于用户来说可以有更好的体验。要是能做个全局的Prompt Cache数据库，应该可以给大规模的LLM Infer系统带来不少的好处。\n","permalink":"https://chenghuawang.github.io/keep-moving-forward/papers/prompt_cache/","summary":"背景和动机 以KV Cache为启发，探索了对time-to-first-token (TTFT) Latency的优化。类似于KV Cache，Prompt Cache(PC)推理加速的核心思想是复用注意力的中间状态(Attention States)。然而与KV Cache不同的是，PC是在不同的prompt之间进行复用。\n在大部分的LLM任务中，prompt有重叠(overlapping)的现象，这些重叠的prompt可以被存储起来，进而在接下来的LLM处理阶段可以像KV Cache一样，提取出来直接使用。在TTFT的推理过程中，免去计算不同prompt中重叠部分的注意力状态，从而缩短TTFT的生成时间。\n与KV Cache不同的点是：\n相同的文本段可能出现在不同prompt的不同位置，如何对它们的Attention States进行复用。因为不同位置的文本段的Position Encoding进去的值是不一样的。在KV Cache中不需要考虑这一点，因为cache是从前往后线性增长的，但Prompt所在的位置是不确定的。 如何从不同的prompt中识别出已经缓存过的文本。 算法 实验经验 一段prompt的Position值不连续没有关系。只要这一段prompt本身的Position值是连续的就行。意思是部分连续对于LLM就够了，不一定要完全连续。请注意：这是一个实验性验证的结论。\nPrompt Schema Fig 1. Prompt Schema 作者团队定义了一个Prompt Markup Language(PML)。上图中的例子有：可以复用的module和不能复用的填充部分，填充部分需要用Param指出，并给出长度。Prompt Attention States中的红色部分是可以被复用的区域。\nFig 2. 原始LLM/KV Cache/Prompt Cache 我们来对比下普通的自回归LLM、使用了KV Cache的LLM和使用了Prompt Cache的LLM。普通的LLM每次都要通过输入的Prompt来预测出下一个Token，Prompt是全量的计算。使用了KV Cache的LLM，每次Token预测不用全量计算了，可以使用上次Attention的中间结果。而使用了Prompt Cache的LLM，在后期预测Token的过程和原来的KV Cache没有什么区别。主要区别是在一开始的Prompt输入的阶段，Prompt Cache中常用的Prompt Attention States可以被利用起来，这会极大的缩减第一个Token输出的时间。 Prompt Schema有很多的细节，这里只讲大致的思路，具体的请看文章和代码仓库。\n我对module怎么复用不是很理解，应该是通过将文本内容进行sha256编码来对其进行识别。\n本文主要是对首Token输出时间的优化，对于用户来说可以有更好的体验。要是能做个全局的Prompt Cache数据库，应该可以给大规模的LLM Infer系统带来不少的好处。","title":"✅[April 2024] Prompt Cache: Modular Attention Reuse for Low-Latency Inference"},{"content":"Li L, Qian S, Lu J, et al. Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs. arxiv preprint arxiv:2403.20041, 2024.\n无代码，技术报告\n⭐️⭐️⭐️\nhttp://arxiv.org/abs/2403.20041\n1. 背景 \u0026amp; 动机 这篇论文是OPPO AI Center发表的，其提出了Transformer-Lite框架来缓解移动设备GPU上部署大型语言模型（LLM）时存在的性能问题。大型语言模型（如ChatGLM2 6B和Gemma 2B）被广泛应用于智能助手、文本摘要、翻译和多模态任务等，但它们在移动设备上的部署面临着几个关键问题：\n计算能力和内存带宽的需求：LLMs需要大量的计算能力和内存带宽，这些资源在移动设备上是有限的。 用户体验：当前的在端侧设备上部署LLM的方法通常有着较慢的推理速度，这会严重影响用户体验。 成本：在云上大规模部署LLM有着可观的成本，而且随着移动设备性能的不断提升，本地部署LLM不仅可以减少与云部署相关的高成本，还可以扩大LLM在移动设备上的应用前景以及保护用户隐私。 为了解决上述问题。这篇文章文章提出了以下优化技术：\n符号表达式的方法：支持动态形状模型推理，包括动态形状推导、内存重用和执行调度等。 算子优化和执行优先级设置：加快推理速度并减少手机延迟。 FP4量化方法：称为M0E4，减少dequantize开销，使得矩阵乘法更高效。 基于子张量的技巧：避免了在LLM推理后复制KV缓存的内存压力。 作者团队还实现了一个名为Transformer-Lite的移动推理引擎，该引擎与高通和联发科处理器兼容，并通过一系列实验评估了其性能。通过这些技术，Transformer-Lite引擎在prefill和decoding方面相比基于CPU的FastLLM和基于GPU的MLC-LLM取得了显著的速度提升。\n2. 方法 2.1 对于动态形状推理的符号表达方法 不同于多数静态尺寸的CV模型，LLM 是一种动态形状输入的场景，每次迭代的输入形状都会发生变化。这导致模型中一些激活张量的形状发生变化，给内存重用和算子性能优化带来了巨大挑战。 例如，将形状[“sumN-N”,1,2,128]与axis 0上的[“N”,1,2,128]连接，输出形状为[“sumN”,1,2,128]。为此，作者团队利用类似于 Nimble 和 DISC 的方法，将深度学习模型中的算子分为两类：张量计算算子和形状计算算子。后一类算子包含形状算子和那些其输入取决于形状算子结果的算子。形状计算算子在 CPU 上执行，计算形状信息。相反，负责计算激活的张量和权重的张量的计算算子则在 GPU 上执行。\nFig 1. 形状推理from http://arxiv.org/abs/2403.20041\n作者团队使用了SymPy包且结合了ONNX来实现了这一能力。\n对于内存复用，实现内存复用的先决条件是获得张量之间的内存大小关系，这在静态形状推理中很简单。符号表达式便于轻松确定这种关系。首先，计算每个张量的内存大小，即元素数乘以数据类型的字节数，从而为每个张量的内存大小创建一个符号表达式。**然后结合减法和除法来辨别张量之间的内存大小关系。**例如，用 “N ”*4096 除以 “N ”32128，得到的整数为 1，表示大小相等，可以重复使用内存。另一方面，当 “N” * 4096 与 “sumN” * 2 * 128 相比较时，减法和除法都会产生另一个符号表达式，而不是一个确定的整数。因此，除非探索所有潜在的输入形状，否则它们之间的大小关系是不确定的，这意味着无法进行内存重用。 作者团队使用了OpenCL工具来实现了内存复用。\n通常，LLM的每次推理迭代都需要更新输入形状。在形状更新过程中，需要重新计算动态张量的形状，并更新具有动态形状输入或输出张量的算子参数。因此，频繁的输入形状更新通常会导致不可忽略的性能开销。为了避免这个问题，作者团队采用了Mask机制，在解码阶段将模型输入序列长度填充为64或128的倍数。这种方法可以在实际计数超过填充长度时更新形状，从而使每次推理迭代的形状更新时间开销可以忽略不计。\n2.2 算子优化和执行优先级设置 作者实现了半精度和4bit量化的混合精度矩阵乘法。此外，prefill和decoding阶段对矩阵乘法的形状是不同的。在prefill阶段，矩阵乘法进行矩阵-矩阵计算，而在decoding阶段，这些运算符被简化为向量-矩阵计算。作者对这两种方式的乘法都做了优化，文中没有涉及到算子优化的细节。\nGPU在执行LLM时还需要负责用户界面渲染，LLM的执行可能会造成界面渲染延迟。为了解决这个问题，作者利用高通和ARM提供的OpenCL扩展，将深度学习模型算子的执行优先级设置为最低级别。这种方法有效地缓解了移动设备的渲染延迟。\n2.3 FP4量化方法：M0E4 在传统的量化方法中，如GPTQ和AWQ，模型权重从浮点数被量化为4位整数，以减少模型大小和内存带宽需求。然而，为了保持高精度，计算的时候通常使用半精度进行。在模型推理时，需要将量化的权重dequantize为浮点数以执行矩阵乘法，这个过程涉及到将整数转换为浮点数，会引起显著的性能开销。在将整数转换为浮点数的过程中需要使用复杂的算法或指令来实现。因此，dequantize会引发不可忽略的性能开销。\nM0E4中的\u0026quot;M0\u0026quot;表示指数部分使用0位。这意味着指数部分被设置为一个常量值，通常是用于表示半精度浮点数的最小正指数。\u0026ldquo;E4\u0026quot;表示尾数部分使用4位。这些4位用于存储量化的尾数信息，这与浮点数表示中的尾数（或称为小数部分）相对应。\n作者采用group-wise量化，并通过缩放和偏置系数将每个group的张量(表示为w0)转换为一个新的浮点张量(表示为w1)。并且确保所有w1元素都在$[2^n, 2^{n+1} - eps]$的范围内。$n$的取值为1，2。在这个范围内，就可以保证这些元素的指数部分是一致的。\n为了提高量化的准确性，作者为舍入分离了一个额外的位(表示为y2，如图所示):y1 = min (y1+ y2,15)。这里，15是一个4位无符号整数所能表示的最大值。\nFig 2. M0E4 量化from http://arxiv.org/abs/2403.20041\n在dequantize时候，用如下公式即可\n$$ w1_{half} = as_{half} (\\text{ConstExpBinPart} | (w1_{fp4} \\ll \\text{ConstBitShiftNum})) $$\n2.4 基于subtensor来缓解KV Cache压力 一张图就可以说明，结合SWA应该可以工作的不错。\nFig 3. KV Cache 优化from http://arxiv.org/abs/2403.20041\n发现这个做法不如vLLM种的Page Attention来的统一、简洁。\n3. 实验 3.1 各个LLM上的性能 Fig 4. 各个LLM上的性能from http://arxiv.org/abs/2403.20041\n3.2 和FastLLM和MLC-LLM比较 Fig 5. 和MLC-LLM比较from http://arxiv.org/abs/2403.20041\nFig 6. 和FastLLM比较from http://arxiv.org/abs/2403.20041\n","permalink":"https://chenghuawang.github.io/keep-moving-forward/papers/transformer-lite/","summary":"Transformer-Lite from OPPO","title":"✅[Mar 2024] Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs"},{"content":"Lin J, Tang J, Tang H, et al. Awq: Activation-aware weight quantization for llm compression and acceleration[j]. Machine Learning System. Best Paper. https://arxiv.org/abs/2306.00978\n1. 背景和动机 直接在FP16精度上Round成INT3/INT4会造成极大的性能损失 基于activation distribution对重要的weight做精度保留则可以很大程度上提高模型性能。 但是混合存储FP16和INT3/4，在推理系统实现的时候过于复杂且对于硬件非常的不友好。 2. 算法 2.1 原理和假设 Fig 1. AWQ 原有的Round方法(图a)：\n$$ Q(\\mathbf{w})=\\Delta\\cdot\\mathrm{Round}(\\frac{\\mathbf{w}}\\Delta),\\quad\\Delta=\\frac{\\max(|\\mathbf{w}|)}{2^{N-1}} $$\n其中$\\mathbf{w}$表示一组参数，$Q(\\mathbf{w})$表示量化函数，$N$表示量化位数。\n改进后的量化方法(图c)：\n$$ Q(w\\cdot s)\\cdot\\frac xs=\\Delta^{\u0026rsquo;}\\cdot\\mathrm{Round}(\\frac{ws}{\\Delta^{\u0026rsquo;}})\\cdot x\\cdot\\frac1s $$\n其中$w \\in \\mathbf{W}$。即先对特定的$w$做Scaling然后再Scaling回去。这样做的理由是，误差可以成倍的减小，如下面的公式和观察出来的现象：\n$$ \\begin{aligned}\\operatorname{Err}(Q(w)x)\u0026amp;=\\Delta\\cdot\\operatorname{RoundErr}(\\frac w\\Delta)\\cdot x \\newline \\operatorname{Err}(Q(w\\cdot s)(\\frac xs))\u0026amp;=\\Delta^{\u0026rsquo;}\\cdot\\operatorname{RoundErr}(\\frac{ws}{\\Delta^{\u0026rsquo;}})\\cdot x\\cdot\\frac1s\\end{aligned} $$\n其中由于$\\operatorname{Round}$函数是四舍五入，所以误差$\\operatorname{RoundErr}\\in [0,0.5]$且是一个均匀分布。平均在0.25。不管是否被缩放了，这个分布是不变的。\n由于一组权重$\\mathbf{w}$的最大值在缩放一个$w$后是基本不变的，所以我们可以认为$\\Delta^{\u0026rsquo;} \\approx \\Delta$。 在此基础上，我们可以看出使用了Scaling以后得误差变小了，将上述提到的误差做个比值可以看出，$k=\\frac{\\Delta^{\u0026rsquo;}}{\\Delta} \\times \\frac{1}{s}$。\n2.2 优化：如何找到最优的Scaling值呢？ $$ \\mathbf{s}^{*} = \\arg \\mathop{\\min}_{s}\\mathcal{L}(\\mathbf{s}) $$\n$$ \\mathcal{L}(s) = \\Vert Q(\\mathbf{W} \\cdot \\text{diag}(s))(\\text{diag}(s)^{-1}\\cdot \\mathbf{X}) - \\mathbf{WX} \\Vert $$\n实际上就是用L2 Norm搜索出来一组最为合适的Scaling参数，注意，Scaling是对于每个权值都做的。 作者团队为了简化搜索的空间，对$\\mathbf{s}$做了一定的约束，如下：\n$$ \\mathbf{s} = \\mathbf{s_X}^{\\alpha}，\\alpha^{*}=\\arg \\mathop{\\min}_{\\alpha}\\mathcal{L}(\\mathbf{s_X}^{\\alpha}) $$\n其中，$\\alpha \\in [0, 1]$为缩放系数，$\\mathbf{s_{X}}$是activation perchannel 的值。当$\\alpha = 0$时，意味着不缩放；反之，意味着完全缩放。\n3. 总结 非常有用，简单粗暴，Best Paper实至名归？是一个非常符合直觉的工程性质的工作，但是工程的实现复杂性比较高，在多数场景上没有被使用。 问题：由于一组权重$\\mathbf{w}$的最大值在缩放一个$w$后是基本不变的，所以我们可以认为$\\Delta^{\u0026rsquo;} \\approx \\Delta$。这个假设并不是很成立啊？\n","permalink":"https://chenghuawang.github.io/keep-moving-forward/papers/awq/","summary":"Lin J, Tang J, Tang H, et al. Awq: Activation-aware weight quantization for llm compression and acceleration[j]. Machine Learning System. Best Paper. https://arxiv.org/abs/2306.00978\n1. 背景和动机 直接在FP16精度上Round成INT3/INT4会造成极大的性能损失 基于activation distribution对重要的weight做精度保留则可以很大程度上提高模型性能。 但是混合存储FP16和INT3/4，在推理系统实现的时候过于复杂且对于硬件非常的不友好。 2. 算法 2.1 原理和假设 Fig 1. AWQ 原有的Round方法(图a)：\n$$ Q(\\mathbf{w})=\\Delta\\cdot\\mathrm{Round}(\\frac{\\mathbf{w}}\\Delta),\\quad\\Delta=\\frac{\\max(|\\mathbf{w}|)}{2^{N-1}} $$\n其中$\\mathbf{w}$表示一组参数，$Q(\\mathbf{w})$表示量化函数，$N$表示量化位数。\n改进后的量化方法(图c)：\n$$ Q(w\\cdot s)\\cdot\\frac xs=\\Delta^{\u0026rsquo;}\\cdot\\mathrm{Round}(\\frac{ws}{\\Delta^{\u0026rsquo;}})\\cdot x\\cdot\\frac1s $$\n其中$w \\in \\mathbf{W}$。即先对特定的$w$做Scaling然后再Scaling回去。这样做的理由是，误差可以成倍的减小，如下面的公式和观察出来的现象：\n$$ \\begin{aligned}\\operatorname{Err}(Q(w)x)\u0026amp;=\\Delta\\cdot\\operatorname{RoundErr}(\\frac w\\Delta)\\cdot x \\newline \\operatorname{Err}(Q(w\\cdot s)(\\frac xs))\u0026amp;=\\Delta^{\u0026rsquo;}\\cdot\\operatorname{RoundErr}(\\frac{ws}{\\Delta^{\u0026rsquo;}})\\cdot x\\cdot\\frac1s\\end{aligned} $$\n其中由于$\\operatorname{Round}$函数是四舍五入，所以误差$\\operatorname{RoundErr}\\in [0,0.5]$且是一个均匀分布。平均在0.25。不管是否被缩放了，这个分布是不变的。\n由于一组权重$\\mathbf{w}$的最大值在缩放一个$w$后是基本不变的，所以我们可以认为$\\Delta^{\u0026rsquo;} \\approx \\Delta$。 在此基础上，我们可以看出使用了Scaling以后得误差变小了，将上述提到的误差做个比值可以看出，$k=\\frac{\\Delta^{\u0026rsquo;}}{\\Delta} \\times \\frac{1}{s}$。\n2.2 优化：如何找到最优的Scaling值呢？ $$ \\mathbf{s}^{*} = \\arg \\mathop{\\min}_{s}\\mathcal{L}(\\mathbf{s}) $$","title":"✅[April 2024] 模型量化之AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"},{"content":"1. Algorithm 2. Code void sgemm_micro_6x16_ac_br_cr(int m, int n, int k, float alpha, const float* A, const float* B, float beta, float* C, int ldc) { assert( m == 6 \u0026amp;\u0026amp; n == 16 \u0026amp;\u0026amp; \u0026#34;sgemm micro kernel expects A: 6xk(col major), B: kx16(row major) and C: 6x16(row major)\u0026#34;); uint64_t iters = k / 4; uint64_t remaining = k % 4; uint64_t ldc_ = ldc; #if defined(__AVX__) const float* a_ptr = A; const float* b_ptr = B; __m256 ymm4, ymm5, ymm6, ymm7, ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15; // set all outputs ymm register to zeros. ymm4 = _mm256_setzero_ps(); ymm5 = _mm256_setzero_ps(); ymm6 = _mm256_setzero_ps(); ymm7 = _mm256_setzero_ps(); ymm8 = _mm256_setzero_ps(); ymm9 = _mm256_setzero_ps(); ymm10 = _mm256_setzero_ps(); ymm11 = _mm256_setzero_ps(); ymm12 = _mm256_setzero_ps(); ymm13 = _mm256_setzero_ps(); ymm14 = _mm256_setzero_ps(); ymm15 = _mm256_setzero_ps(); // For C: 6 x 16 // // line0(16xfp32): ymm4, ymm5 // line1(16xfp32): ymm6, ymm7 // line2(16xfp32): ymm8, ymm9 // line3(16xfp32): ymm10, ymm11 // line4(16xfp32): ymm12, ymm13 // line5(16xfp32): ymm14, ymm15 for (uint64_t k_index = 0; k_index \u0026lt; iters; ++k_index) { __m256 ymm0, ymm1, ymm2, ymm3; // performance issues? __builtin_prefetch(b_ptr); __builtin_prefetch(a_ptr); __builtin_prefetch(b_ptr + 64); __builtin_prefetch(a_ptr + 24); // iteration 0 // get a line of matrix B -\u0026gt; 1 x 16 ymm0 = _mm256_load_ps(b_ptr); // -\u0026gt; 1 x 8 ymm1 = _mm256_load_ps(b_ptr + 8); // -\u0026gt; 1 x 8 ymm2 = _mm256_broadcast_ss(a_ptr); ymm3 = _mm256_broadcast_ss(a_ptr + 1); ymm4 = _mm256_fmadd_ps(ymm0, ymm2, ymm4); ymm5 = _mm256_fmadd_ps(ymm1, ymm2, ymm5); ymm6 = _mm256_fmadd_ps(ymm0, ymm3, ymm6); ymm7 = _mm256_fmadd_ps(ymm1, ymm3, ymm7); ymm2 = _mm256_broadcast_ss(a_ptr + 2); ymm3 = _mm256_broadcast_ss(a_ptr + 3); ymm8 = _mm256_fmadd_ps(ymm0, ymm2, ymm8); ymm9 = _mm256_fmadd_ps(ymm1, ymm2, ymm9); ymm10 = _mm256_fmadd_ps(ymm0, ymm3, ymm10); ymm11 = _mm256_fmadd_ps(ymm1, ymm3, ymm11); ymm2 = _mm256_broadcast_ss(a_ptr + 4); ymm3 = _mm256_broadcast_ss(a_ptr + 5); ymm12 = _mm256_fmadd_ps(ymm0, ymm2, ymm12); ymm13 = _mm256_fmadd_ps(ymm1, ymm2, ymm13); ymm14 = _mm256_fmadd_ps(ymm0, ymm3, ymm14); ymm15 = _mm256_fmadd_ps(ymm1, ymm3, ymm15); // iteration 1 // get a line of matrix B -\u0026gt; 1 x 16 ymm0 = _mm256_load_ps(b_ptr + 16); // -\u0026gt; 1 x 8 ymm1 = _mm256_load_ps(b_ptr + 24); // -\u0026gt; 1 x 8 ymm2 = _mm256_broadcast_ss(a_ptr + 6); ymm3 = _mm256_broadcast_ss(a_ptr + 7); ymm4 = _mm256_fmadd_ps(ymm0, ymm2, ymm4); ymm5 = _mm256_fmadd_ps(ymm1, ymm2, ymm5); ymm6 = _mm256_fmadd_ps(ymm0, ymm3, ymm6); ymm7 = _mm256_fmadd_ps(ymm1, ymm3, ymm7); ymm2 = _mm256_broadcast_ss(a_ptr + 8); ymm3 = _mm256_broadcast_ss(a_ptr + 9); ymm8 = _mm256_fmadd_ps(ymm0, ymm2, ymm8); ymm9 = _mm256_fmadd_ps(ymm1, ymm2, ymm9); ymm10 = _mm256_fmadd_ps(ymm0, ymm3, ymm10); ymm11 = _mm256_fmadd_ps(ymm1, ymm3, ymm11); ymm2 = _mm256_broadcast_ss(a_ptr + 10); ymm3 = _mm256_broadcast_ss(a_ptr + 11); ymm12 = _mm256_fmadd_ps(ymm0, ymm2, ymm12); ymm13 = _mm256_fmadd_ps(ymm1, ymm2, ymm13); ymm14 = _mm256_fmadd_ps(ymm0, ymm3, ymm14); ymm15 = _mm256_fmadd_ps(ymm1, ymm3, ymm15); // iteration 2 // get a line of matrix B -\u0026gt; 1 x 16 ymm0 = _mm256_load_ps(b_ptr + 32); // -\u0026gt; 1 x 8 ymm1 = _mm256_load_ps(b_ptr + 40); // -\u0026gt; 1 x 8 ymm2 = _mm256_broadcast_ss(a_ptr + 12); ymm3 = _mm256_broadcast_ss(a_ptr + 13); ymm4 = _mm256_fmadd_ps(ymm0, ymm2, ymm4); ymm5 = _mm256_fmadd_ps(ymm1, ymm2, ymm5); ymm6 = _mm256_fmadd_ps(ymm0, ymm3, ymm6); ymm7 = _mm256_fmadd_ps(ymm1, ymm3, ymm7); ymm2 = _mm256_broadcast_ss(a_ptr + 14); ymm3 = _mm256_broadcast_ss(a_ptr + 15); ymm8 = _mm256_fmadd_ps(ymm0, ymm2, ymm8); ymm9 = _mm256_fmadd_ps(ymm1, ymm2, ymm9); ymm10 = _mm256_fmadd_ps(ymm0, ymm3, ymm10); ymm11 = _mm256_fmadd_ps(ymm1, ymm3, ymm11); ymm2 = _mm256_broadcast_ss(a_ptr + 16); ymm3 = _mm256_broadcast_ss(a_ptr + 17); ymm12 = _mm256_fmadd_ps(ymm0, ymm2, ymm12); ymm13 = _mm256_fmadd_ps(ymm1, ymm2, ymm13); ymm14 = _mm256_fmadd_ps(ymm0, ymm3, ymm14); ymm15 = _mm256_fmadd_ps(ymm1, ymm3, ymm15); // iteration 3 // get a line of matrix B -\u0026gt; 1 x 16 ymm0 = _mm256_load_ps(b_ptr + 48); // -\u0026gt; 1 x 8 ymm1 = _mm256_load_ps(b_ptr + 56); // -\u0026gt; 1 x 8 ymm2 = _mm256_broadcast_ss(a_ptr + 18); ymm3 = _mm256_broadcast_ss(a_ptr + 19); ymm4 = _mm256_fmadd_ps(ymm0, ymm2, ymm4); ymm5 = _mm256_fmadd_ps(ymm1, ymm2, ymm5); ymm6 = _mm256_fmadd_ps(ymm0, ymm3, ymm6); ymm7 = _mm256_fmadd_ps(ymm1, ymm3, ymm7); ymm2 = _mm256_broadcast_ss(a_ptr + 20); ymm3 = _mm256_broadcast_ss(a_ptr + 21); ymm8 = _mm256_fmadd_ps(ymm0, ymm2, ymm8); ymm9 = _mm256_fmadd_ps(ymm1, ymm2, ymm9); ymm10 = _mm256_fmadd_ps(ymm0, ymm3, ymm10); ymm11 = _mm256_fmadd_ps(ymm1, ymm3, ymm11); ymm2 = _mm256_broadcast_ss(a_ptr + 22); ymm3 = _mm256_broadcast_ss(a_ptr + 23); ymm12 = _mm256_fmadd_ps(ymm0, ymm2, ymm12); ymm13 = _mm256_fmadd_ps(ymm1, ymm2, ymm13); ymm14 = _mm256_fmadd_ps(ymm0, ymm3, ymm14); ymm15 = _mm256_fmadd_ps(ymm1, ymm3, ymm15); a_ptr += 24; b_ptr += 64; } for (uint64_t k_index = 0; k \u0026lt; remaining; ++k_index) { __m256 ymm0, ymm1, ymm2, ymm3; // get a line of matrix B -\u0026gt; 1 x 16 ymm0 = _mm256_load_ps(b_ptr); // -\u0026gt; 1 x 8 ymm1 = _mm256_load_ps(b_ptr + 8); // -\u0026gt; 1 x 8 ymm2 = _mm256_broadcast_ss(a_ptr); ymm3 = _mm256_broadcast_ss(a_ptr + 1); ymm4 = _mm256_fmadd_ps(ymm0, ymm2, ymm4); ymm5 = _mm256_fmadd_ps(ymm1, ymm2, ymm5); ymm6 = _mm256_fmadd_ps(ymm0, ymm3, ymm6); ymm7 = _mm256_fmadd_ps(ymm1, ymm3, ymm7); ymm2 = _mm256_broadcast_ss(a_ptr + 2); ymm3 = _mm256_broadcast_ss(a_ptr + 3); ymm8 = _mm256_fmadd_ps(ymm0, ymm2, ymm8); ymm9 = _mm256_fmadd_ps(ymm1, ymm2, ymm9); ymm10 = _mm256_fmadd_ps(ymm0, ymm3, ymm10); ymm11 = _mm256_fmadd_ps(ymm1, ymm3, ymm11); ymm2 = _mm256_broadcast_ss(a_ptr + 4); ymm3 = _mm256_broadcast_ss(a_ptr + 5); ymm12 = _mm256_fmadd_ps(ymm0, ymm2, ymm12); ymm13 = _mm256_fmadd_ps(ymm1, ymm2, ymm13); ymm14 = _mm256_fmadd_ps(ymm0, ymm3, ymm14); ymm15 = _mm256_fmadd_ps(ymm1, ymm3, ymm15); a_ptr += 6; b_ptr += 16; } __m256 c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11; // For C: 6 x 16 // // line0(16xfp32): c0, c1 // line1(16xfp32): c2, c3 // line2(16xfp32): c4, c5 // line3(16xfp32): c6, c7 // line4(16xfp32): c8, c9 // line5(16xfp32): c10, c11 float* c_ptr0 = C; float* c_ptr1 = C + ldc_; float* c_ptr2 = c_ptr1 + ldc_; float* c_ptr3 = c_ptr2 + ldc_; float* c_ptr4 = c_ptr3 + ldc_; float* c_ptr5 = c_ptr4 + ldc_; c0 = _mm256_load_ps(c_ptr0); c1 = _mm256_load_ps(c_ptr0 + 8); c2 = _mm256_load_ps(c_ptr1); c3 = _mm256_load_ps(c_ptr1 + 8); c4 = _mm256_load_ps(c_ptr2); c5 = _mm256_load_ps(c_ptr2 + 8); c6 = _mm256_load_ps(c_ptr3); c7 = _mm256_load_ps(c_ptr3 + 8); c8 = _mm256_load_ps(c_ptr4); c9 = _mm256_load_ps(c_ptr4 + 8); c10 = _mm256_load_ps(c_ptr5); c11 = _mm256_load_ps(c_ptr5 + 8); c0 = _mm256_add_ps(c0, ymm4); c1 = _mm256_add_ps(c1, ymm5); c2 = _mm256_add_ps(c2, ymm6); c3 = _mm256_add_ps(c3, ymm7); c4 = _mm256_add_ps(c4, ymm8); c5 = _mm256_add_ps(c5, ymm9); c6 = _mm256_add_ps(c6, ymm10); c7 = _mm256_add_ps(c7, ymm11); c8 = _mm256_add_ps(c8, ymm12); c9 = _mm256_add_ps(c9, ymm13); c10 = _mm256_add_ps(c10, ymm14); c11 = _mm256_add_ps(c11, ymm15); _mm256_store_ps(c_ptr0, c0); _mm256_store_ps(c_ptr0 + 8, c1); _mm256_store_ps(c_ptr1, c2); _mm256_store_ps(c_ptr1 + 8, c3); _mm256_store_ps(c_ptr2, c4); _mm256_store_ps(c_ptr2 + 8, c5); _mm256_store_ps(c_ptr3, c6); _mm256_store_ps(c_ptr3 + 8, c7); _mm256_store_ps(c_ptr4, c8); _mm256_store_ps(c_ptr4 + 8, c9); _mm256_store_ps(c_ptr5, c10); _mm256_store_ps(c_ptr5 + 8, c11); #endif //! defined(__AVX__) } ","permalink":"https://chenghuawang.github.io/keep-moving-forward/tech/x86_avx_sgemm_6x16/","summary":"How to impl high performance 6xKx16 micro kernel","title":"【施工中】6xKx16 SGEMM Kernel on X86-AVX"},{"content":"人工智能领域的许多最新进展都围绕着大规模神经网络展开，但训练大规模神经网络是一项艰巨的工程和研究挑战，需要协调GPU集群来执行单个同步计算。随着集群数和模型规模的增长，机器学习从业者开发了多项技术，让机器学习模型能在多个GPU上进行并行模型训练。\n乍一看，这些并行技术令人生畏，但只需对计算结构进行一些假设，这些技术就会变得清晰：从某些角度来看，这也只是从 A 到 B 传递并不透明的位，就像数据包在网络交换机之间传递一样。\nFig 1. 各种 Parallel 模型 不同的并行技术将训练过程划分为不同的维度，包括：\n数据并行（Data Parallelism）在不同的GPU上运行同一批数据的不同子集 DP（Data Parallel） DDP（Distributed Data Parallel） FSDP（Fully Shared Data Parallel） 流水并行（Pipeline Parallelism）在不同的GPU上运行模型的不同层 模型并行（Tensor Parallelism）将单个数学运算（如矩阵乘法）拆分到不同的GPU上运行 专家混合（Mixture-of-Experts）只用模型每一层中的一小部分来处理数据。 Note：Tensor Parallelism 翻译成模型并行可能并不是非常的恰当🤣\n1. 并行模型 1.1 数据并行 （Data Parallesim） 数据并行是指将相同的参数复制到多个工作节点上，并为每个工作节点分配不同的数据子集同时进行处理。每个工作节点拥有完整的神经网络模型，每次训练仅将一批数据输入模型，进行前向传播、计算误差、反向传播，最后进行参数的更新。储了参数的更新，其余的操作都是互相独立的，所以可以在多个节点上进行并发的执行。\n每个工作节点都有自己的模型和输入，当属于自己的模型参数推理完成后（产生了梯度参数），所有的工作节点会把参数（梯度参数）发给一个 Master，这个 Master 会把所有节点传进来的参数做融合，通过这些梯度参数更新生成新的模型参数，然后把这个模型的参数再发送给每个工作节点，拱他们进行下一轮的计算。\n在 Pytorch 中提供了DP（Data Parallel）、DDP（Distributed Data Parallel）、FSDP（Fully Shared Data Parallel）三种不同的数据并行方法。\n1.1.1 DP（Data Parallel）Parameter Server DP 使用了 Parameter Server（PS） 作为理论依据。PS 结构是李沐老师提出来的方法，由server节点和worker节点组成。\n[点击折叠] 李沐老师的 PS 讲解 Server 节点的主要功能是初始化和保存模型参数、接受worker节点计算出的局部梯度、汇总计算全局梯度，并更新模型参数。\nFig 2. Parameter Server Worker 节点的主要功能是各自保存部分训练数据，初始化模型，从 Server 节点拉取（Pull）最新的模型参数，再读取参数，根据训练数据计算局部梯度，上传（Push）给 Server 节点。ps：李沐老师说这个 Pull 和 Push 叫法来源于 Git。\n在这个模式下，会有负载不均衡的情况出现。因为在 Server 需要大量的显存来保存从 Woker 到来的梯度参数，而 Server 还需要将更新后的参数广播到每一个 Worker 上，那么 Server 的网络带宽就变得是非重要了。随着 Worker 数目的增多，网络的需求会更加大。并且 Server 作为最终处理所有 Worker 传来的梯度参数的机器，其计算消耗时间也是制约其他 Worker 不间断工作的瓶颈。\n1.1.2 DDP（Distributed Data Parallel）Ring-All-Reduce DDP 主要基于 Ring-All-Reduce 算法，该算法主要分两步：\nshare-reduce：会逐步交换彼此的梯度并融合，最后每个 GPU 都会包含完整融合梯度的一部分。 share-only：GPU 会逐步交换彼此不完整的融合梯度，最后所有 GPU 都会得到完整的融合梯度 Fig 3. Ring-All-Reduce 示意图 在 share-reduce 阶段，每个进程 p 将数据发送给进程 (p+1) % p，其中 % 是模运算符。所以进程A会向进程 B 发送。这就是创建类似于环形队列状态的的原因。此外，长度为 n 的数组被划分成 p 块，这些块中的每一个都将以 i 为索引。它看起来会是这样的：\nFig 4. Ring-All-Reduce 1 第一个 share-reduce 操作进程 A 向进程 B 发送 a0，进程 B 将向进程 C 发送 b1，等等。类似这样：\nFig 5. Ring-All-Reduce 2 然后，当每个进程收到前一个进程的数据时，它就会应用 reduce operation，然后继续将其再次发送到环中的下一个进程。如果 reduce operation 是 sum。它看起来就会像这样：\nFig 6. Ring-All-Reduce 3 然后，再进一步：\nFig 7. Ring-All-Reduce 4 第二步，shared-only，这个操作非常的简单，只是以环形方式共享数据，而不应用 reduce operation。这就把每个进程中的每个分块的结果合并起来。\nFig 8. Ring-All-Reduce 5 Ring-All-Reduce 有这些优点：\n负载分散在每个 Worker 上，通讯时间基本是一致的。并且不需要通过 Server 分发全模型的参数到每个 Worker 上。 使用 ring-all-reduce 的方式进行通讯，随着 Worker 数量 N 增加，总传输量恒定。也就是理论上，随着 Worker 数量的增加，ring all-reduce 有线性性能拓展能力。 DP 和 DDP 之间的区别\nData Parallel Distributed Data Parallel 更多的开销；模型在每次参数聚合后需要被复制，并在每次前向传递后被销毁（因为只需要梯度参数被传回就够了）。 模型只会被复制一次 只支持单个节点的 Master 可以拓展到多台机器上 单进程多线程的实现方式 采用多进程的方式 1.1.3 FSDP（Fully Shared Data Parallel） DDP 用起来简单方便，但是要求整个模型能加载一个GPU上，这使得大模型的训练需要使用额外复杂的设置进行模型拆分。Pytorch 的 FSDP 从 DeepSpeed ZeRO [8] 以及 FairScale 的 FSDP 中获取灵感，打破模型分片的障碍（包括模型参数，梯度，优化器状态），同时仍然保持了数据并行的简单性。\nFSDP 是对模型参数、优化器状态和梯度进行分片，可以选择将部分模型参数卸载到 CPU 中。FSDP 是在 DDP 的基础上，将 All-Reduce 操作分解为独立的 Reduce-Scatter 和 All-gather 的操作这种方式能够使得每个工作节点仅需存储一个参数和优化器状态的分片。FSDP 非常类似于 ZeRO-3。\nFig 9. FSDP workflow [3] 如 FSDP 完全流程图所示，通过 All-gather 操作可以从其他工作节点获取模型权重来进行前向传播和反向传播。Reduce-Scatter 操作聚合局部梯度，并在各工作的节点上分片，还可以选择将梯度卸载到CPU 中，等需要时再从 CPU 中加载回来。\nFig 10. Reduce-Scatter and All-gather 原本的 All-Reduce 操作可以拆分成 Reduce-Scatter 和 All-gather 操作。在进行Reduce-Scatter 时，gradients 在相同块的各个 ranks 中被加起来，在进行 All-gather 时，每个 Worker 上可用的聚合梯度分片的部分可供所有 Worker 使用。\n1.2 流水并行（Pipeline Parallesim） 通过流水线机制进行并行训练，我们将模型的参数分配到GPU上。每个GPU只持有一部分参数，因此，同一个模型在每个GPU上消耗的内存成比例减小。\n将大模型分为若干份连续的layer很简单。然而，各层的输入和输出之间存在着顺序上的依赖性，所以一个简单的实现会导致大量的GPU空闲时间，因为 Worker 在等待前一个 Worker 的输出来用作其输入。这些等待时间块被称为 \u0026ldquo;气泡(bubble)\u0026rdquo;，而在这些等待的过程中，空闲的机器本可以继续进行计算。。\nFig 11. Naive way for Streaming [1] 为了减少气泡的开销，我们可以重用数据并行的思想，通过让每个 Worker 一次只处理一个数据元素的子集，使我们能够巧妙地将新的计算与等待时间重叠(overlapping)起来。核心思想是将一个批次分成多个微批(microbatches)；每个微批的处理速度应该是成比例的，每个 Worker 在下一个微批可用时就开始工作，从而加速流的执行。有了足够的微批，Worker 可以在大部分时间内被利用，在进程的开始和结束时，气泡最小。梯度是跨微批的平均数，只有在所有微批完成后才会对参数进行更新。\n模型被分割的节点数的数量通常被称为流水线深度(Pipline depth)。\n在前向传递期间，Worker 只需要将其 Layer 的输出（称为激活）发送给下一个 Worker；在反向传递期间，它只将这些激活的梯度发送给前一个 Worker 。如何安排这些传递以及如何在微批中聚集梯度，有很大的设计空间。 GPipe让每个工作者连续处理前向和反向的传递，然后在最后同步聚合来自多个微批的梯度。而 PipeDream 则安排每个 Worker 交替地处理前向和反向 Stream。\nFig 12. GPipe and PipeDream [1] 1.3 模型并行（Tensor Parallesim） 流水并行将一个模型按层“垂直”拆分。也可以在一个 layer 内 “水平” 分割某些操作，这通常被称为模型并行训练。对于许多现代模型（如Transformer）来说，计算的瓶颈是将激活值与大的权重矩阵相乘。矩阵乘法可以被认为是成对的行和列之间的点积：有可能在不同的GPU上计算独立的点积，或者在不同的GPU上计算每个点积的一部分，然后将结果相加。无论采用哪种策略，我们都可以将权重矩阵切成大小均匀的 “块(Shards)”，将每个块放在不同的GPU上。要得到完整矩阵的结果，需要进行通信将不同部分的结果进行整合。\n一个例子是 Megatron-LM，它在 Transformer 的 自注意 和 MLP层内并行化矩阵乘法。 PTD-P 同时使用张量、数据和流水线并行；它的流水线并行将多个不连续的 layer 分配到单设备上运行，以更多网络通信为代价来减少气泡开销。\n有时，网络的输入可以在一个维度上进行并行化，相对于交叉通信来说，并行计算的程度很高。 序列并行就是这样一个想法，一个输入序列在不同时间被分割成多个子集，通过在更细粒度的子集上进行计算，峰值内存消耗可以成比例地减少。\n1.4 混合并行（Mixture-of-Experts） 混合专家（MoE）方法，对于任何一个输入，只有一部分网络被用来计算输出。在有许多套权重的情况下，网络可以在推理时通过门控机制选择使用哪一套。这样就可以在不增加计算成本的情况下增加许多参数。每组权重被称为 \u0026ldquo;专家(experts)\u0026quot;，希望网络能够学会将专门的计算任务分配给每个专家。不同的专家可以托管在不同的GPU上，这也为扩大模型使用的GPU数量提供了一个明确的方法。\n混合专家（MoEIllustration of a mixture-of-experts (MoE) layer. Only 2 out of the n experts are selected by the gating network. (Image adapted from: Shazeer et al., 2017)[1]\nGShard将 MoE Transformer 的规模扩大到6000亿个参数，其中MoE layers被拆分到多个TPU上，其他layers是完全重复的。 Switch Transformer 通过将一个输入只路由到一个专家，将模型规模扩展到数万亿的参数，甚至有更高的稀疏度。\n难点：\n现在的设备，比如GPU，比较擅长做运算，不擅长做分支。 大批量是训练模型的必须，但是这种方式下会导致每个小模型的样本数较少，无法训练得到好的模型。 为了控制稀疏性，可能需要在loss上去做些改进，来保障小模型上的均衡负载。 Moe Github, code\n2. 自动并行方法 2.1 Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks [9] Get This Paper\n这篇文章勾勒了自动并行的基本框架，很多解决自动并行的工作都是这样一个流程。\nReference:\n[1] Techniques for training large neural networks (openai.com)\n[2] 先进编译实验室-自动并行-并行划分 - 知乎 (zhihu.com)\n[3] Getting Started with Fully Sharded Data Parallel(FSDP)\n[4] chenzomi12/DeepLearningSystem: Deep Learning System core principles introduction. (github.com)\n[5] Deep Learning in a Nutshell: Core Concepts | NVIDIA Technical Blog\n[6] Visual intuition on ring-Allreduce for distributed Deep Learning | by Edir Garcia Lazo | Towards Data Science\n[7] 数据并行Deep-dive: 从DP 到 Fully Sharded Data Parallel （FSDP）完全分片数据并行 - 知乎 (zhihu.com)\n[8] DeepSpeed: Extreme-scale model training for everyone - Microsoft Research\n[9] Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks\n","permalink":"https://chenghuawang.github.io/keep-moving-forward/tech/introduction_mldistri/","summary":"数据并行、模型并行、流水并行、专家混合","title":"浅析机器学习中的并行模型和自动并行方法"},{"content":"NSight System Document\nWSL 2 的 cudaMallocHost() 不能正常申请到 VM 的内存。也许是 WSL 2 上的 cuda 是 ubuntu20.04 的版本，不是 WSL 2 特供版。WSL 2 的 cuda 也有一些限制，详细见 WSL2 User guide 。\n1. 什么是 Nsight System 我们先看下 Nsight System 官网对该工具的描述：\nNVIDIA Nsight™ Systems is a system-wide performance analysis tool designed to visualize an application’s algorithms, help you identify the largest opportunities to optimize, and tune to scale efficiently across any quantity or size of CPUs and GPUs, from large servers to our smallest system on a chip (SoC).\n如 gperoftools 一样，这是个性能调优工具，聚焦在 N 家的 GPU 上，当然，CPU 也是在其性能分析的范围内。Nsight system 更多的时候是查看 Memory Stream(Host2Device, Device2Host) 和 计算 Kernel 之间的关系，查看有无合理的填充满流水线，更好的利用 GPU 的并行性。\nNsight system 主要是通过采样和追踪来做抓取系统信息：\nsampling 是硬件层面的实现 ，利用了Linux OS\u0026rsquo; perf subsystem，跟Linux perf工具用的一样，周期性地停止目标程序（比如每100w个cycle），收集每个线程的 CPU Instruction Pointers(IP, 指令指针)，便于了解某一时刻系统的执行状态。 tracing 是精确地采集各个活动开始和结束的时间，便于了解系统各个环节的时间开销和运转情况。 2. 如何使用 Nsight System 我以 BGR 2 YUV 的例子(代码来自于[1])来展示了 Nsight system 的信息。该示例使用了两种方式：1. 单 Stream 执行；2. 16 Stream 执行(Stream 的数量没有明确的限制，但是貌似在我的机器上，性能最优的结果就是 16，应该 stream 多了以后就后会被加入调度队列了)。\n一般在调优的时候先使用 Nsight system 来大体的看一下同步，overlap 数据搬运和计算等是不是合理。对于 Kernel 的调优一般是在 Nsight compute 中。当然，该工具实际上也可以来监测 Graphic 相关的东西，不仅仅是只有 CUDA。\n为了便利，直接使用 GUI 界面来操作，个人也推荐 GUI 启动，毕竟最终还是要看时间轴图来的直观。对于没装 GUI 的机器，也可以使用 SSH 远程连接它，便于操作。\n对于 BGR 2 YUV 的例子，我们通过在 GUI 中设置程序的位置和程序的启动命令即可配置完成一个 Nsight system Project。(可以看到，这个perf工具还支持很多的信息统计，如 Vulkan 和 OpenGL)\nFig 1. NSight System Project Settings. 在配置完采样信息，需要追踪的信息后，点击 Start 就愉快的开始程序的分析了。在分析后的 Timeline View 中，我们可以清晰的看到每个阶段的时间消耗。\nFig 2. 1 Stream. 比如在 BGR 2 YUV 的第一个例子中(上图，只使用单个Stream)，从时间轴上可以看到并行性并没有起来，我们可以多开几个 Stream 让数据传输和计算并行起来。通让每个 Stream 做不同的工作(数据搬运，计算)，来最大化并行。如下图所示:\nFig 3. 16 streams. code:\n[Click to expand] 主项目 CMake 设置\ncmake_minimum_required(VERSION 3.18) project( cmake_learn LANGUAGES CXX CUDA ) if(CUDA_ENABLE) enable_language(CUDA) endif() set(CMAKE_EXPORT_COMPILE_COMMANDS ON CACHE BOOL \u0026#34;\u0026#34;) message(STATUS \u0026#34;cuda version: \u0026#34; ${CUDA_VERSION_STRING}) include_directories(${CUDA_INCLUDE_DIRS}) add_subdirectory(\u0026#34;./stream\u0026#34;) 子项目 CMake 设置\nproject(cuda_stream) add_executable(cuda_stream main.cu) add_compile_options(--cuda-gpu-arch=sm_20) main.cu\n#include \u0026lt;vector\u0026gt; #include \u0026lt;random\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;cuda.h\u0026gt; #include \u0026lt;cuda_runtime.h\u0026gt; #ifdef DEBUG #define CUDA_CALL(F) if( (F) != cudaSuccess ) \\ {printf(\u0026#34;Error %s at %s:%d\\n\u0026#34;, cudaGetErrorString(cudaGetLastError()), \\ __FILE__,__LINE__); exit(-1);} #define CUDA_CHECK() if( (cudaPeekAtLastError()) != cudaSuccess ) \\ {printf(\u0026#34;Error %s at %s:%d\\n\u0026#34;, cudaGetErrorString(cudaGetLastError()), \\ __FILE__,__LINE__-1); exit(-1);} #else #define CUDA_CALL(F) (F) #define CUDA_CHECK() #endif void PrintDeviceInfo(); void GenerateBgra8K(uint8_t* buffer, int dataSize); void convertPixelFormatCpu(uint8_t* inputBgra, uint8_t* outputYuv, int numPixels); __global__ void convertPixelFormat(uint8_t* inputBgra, uint8_t* outputYuv, int numPixels); int main() { PrintDeviceInfo(); uint8_t* bgraBuffer; uint8_t* yuvBuffer; uint8_t* deviceBgraBuffer; uint8_t* deviceYuvBuffer; const int dataSizeBgra = 7680 * 4320 * 4; const int dataSizeYuv = 7680 * 4320 * 3; CUDA_CALL(cudaMallocHost(\u0026amp;bgraBuffer, dataSizeBgra)); CUDA_CALL(cudaMallocHost(\u0026amp;yuvBuffer, dataSizeYuv)); CUDA_CALL(cudaMalloc(\u0026amp;deviceBgraBuffer, dataSizeBgra)); CUDA_CALL(cudaMalloc(\u0026amp;deviceYuvBuffer, dataSizeYuv)); std::vector\u0026lt;uint8_t\u0026gt; yuvCpuBuffer(dataSizeYuv); cudaEvent_t start, stop; float elapsedTime; float elapsedTimeTotal; float dataRate; CUDA_CALL(cudaEventCreate(\u0026amp;start)); CUDA_CALL(cudaEventCreate(\u0026amp;stop)); std::cout \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;Generating 7680 x 4320 BRGA8888 image, data size: \u0026#34; \u0026lt;\u0026lt; dataSizeBgra \u0026lt;\u0026lt; std::endl; GenerateBgra8K(bgraBuffer, dataSizeBgra); std::cout \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;Computing results using CPU.\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; std::endl; CUDA_CALL(cudaEventRecord(start, 0)); convertPixelFormatCpu(bgraBuffer, yuvCpuBuffer.data(), 7680*4320); CUDA_CALL(cudaEventRecord(stop, 0)); CUDA_CALL(cudaEventSynchronize(stop)); CUDA_CALL(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); std::cout \u0026lt;\u0026lt; \u0026#34; Whole process took \u0026#34; \u0026lt;\u0026lt; elapsedTime \u0026lt;\u0026lt; \u0026#34;ms.\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;Computing results using GPU, default stream.\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; Move data to GPU.\u0026#34; \u0026lt;\u0026lt; std::endl; CUDA_CALL(cudaEventRecord(start, 0)); CUDA_CALL(cudaMemcpy(deviceBgraBuffer, bgraBuffer, dataSizeBgra, cudaMemcpyHostToDevice)); CUDA_CALL(cudaEventRecord(stop, 0)); CUDA_CALL(cudaEventSynchronize(stop)); CUDA_CALL(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); dataRate = dataSizeBgra/(elapsedTime/1000.0)/1.0e9; elapsedTimeTotal = elapsedTime; std::cout \u0026lt;\u0026lt; \u0026#34; Data transfer took \u0026#34; \u0026lt;\u0026lt; elapsedTime \u0026lt;\u0026lt; \u0026#34;ms.\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; Performance is \u0026#34; \u0026lt;\u0026lt; dataRate \u0026lt;\u0026lt; \u0026#34;GB/s.\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; Convert 8-bit BGRA to 8-bit YUV.\u0026#34; \u0026lt;\u0026lt; std::endl; CUDA_CALL(cudaEventRecord(start, 0)); convertPixelFormat\u0026lt;\u0026lt;\u0026lt;32400, 1024\u0026gt;\u0026gt;\u0026gt;(deviceBgraBuffer, deviceYuvBuffer, 7680*4320); CUDA_CHECK(); CUDA_CALL(cudaDeviceSynchronize()); CUDA_CALL(cudaEventRecord(stop, 0)); CUDA_CALL(cudaEventSynchronize(stop)); CUDA_CALL(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); dataRate = dataSizeBgra/(elapsedTime/1000.0)/1.0e9; elapsedTimeTotal += elapsedTime; std::cout \u0026lt;\u0026lt; \u0026#34; Processing of 8K image took \u0026#34; \u0026lt;\u0026lt; elapsedTime \u0026lt;\u0026lt; \u0026#34;ms.\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; Performance is \u0026#34; \u0026lt;\u0026lt; dataRate \u0026lt;\u0026lt; \u0026#34;GB/s.\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; Move data to CPU.\u0026#34; \u0026lt;\u0026lt; std::endl; CUDA_CALL(cudaEventRecord(start, 0)); CUDA_CALL(cudaMemcpy(yuvBuffer, deviceYuvBuffer, dataSizeYuv, cudaMemcpyDeviceToHost)); CUDA_CALL(cudaEventRecord(stop, 0)); CUDA_CALL(cudaEventSynchronize(stop)); CUDA_CALL(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); dataRate = dataSizeYuv/(elapsedTime/1000.0)/1.0e9; elapsedTimeTotal += elapsedTime; std::cout \u0026lt;\u0026lt; \u0026#34; Data transfer took \u0026#34; \u0026lt;\u0026lt; elapsedTime \u0026lt;\u0026lt; \u0026#34;ms.\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; Performance is \u0026#34; \u0026lt;\u0026lt; dataRate \u0026lt;\u0026lt; \u0026#34;GB/s.\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; Whole process took \u0026#34; \u0026lt;\u0026lt; elapsedTimeTotal \u0026lt;\u0026lt; \u0026#34;ms.\u0026#34; \u0026lt;\u0026lt;std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; Compare CPU and GPU results ...\u0026#34; \u0026lt;\u0026lt; std::endl; bool foundMistake = false; for(int i=0; i\u0026lt;dataSizeYuv; i++){ if(yuvCpuBuffer[i]!=yuvBuffer[i]){ foundMistake = true; break; } } if(foundMistake){ std::cout \u0026lt;\u0026lt; \u0026#34; Results are NOT the same.\u0026#34; \u0026lt;\u0026lt; std::endl; } else { std::cout \u0026lt;\u0026lt; \u0026#34; Results are the same.\u0026#34; \u0026lt;\u0026lt; std::endl; } const int nStreams = 16; std::cout \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;Computing results using GPU, using \u0026#34;\u0026lt;\u0026lt; nStreams \u0026lt;\u0026lt;\u0026#34; streams.\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; std::endl; cudaStream_t streams[nStreams]; std::cout \u0026lt;\u0026lt; \u0026#34; Creating \u0026#34; \u0026lt;\u0026lt; nStreams \u0026lt;\u0026lt; \u0026#34; CUDA streams.\u0026#34; \u0026lt;\u0026lt; std::endl; for (int i = 0; i \u0026lt; nStreams; i++) { CUDA_CALL(cudaStreamCreate(\u0026amp;streams[i])); } int brgaOffset = 0; int yuvOffset = 0; const int brgaChunkSize = dataSizeBgra / nStreams; const int yuvChunkSize = dataSizeYuv / nStreams; CUDA_CALL(cudaEventRecord(start, 0)); for(int i=0; i\u0026lt;nStreams; i++) { std::cout \u0026lt;\u0026lt; \u0026#34; Launching stream \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;.\u0026#34; \u0026lt;\u0026lt; std::endl; brgaOffset = brgaChunkSize*i; yuvOffset = yuvChunkSize*i; CUDA_CALL(cudaMemcpyAsync( deviceBgraBuffer+brgaOffset, bgraBuffer+brgaOffset, brgaChunkSize, cudaMemcpyHostToDevice, streams[i] )); convertPixelFormat\u0026lt;\u0026lt;\u0026lt;4096, 1024, 0, streams[i]\u0026gt;\u0026gt;\u0026gt;(deviceBgraBuffer+brgaOffset, deviceYuvBuffer+yuvOffset, brgaChunkSize/4); CUDA_CALL(cudaMemcpyAsync( yuvBuffer+yuvOffset, deviceYuvBuffer+yuvOffset, yuvChunkSize, cudaMemcpyDeviceToHost, streams[i] )); } CUDA_CHECK(); CUDA_CALL(cudaDeviceSynchronize()); CUDA_CALL(cudaEventRecord(stop, 0)); CUDA_CALL(cudaEventSynchronize(stop)); CUDA_CALL(cudaEventElapsedTime(\u0026amp;elapsedTime, start, stop)); std::cout \u0026lt;\u0026lt; \u0026#34; Whole process took \u0026#34; \u0026lt;\u0026lt; elapsedTime \u0026lt;\u0026lt; \u0026#34;ms.\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; Compare CPU and GPU results ...\u0026#34; \u0026lt;\u0026lt; std::endl; for(int i=0; i\u0026lt;dataSizeYuv; i++){ if(yuvCpuBuffer[i]!=yuvBuffer[i]){ foundMistake = true; break; } } if(foundMistake){ std::cout \u0026lt;\u0026lt; \u0026#34; Results are NOT the same.\u0026#34; \u0026lt;\u0026lt; std::endl; } else { std::cout \u0026lt;\u0026lt; \u0026#34; Results are the same.\u0026#34; \u0026lt;\u0026lt; std::endl; } CUDA_CALL(cudaFreeHost(bgraBuffer)); CUDA_CALL(cudaFreeHost(yuvBuffer)); CUDA_CALL(cudaFree(deviceBgraBuffer)); CUDA_CALL(cudaFree(deviceYuvBuffer)); return 0; } void PrintDeviceInfo(){ int deviceCount = 0; cudaGetDeviceCount(\u0026amp;deviceCount); std::cout \u0026lt;\u0026lt; \u0026#34;Number of device(s): \u0026#34; \u0026lt;\u0026lt; deviceCount \u0026lt;\u0026lt; std::endl; if (deviceCount == 0) { std::cout \u0026lt;\u0026lt; \u0026#34;There is no device supporting CUDA\u0026#34; \u0026lt;\u0026lt; std::endl; return; } cudaDeviceProp info; for(int i=0; i\u0026lt;deviceCount; i++){ cudaGetDeviceProperties(\u0026amp;info, i); std::cout \u0026lt;\u0026lt; \u0026#34;Device \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; Name: \u0026#34; \u0026lt;\u0026lt; std::string(info.name) \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; Glocbal memory: \u0026#34; \u0026lt;\u0026lt; info.totalGlobalMem/1024.0/1024.0 \u0026lt;\u0026lt; \u0026#34; MB\u0026#34;\u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; Shared memory per block: \u0026#34; \u0026lt;\u0026lt; info.sharedMemPerBlock/1024.0 \u0026lt;\u0026lt; \u0026#34; KB\u0026#34;\u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; Warp size: \u0026#34; \u0026lt;\u0026lt; info.warpSize\u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; Max thread per block: \u0026#34; \u0026lt;\u0026lt; info.maxThreadsPerBlock\u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; Thread dimension limits: \u0026#34; \u0026lt;\u0026lt; info.maxThreadsDim[0]\u0026lt;\u0026lt; \u0026#34; x \u0026#34; \u0026lt;\u0026lt; info.maxThreadsDim[1]\u0026lt;\u0026lt; \u0026#34; x \u0026#34; \u0026lt;\u0026lt; info.maxThreadsDim[2]\u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; Max grid size: \u0026#34; \u0026lt;\u0026lt; info.maxGridSize[0]\u0026lt;\u0026lt; \u0026#34; x \u0026#34; \u0026lt;\u0026lt; info.maxGridSize[1]\u0026lt;\u0026lt; \u0026#34; x \u0026#34; \u0026lt;\u0026lt; info.maxGridSize[2]\u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; Compute capability: \u0026#34; \u0026lt;\u0026lt; info.major \u0026lt;\u0026lt; \u0026#34;.\u0026#34; \u0026lt;\u0026lt; info.minor \u0026lt;\u0026lt; std::endl; } } void GenerateBgra8K(uint8_t* buffer, int dataSize){ std::random_device rd; std::mt19937 gen(rd()); std::uniform_int_distribution\u0026lt;\u0026gt; sampler(0, 255); for(int i=0; i\u0026lt;dataSize/4; i++){ buffer[i*4] = sampler(gen); buffer[i*4+1] = sampler(gen); buffer[i*4+2] = sampler(gen); buffer[i*4+3] = 255; } } void convertPixelFormatCpu(uint8_t* inputBgra, uint8_t* outputYuv, int numPixels){ short3 yuv16; char3 yuv8; for(int idx=0; idx\u0026lt;numPixels; idx++){ yuv16.x = 66*inputBgra[idx*4+2] + 129*inputBgra[idx*4+1] + 25*inputBgra[idx*4]; yuv16.y = -38*inputBgra[idx*4+2] + -74*inputBgra[idx*4+1] + 112*inputBgra[idx*4]; yuv16.z = 112*inputBgra[idx*4+2] + -94*inputBgra[idx*4+1] + -18*inputBgra[idx*4]; yuv8.x = (yuv16.x\u0026gt;\u0026gt;8)+16; yuv8.y = (yuv16.y\u0026gt;\u0026gt;8)+128; yuv8.z = (yuv16.z\u0026gt;\u0026gt;8)+128; *(reinterpret_cast\u0026lt;char3*\u0026gt;(\u0026amp;outputYuv[idx*3])) = yuv8; } } __global__ void convertPixelFormat(uint8_t* inputBgra, uint8_t* outputYuv, int numPixels){ int stride = gridDim.x * blockDim.x; int idx = threadIdx.x + blockIdx.x * blockDim.x; short3 yuv16; char3 yuv8; while(idx\u0026lt;=numPixels){ if(idx\u0026lt;numPixels){ yuv16.x = 66*inputBgra[idx*4+2] + 129*inputBgra[idx*4+1] + 25*inputBgra[idx*4]; yuv16.y = -38*inputBgra[idx*4+2] + -74*inputBgra[idx*4+1] + 112*inputBgra[idx*4]; yuv16.z = 112*inputBgra[idx*4+2] + -94*inputBgra[idx*4+1] + -18*inputBgra[idx*4]; yuv8.x = (yuv16.x\u0026gt;\u0026gt;8)+16; yuv8.y = (yuv16.y\u0026gt;\u0026gt;8)+128; yuv8.z = (yuv16.z\u0026gt;\u0026gt;8)+128; *(reinterpret_cast\u0026lt;char3*\u0026gt;(\u0026amp;outputYuv[idx*3])) = yuv8; } idx += stride; } } Reference:\n[1] CUDA随笔之Stream的使用\n","permalink":"https://chenghuawang.github.io/keep-moving-forward/tech/cuda_nsight_system/","summary":"Usage of Nsight System","title":"CUDA: NSight System"},{"content":"xv6 中的 fork() 系统调用将父进程的所有用户空间内存复制到子进程中。如果父进程很大，复制可能需要很长的时间。更糟糕的是，这项工作通常是无用的：fork() 之后通常是子进程的 exec()，它放弃了复制的内存，没有使用复制过来的大部分内存。如果父代和子代都使用了复制的页面，并且其中一个或两个都写了它，那么这个复制才是真正需要的。\n所以写时复制(copy on write, cow) 这个技术变得十分的重要。在这个 lab 中，我们需要实现一个写时复制的 fork()。我们需要修改原本的 fork() 程序中做内存申请的模块，同时我们还需要实现 usertrap 来在写时(在实际写的时候碰到 cow flag，抛出分页错误)的时候处理这个 trap。\n因为一个程序很可能被 fork() 了很多的分支，所以我们需要一个计数器来确定这个 page 是要被释放还是保留。\n一般来说，一个正常运行的程序在写时复制的时候会有下面几个流程:\nfork() 出一个子进程 child。 child 拥有父进程的页的引用，并且页的 flag 有 cow 标识。 并且要把 页 引用 ++ child 需要向自己的页中写入新的数据. 此时需要重新分配内存，页引用 \u0026ndash;。 当 child 返回的时候，可能有内存需要由 kernel 转换到 user。 当父进程销毁的时候，如果计数器为 0，则销毁，反之，不变。 每一页都需要一个计数器来进行计数，所以我们需要在 kernel 中加入一个数组来记录，查阅 xv6 book，我们发现有如下的图:\nFig 1. memlayoutXV6 手册\n我们需要在 kernel data 之后的区域内申请一块内存来作为计数器存储的数组。我们可以在 kalloc.c 中实现。\n// ./kernel/kalloc.c struct { struct spinlock lock; struct run *freelist; // lab 5 uint* ref_cnt; struct spinlock ref_cnt_lock; char * pa_start; } kmem; 在这个 kmem 结构体中加入了一个 ref_cnt_lock 来保证计数器的正确引用。一个 ref_cnt pointer 来指示 ref array 的起始位置，使用 pa_start 来表示实际的 free memory 的起始位置。我们还需要修改初始化程序来正确的申请计数器数组，并且对 free memory 填充上一个初始值。\n// ./kernel/kalloc.c void kinit() { initlock(\u0026amp;kmem.lock, \u0026#34;kmem\u0026#34;); // lab 5 initlock(\u0026amp;kmem.ref_cnt_lock, \u0026#34;kmemrefcnt\u0026#34;); uint64 pg_size = (((PHYSTOP - (uint64)end)) \u0026gt;\u0026gt; PGSHIFT) + 1; // get the page size uint64 ref_array_size = ((pg_size * sizeof(uint)) \u0026gt;\u0026gt; PGSHIFT) + 1; // caculate how many page wes need to store ref cnt. kmem.ref_cnt = (uint*)end; // the start of ref cnt array kmem.pa_start = end + (ref_array_size \u0026lt;\u0026lt; PGSHIFT); // the start of user memory space. char *p; p = (char*)PGROUNDUP((uint64)kmem.pa_start); for (; p + PGSIZE \u0026lt;= (char*)PHYSTOP; p+= PGSIZE) { acquire(\u0026amp;kmem.ref_cnt_lock); uint64 idx = (uint64)((PGROUNDDOWN((uint64)p) - PGROUNDUP((uint64)kmem.pa_start)) / PGSIZE); kmem.ref_cnt[idx] = 1; // set to 1, force freerange(.., ...) function to remove all mem in start-\u0026gt;end scope. release(\u0026amp;kmem.ref_cnt_lock); } freerange((void*)kmem.pa_start, (void*)PHYSTOP); // free memory in those scope. } PGSHIFT, PHYSTOP 是定义在 riscv.h 中便于使用的 macro。下面是 kfree 的代码，需要在 ref \u0026gt; 1 的时候减少引用，在 ref == 1 的时候释放这块内存，并且把计数器的引用次数置成 0。\nvoid kfree(void *pa) { struct run *r; // lab 5 // If ref is not 0 acquire(\u0026amp;kmem.ref_cnt_lock); // get the ref cnt location of pa in ref cnt array uint64 idx = (uint64)((PGROUNDDOWN((uint64)pa) - PGROUNDUP((uint64)kmem.pa_start)) / PGSIZE); if (kmem.ref_cnt[idx] \u0026gt; 1) { kmem.ref_cnt[idx] --; release(\u0026amp;kmem.ref_cnt_lock); return; } // If ref is 1; if(((uint64)pa % PGSIZE) != 0 || (char*)pa \u0026lt; end || (uint64)pa \u0026gt;= PHYSTOP) panic(\u0026#34;kfree\u0026#34;); // Fill with junk to catch dangling refs. memset(pa, 1, PGSIZE); // lab 5 kmem.ref_cnt[idx] = 0; release(\u0026amp;kmem.ref_cnt_lock); r = (struct run*)pa; acquire(\u0026amp;kmem.lock); r-\u0026gt;next = kmem.freelist; kmem.freelist = r; release(\u0026amp;kmem.lock); } 在申请内存的同时，也要加上计数器:\nvoid * kalloc(void) { struct run *r; acquire(\u0026amp;kmem.lock); r = kmem.freelist; if(r) kmem.freelist = r-\u0026gt;next; release(\u0026amp;kmem.lock); // lab 5 if (r) { acquire(\u0026amp;kmem.ref_cnt_lock); uint64 idx = (uint64)((PGROUNDDOWN((uint64)r) - PGROUNDUP((uint64)kmem.pa_start)) / PGSIZE); kmem.ref_cnt[idx] = 1; release(\u0026amp;kmem.ref_cnt_lock); } if(r) memset((char*)r, 5, PGSIZE); // fill with junk return (void*)r; } 在 vm.c 中我们需要修改 uvmcopy 函数，在这个函数中，我们要注释掉原来的内存申请的代码，然后使用 cow 策略来“申请”内存\nfor (i = 0; i \u0026lt; sz; i += PGSIZE) { if ((pte = walk(old, i, 0)) == 0) { panic(\u0026#34;uvmcopy: pte should exist\u0026#34;); } if ((*pte \u0026amp; PTE_V) == 0) { panic(\u0026#34;uvmcopy: page not present\u0026#34;); } pa = PTE2PA(*pte); // clear write flag and add read flag. *pte = ((*pte) \u0026amp; (~PTE_W)) | PTE_COW; flags = PTE_FLAGS(*pte); // Map to child process\u0026#39;s pagetable directly. Make both read-only. if (mappages(new, i, PGSIZE, pa, flags) != 0) { goto err; } // increase ref of page old. kincre_ref(pa); } return 0; 在上面的这段代码中，我将 flag 由原来的可写转变成了 cow(PTE_COW 需要在 reiscv.h 中定义)，然后将页共享，如果共享成功，则将 reference ++。\n当用户需要写内存的时候，会触发一个 trap，同上一个实验，我们需要在 usertrap 中加入代码来处理这个 trap。\nelse if (r_scause() == 15) { uint64 va = r_stval(); if (va \u0026gt; p-\u0026gt;sz) p-\u0026gt;killed = 1; else if (kalloc_cow(p-\u0026gt;pagetable, va) != 0) p-\u0026gt;killed = 1; } int kalloc_cow(pagetable_t pagetable, uint64 va) { va = PGROUNDDOWN(va); if (va \u0026gt;= MAXVA) return -1; pte_t *pte = walk(pagetable, va, 0); if (!pte) return -1; uint64 pa = PTE2PA(*pte); if (!pa) return -1; uint64 flags = PTE_FLAGS(*pte); if (flags \u0026amp; PTE_COW) { uint64 mem = (uint64)kalloc(); if (!mem) return -1; memmove((char*)mem, (char*)pa, PGSIZE); uvmunmap(pagetable, va, 1, 1); flags = (flags | PTE_W) \u0026amp; (~PTE_COW); if (mappages(pagetable, va, PGSIZE, mem, flags) != 0) { kfree((void*)mem); return -1; } } return 0; } 当用户所访问的内存需要写时复制的时候，由 trap 处理程序来进行处理，kalloc_cow 程序负责申请出与原来的内存区块相同大小的区间，然后把内存 copy 过去，并且把 flag 改为可写而不是 cow。\n当 kernel 需要把内存交换到 user 区时，也需要对 cow 特殊对待。在 copyout 函数中，需要加入下述代码:\nif (kalloc_cow(pagetable, va0) != 0) return -1; ","permalink":"https://chenghuawang.github.io/keep-moving-forward/tech/xv6_lab5_cow/","summary":"xv6 中的 fork() 系统调用将父进程的所有用户空间内存复制到子进程中。如果父进程很大，复制可能需要很长的时间。更糟糕的是，这项工作通常是无用的：fork() 之后通常是子进程的 exec()，它放弃了复制的内存，没有使用复制过来的大部分内存。如果父代和子代都使用了复制的页面，并且其中一个或两个都写了它，那么这个复制才是真正需要的。\n所以写时复制(copy on write, cow) 这个技术变得十分的重要。在这个 lab 中，我们需要实现一个写时复制的 fork()。我们需要修改原本的 fork() 程序中做内存申请的模块，同时我们还需要实现 usertrap 来在写时(在实际写的时候碰到 cow flag，抛出分页错误)的时候处理这个 trap。\n因为一个程序很可能被 fork() 了很多的分支，所以我们需要一个计数器来确定这个 page 是要被释放还是保留。\n一般来说，一个正常运行的程序在写时复制的时候会有下面几个流程:\nfork() 出一个子进程 child。 child 拥有父进程的页的引用，并且页的 flag 有 cow 标识。 并且要把 页 引用 ++ child 需要向自己的页中写入新的数据. 此时需要重新分配内存，页引用 \u0026ndash;。 当 child 返回的时候，可能有内存需要由 kernel 转换到 user。 当父进程销毁的时候，如果计数器为 0，则销毁，反之，不变。 每一页都需要一个计数器来进行计数，所以我们需要在 kernel 中加入一个数组来记录，查阅 xv6 book，我们发现有如下的图:\nFig 1. memlayoutXV6 手册\n我们需要在 kernel data 之后的区域内申请一块内存来作为计数器存储的数组。我们可以在 kalloc.c 中实现。\n// ./kernel/kalloc.c struct { struct spinlock lock; struct run *freelist; // lab 5 uint* ref_cnt; struct spinlock ref_cnt_lock; char * pa_start; } kmem; 在这个 kmem 结构体中加入了一个 ref_cnt_lock 来保证计数器的正确引用。一个 ref_cnt pointer 来指示 ref array 的起始位置，使用 pa_start 来表示实际的 free memory 的起始位置。我们还需要修改初始化程序来正确的申请计数器数组，并且对 free memory 填充上一个初始值。","title":"XV6 Lab 5: Copy On Write"},{"content":"RISC-V assembly Which registers contain arguments to functions? For example, which register holds 13 in main\u0026rsquo;s call to printf?\nRegister: a0, a1, a2\u0026hellip;, a7 for integer arguments. Register fa0, fa1, fa2\u0026hellip;, fa7 for float arguments.\nRegister a2 holds 13 when we call printf().\nWhere is the call to function f in the assembly code for main? Where is the call to g? (Hint: the compiler may inline functions.)\nCompiler inlined f(8) and g() in printf() function. And in assembly code it just pass a immediate 12 to register a1.\nHowerver, we can set -O0 to disable advanced optimization and get the call address of function f and g.\nAt what address is the function printf located?\nThe auipc instruction get the PC address and load it to register ra. jalr instruction will combine 1562(0x61A) to 0x30 to get the final result.\n0x30 is 0011 0000 0000 0000 0000\n0x61A is 0110 0001 1010\nAnd the final result is 0x64A\nWhat value is in the register ra just after the jalr to printf in main?\nThe PC will reset to the main function(the address called printf() + 1) after printf returned. ra = 0x38.\nOutput is He110 World, there is no need to change var i when risc-v processors set to big-endian.\nThe x=3, and y depends on the register setting. (Mostly like a random number).\nBacktrace As mentioned in the question, the compiler will set the frame pointer to register s0. We need to add a piece of code in kernel/riscv.h to get the value of s0.\nstatic inline uint64 r_fp() { uint64 x; asm volatile(\u0026#34;mv %0, s0\u0026#34; : \u0026#34;=r\u0026#34;(x)); return x; } Form hints, we knows that:\nThe return address lives at a fixed offset (-8) from the frame pointer of a stackframe, and that the saved frame pointer lives at fixed offset (-16) from the frame pointer.\nThese lecture nots have a picture of layouts of stack frames.\nStack . . +-\u0026gt; . | +-----------------+ | | | return address | | | | previous fp ------+ | | saved registers | | | local variables | | | ... | \u0026lt;-+ | +-----------------+ | | | return address | | +------ previous fp | | | saved registers | | | local variables | | +-\u0026gt; | ... | | | +-----------------+ | | | return address | | | | previous fp ------+ | | saved registers | | | local variables | | | ... | \u0026lt;-+ | +-----------------+ | | | return address | | +------ previous fp | | | saved registers | | | local variables | | $fp --\u0026gt; | ... | | +-----------------+ | | return address | | | previous fp ------+ | saved registers | $sp --\u0026gt; | local variables | +-----------------+ In that case, if we want to get the live address, we need to add -8 to s0. To get the previous fp, add -16 to s0;\nThe fp is the top of one frame.\nYou can use PGROUNDDOWN(fp) (see kernel/riscv.h) to identify the page that a frame pointer refers to. But what we need to use is PGROUNDUP(fp) here.\nIn ./kernel/printf.c, we neded to impl backtrace function:\nvoid backtrace() { uint64 fp = r_fp(); uint64 bound_high = PGROUNDUP(fp); // the page frame pointer ref to. while(fp \u0026lt; bound_high) { uint64 tmp = *(uint64*)(fp - 8); fp = *(uint64*)(fp - 16); printf(\u0026#34;%p\\n\u0026#34;, tmp); } return; } Alarm In this exercise we need to add a feature to xv6 that periodically alerts a process as it uses CPU time. This might be useful for compute-bound processes that want to limit how much CPU time they chew up, or for processes that want to compute but also want to take some periodic action.\nWe should add a new sigalarm(interval, handler) system call. If an application calls sigalarm(n, fn), then after every n \u0026ldquo;ticks\u0026rdquo; of CPU time that the program consumes, the kernel should cause application function fn to be called. When fn returns, the application should resume where it left off. A tick is a fairly arbitrary unit of time in xv6, determined by how often a hardware timer generates interrupts. If an application calls sigalarm(0, 0), the kernel should stop generating periodic alarm calls.\ntest 0 In oerder to let system knows what function need to execute when a process\u0026rsquo;s alarm interval expires, we need to add some structs to proc. First, we need to store the alarm interval and functions pointer. Second, we need to backup the trapframe for user/kernel switch.\nThe figure shown below illuminate the workflow of sigalarm and sigreturn:\nFig 1. workflow© chenghua.wang\nIn ./kernel/proc.h, we need to add some infomation to let process know the state of current sigalarm/sigreturn.\nstruct proc { ... int alarm_interval; // n ticks per action int alarm_ticks_left; // how many times has timer intrupt void (*alarm_handler)(); // the function pointer for exec struct trapframe *trapframe_bk; // backup, for user/trap switch ... } Also, we need to modified alloc and free functions for process. In ./kernel/proc.c:\nstatic void allocproc(void) { ... if ((p-\u0026gt;trapframe_bk = (struct trapframe *)kalloc()) == 0) { freeproc(p); release(\u0026amp;p-\u0026gt;lock); return 0; } ... p-\u0026gt;alarm_interval = 0; p-\u0026gt;alarm_handler = 0; p-\u0026gt;alarm_ticks_left = 0; ... } static void freeproc(struct proc* p) { ... if(p-\u0026gt;trapframe_bk) kfree((void*)p-\u0026gt;trapframe_bk); ... p-\u0026gt;alarm_interval = 0; p-\u0026gt;alarm_handler = 0; p-\u0026gt;alarm_ticks_left = 0; } And we need to register this two system-call\u0026rsquo;s definitions in the ./user/usys.pl to let it generate the trap entries(it\u0026rsquo;s exactly same as previous lab).\nThen we need to handle the timer interupt. In ./kernel/trap.c, we need to impl the logic when which_dev==2. First, we need to judge if alarm_interval is zero or not. Then, increase the alarm_ticks_left and compare it with alarm_interval. If timer interupts times equles to n tick, execute the alarm_handler user passed in.\nNote that, if we want to execute the alarm_handler, the pc should point to code section in user process. So, we need to figure the epc register manually. When executing alarm_handler, registers may changed. So we need to back up current trapframe before calling alarm_handler. After alarm_handler returned, we can use this backuped trapframe to restore the register state.\nIn ./kernel/trap.c:\nvoid usertrap(void) { ... // give up the CPU if this is a timer interrupt. if(which_dev == 2) { if (p-\u0026gt;alarm_interval) { if (++p-\u0026gt;alarm_ticks_left == p-\u0026gt;alarm_interval) { *p-\u0026gt;trapframe_bk = *p-\u0026gt;trapframe; p-\u0026gt;trapframe-\u0026gt;epc = (uint64)p-\u0026gt;alarm_handler; } } yield(); } ... } Then, let us impl the sigalarm and sigreturn functions in ./kernel/sysproc.c:\nint sigalarm(int ticks, void (* handler)()) { struct proc *p = myproc(); p-\u0026gt;alarm_interval = ticks; p-\u0026gt;alarm_handler = handler; p-\u0026gt;alarm_ticks_left = 0; return 0; } uint64 sys_sigalarm(void) { int ticks; uint64 func_ptr; if (argint(0, \u0026amp;ticks), ticks \u0026lt; 0) return -1; if (argaddr(1, \u0026amp;func_ptr), func_ptr \u0026lt; 0) return -1; return sigalarm(ticks, (void(*)())func_ptr); } int sigreturn() { struct proc *p = myproc(); *p-\u0026gt;trapframe = *p-\u0026gt;trapframe_bk; p-\u0026gt;alarm_ticks_left = 0; return p-\u0026gt;trapframe-\u0026gt;a0; } uint64 sys_sigreturn(void) { return sigreturn(); } test 1/2/3 The code is shown in previous section.\nIf alarm_handler is still running while timer interupt n ticks. We need to make sure there is only one alarm_handler running in the system emitted by sigalarm.\nIn order to impl this, we can add a lock-like flag to record the state(alarm_handler is running or not)\nHowever, we can reset the alarm_ticks_left to zero in the sigreturn instead of in the function usertrap. The alarm_handler will not be called util user calls sigreturn.\nThe return value of sigreturn is stored in the a0 register, which is not the behavior we expected. This value may cover the original a0 register. So, just return p-\u0026gt;trapfraame-\u0026gt;a0 is ok. It will set the a0 register to previous value. It\u0026rsquo;s tricky but useful.\n","permalink":"https://chenghuawang.github.io/keep-moving-forward/tech/xv6_lab4_trap/","summary":"RISC-V assembly Which registers contain arguments to functions? For example, which register holds 13 in main\u0026rsquo;s call to printf?\nRegister: a0, a1, a2\u0026hellip;, a7 for integer arguments. Register fa0, fa1, fa2\u0026hellip;, fa7 for float arguments.\nRegister a2 holds 13 when we call printf().\nWhere is the call to function f in the assembly code for main? Where is the call to g? (Hint: the compiler may inline functions.)\nCompiler inlined f(8) and g() in printf() function.","title":"XV6 Lab 4: Traps"},{"content":"MIT 6.S081 Lab3 website\n做 Lab 3 需要提前阅读 XV6 book，了解 RISC-V SR39 的地址格式，并且实验中大量用到了页表的准换函数，需要查阅 XV6 手册。不过，熟记 RISC-V 的地址说实在的没有什么用处，通过这个实验理解页表的工作方式并且 hands on 才是真的。\nSpeed up system calls 目前有很多的操作系统(Linux)在用户区和内核区之间共享一块数据(Read-Only for user)，这样用户在进行系统调用的时候就不需要陷入内核态后，由内核态拷贝数据进用户态，而是将数据写在这个共享的区块内。这样可以加快操作系统的运行速度(毕竟大部分系统调用需要的内存消耗是很小的，内存的消耗在当今已经不是问题)。\n在本实验中我们需要使用 ugetpid() 来进行加速获得进程的 pid。\n首先就是为每一个进程创建一个页表作为共享内存区块。我们发现在 kernel\\memlayout.h 中已经为我们定义好了需要的数据结构:\nstruct usyscall { int pid; // Process ID }; 那么我们只需要在 kernel/proc.c /proc.h 中加入代码，来实现进程创建时创建页表，销毁时销毁页表的动作就行了。\n在 proc.h 中，我们需要在进程的 PCB 中加入新的数据结构:\nstruct proc { ... struct usyscall *usyscall; // using read only shared data to accelerate. ... } 为了让进程的正常创建和释放，我们需要向进程创建和销毁函数中加入对应的页表操作代码。\nstatic struct proc * allocproc(void) { ... found: ... // -- Modified -- // To alloc a mem for usyscall_page p-\u0026gt;usyscall_page = (struct usyscall *)kalloc(); if (p-\u0026gt;usyscall_page == 0) { freeproc(p); release(\u0026amp;p-\u0026gt;lock); return 0; } ... // -- Modified -- // Init the syscall pid p-\u0026gt;usyscall_page-\u0026gt;pid = p-\u0026gt;pid; return p; } static void freeproc(struct proc *p) { ... // -- Modified -- // free usyscall page if (p-\u0026gt;usyscall_page) { kfree((void*)p-\u0026gt;usyscall_page); } p-\u0026gt;usyscall_page = 0; ... } 因为用户态寻址的时候都要经过页表硬件的翻译，所以 usyscall 也要映射在进程的 pagetable 上。我们需要修改映射函数和释放映射的函数。\npagetable_t proc_pagetable(struct proc *p) { ... if (mappages(pagetable, USYSCALL, PGSIZE, (uint64)(p-\u0026gt;usyscall_page), PTE_R | PTE_U) \u0026lt; 0) { uvmunmap(pagetable, TRAMPOLINE, 1, 0); uvmunmap(pagetable, TRAPFRAME, 1, 0); uvmfree(pagetable, 0); return 0; } ... } void proc_freepagetable(pagetable_t pagetable, uint64 sz) { uvmunmap(pagetable, TRAMPOLINE, 1, 0); uvmunmap(pagetable, TRAPFRAME, 1, 0); // -- Modified -- // free mapping of usyscall page uvmunmap(pagetable, USYSCALL, 1, 0); uvmfree(pagetable, sz); } 总结：\n先使用 kalloc(kernel malloc) 在系统的页表中申请到一块空间，初始化。\n然后把这块空间的地址翻译到用户空间对应的页表中去，页表中对应着 USYSCALL，在 kernel\\memlayout.h 中是这样定义的\n#define USYSCALL (TRAPFRAME - PGSIZE) 可以看到这是紧挨着 TRAPFRAME 的。\n当应用程序调用了 ugetpid() 指令的时候，只需要去 USYSCALL 页表找到对应的内容就可以了。而 USYCALL 页表的内容实际上是 proc.c 中调用 kalloc() 后获得的实际物理地址的引用。\nPrint a page table 按照一定格式打印出页表的信息，这个非常的简单，只要递归的调用就行了。\n// Lab 3. Not visiable to user. void k_vmprint_recur(pagetable_t pgt, int blank) { for(int i = 0; i \u0026lt; 512; i++) { pte_t pte = pgt[i]; if (pte \u0026amp; PTE_V) { uint64 child = PTE2PA(pte); for (int j = 0; j \u0026lt; blank; j++) { printf(\u0026#34; ..\u0026#34;); } printf(\u0026#34;%d: pte %p pa %p\\n\u0026#34;, i, pte, child); if ((pte \u0026amp; (PTE_R|PTE_W|PTE_X)) == 0) { k_vmprint_recur((pagetable_t)child, blank+1); } } } } // Lab 3 void vmprint(pagetable_t pgt) { // recursively print the three level page. printf(\u0026#34;page table %p\\n\u0026#34;, pgt); k_vmprint_recur(pgt, 1); } Detecting which pages have been accessed 一些 GC(garbage cllector) 可以从有关哪些页面已被访问（读取或写入）的信息中获益。 在实验的这一部分，我们将向 xv6 添加一项新功能，通过检查 RISC-V 页表中的访问位来检测此信息并将其报告给用户空间。每当解决 TLB 未命中时，RISC-V 硬件页面遍历器都会在 PTE 中标记这些位。\n首先要在 kernel/sysproc.c 中加入系统调用的实现\n#ifdef LAB_PGTBL int sys_pgaccess(void) { // lab pgtbl: your code here. uint64 buffer, ans; int number; if (argaddr(0, \u0026amp;buffer), buffer \u0026lt; 0) return -1; if (argint(1, \u0026amp;number), number \u0026lt; 0) return -1; if (argaddr(2, \u0026amp;ans), ans \u0026lt; 0) return -1; return pgaccess((void*)buffer, number, (void*)ans); } #endif 然后着手实现 pgacess 的实现，这个实现需要添加头文件定义，并且在 kernel/proc.c 中添加实现:\nuint64 pgaccess(void *pg, int number, void *store) { struct proc *p = myproc(); if (p == 0) return 1; int ans = 0; pagetable_t pagetable = p-\u0026gt;pagetable; for (int i = 0; i \u0026lt; number \u0026amp;\u0026amp; i \u0026lt; 32; i++) { pte_t *pte; pte = walk(pagetable, (uint64)pg + (uint64)PGSIZE * i, 0); if (pte != 0 \u0026amp;\u0026amp; ((*pte) \u0026amp; PTE_A)) { ans |= 1 \u0026lt;\u0026lt; i; *pte ^= PTE_A; //clear PTE_A } } // copy the value to user page. return copyout(pagetable, (uint64)store, (char *)\u0026amp;ans, sizeof(int)); } 这个实现是这样的，通过遍历指定的页表之后的 n 个 page，访问每个 page 的 PTE_A bit，如果这个 bit 是 1，就使用 bit map 来记录他。最终别忘了把这个 int 类型的 ans 返回给用户态。\n对于 PTE_A，需要在 kernel\\risc.h 中进行添加，查阅 XV6 手册得知，改 PTE_A 位于第 6 位，故为 1 \u0026lt;\u0026lt; 6。\n","permalink":"https://chenghuawang.github.io/keep-moving-forward/tech/xv6_lab3_pagetable/","summary":"MIT 6.S081 Lab3 website\n做 Lab 3 需要提前阅读 XV6 book，了解 RISC-V SR39 的地址格式，并且实验中大量用到了页表的准换函数，需要查阅 XV6 手册。不过，熟记 RISC-V 的地址说实在的没有什么用处，通过这个实验理解页表的工作方式并且 hands on 才是真的。\nSpeed up system calls 目前有很多的操作系统(Linux)在用户区和内核区之间共享一块数据(Read-Only for user)，这样用户在进行系统调用的时候就不需要陷入内核态后，由内核态拷贝数据进用户态，而是将数据写在这个共享的区块内。这样可以加快操作系统的运行速度(毕竟大部分系统调用需要的内存消耗是很小的，内存的消耗在当今已经不是问题)。\n在本实验中我们需要使用 ugetpid() 来进行加速获得进程的 pid。\n首先就是为每一个进程创建一个页表作为共享内存区块。我们发现在 kernel\\memlayout.h 中已经为我们定义好了需要的数据结构:\nstruct usyscall { int pid; // Process ID }; 那么我们只需要在 kernel/proc.c /proc.h 中加入代码，来实现进程创建时创建页表，销毁时销毁页表的动作就行了。\n在 proc.h 中，我们需要在进程的 PCB 中加入新的数据结构:\nstruct proc { ... struct usyscall *usyscall; // using read only shared data to accelerate. ... } 为了让进程的正常创建和释放，我们需要向进程创建和销毁函数中加入对应的页表操作代码。","title":"XV6 Lab 3: Page Table"},{"content":"MIT 6.S081 Lab2 website\n为了完成 Syscall 作业，需要阅读:\nXV6-book, Chapter 2, Sections 4.3 and 4.4\nfiles: user/user.h, kernel/proc.c kernel/proc.h, kernel/syscall.c kernel/syscall.h\nsystem call tracing system call tracing 需要我们补充 kernel 中的一些程序，将某程序中指定的 system call 打印出来。当然，这需要我们新增一个 system_trace 系统调用函数。题中，给定的 tracing 程序以 trace [system-call-number] [cmd] 的方式运行。我们首先去看 user/trace.c 中的内容，看看 system call number 是怎么传入系统调用的。\nuser/trace.c 的程序如下所示:\nint main(int argc, char *argv[]) { int i; char *nargv[MAXARG]; if(argc \u0026lt; 3 || (argv[1][0] \u0026lt; \u0026#39;0\u0026#39; || argv[1][0] \u0026gt; \u0026#39;9\u0026#39;)){ fprintf(2, \u0026#34;Usage: %s mask command\\n\u0026#34;, argv[0]); exit(1); } if (trace(atoi(argv[1])) \u0026lt; 0) { fprintf(2, \u0026#34;%s: trace failed\\n\u0026#34;, argv[0]); exit(1); } for(i = 2; i \u0026lt; argc \u0026amp;\u0026amp; i \u0026lt; MAXARG; i++){ nargv[i-2] = argv[i]; } exec(nargv[0], nargv); exit(0); } 我们发现这个程序直接使用了 trace 系统调用来实现。所以接下来的任务是进行 trace 系统调用的实现。再次回顾 Lab 1 中的内容，在 Lab 1 中我们分析系统调用是通过在 usys.S 的汇编程序中进入的，然后 ecall 到 kernel/syscall.c 中映射到的函数，最终执行。那么 trace 系统调用也是这个逻辑。\n在汇编中，是这样实现的:\nli a7, SYS_trace\recall\rret 这个 lab 使用的是 risc-v 汇编。li a7, SYS_trace 表示把 32 位的数据 SYS_trace 加载到指定的寄存器 a7 中。\necall 将异常的类型写在 a7 寄存器中，参数写在 a0-a5 寄存器中。syscall 场景下，使用 ecall 会把处理器的特权级别由 User-Mode 转到 Supervisor-Mode。那 ecall 跳转的地址在哪里呢？在操作系统启动的时候，会把异常表地址绑定到 stevc 寄存器中。\n为了能够让每个进程知道需要打印出什么样的 syscall 调用，我们需要将 trace 调用传入的值传给进程？那如何将一个值传给进程，进程又该如何保存这个值呢？\n显然，我们需要修改进程的定义，在 kernel/proc.h 中修改进程的数据结构，加上一个 mask 项来存储需要跟踪的系统调用的数值。\nstruct proc { struct spinlock lock; ... char name[16]; // Process name (debugging) int mask; // -- Modified -- Lab2, system call. }; 接下来，我们需要考虑在哪个地方插入打印指令来打印出 trace 程序追踪的系统调用？是在 process 中加入吗？显然不能在进程中做到这一点。我们只能在 syscall 这个通用的系统调用函数里面加入处理代码。在文件 kernel/syscall.c 的函数 syscall 中加入打印代码:\nvoid syscall(void) { ... if(num \u0026gt; 0 \u0026amp;\u0026amp; num \u0026lt; NELEM(syscalls) \u0026amp;\u0026amp; syscalls[num]) { ... // -- modified -- if ((1 \u0026lt;\u0026lt; num) \u0026amp; p-\u0026gt;mask) { printf(\u0026#34;%d: syscall %s -\u0026gt; %d\\n\u0026#34;, p-\u0026gt;pid, sysname[num], p-\u0026gt;trapframe-\u0026gt;a0); } } else { ... } } 在上述代码中，syscall 指令会将调用号(a7 寄存器)读取到 num 中，然后查找系统调用表并执行。我在这里定义了一个新的数组 sysname 来存放每个系统调用的名称。\n对于系统调用，a0 寄存器中保存的是返回值\n为了便利的增加新的系统调用，xv6 lab 实现了用 perl 脚本来生成 asm 的一段小程序。我们需要在 user/usys.pl 中加入:\nentry(\u0026#34;trace\u0026#34;); 在 make qemu 的过程中，它会自动生成对应的陷入系统调用的汇编如下:\n.global trace\rtrace:\rli a7, SYS_trace\recall\rret 接下来，我们还需要给 sys_trace 函数在内核中新加一段声明，修改 kernel/syscall.c 文件，加入\nextern uint64 sys_trace(); 然后在 kernel/sysproc.c 中实现系统调用:\nuint64 sys_trace() { int mask; argint(0, \u0026amp;mask); if(mask \u0026lt; 0) return -1; myproc()-\u0026gt;mask = mask; return 0; } 这里的 argint 函数会去调用 argraw 函数如下:\nstatic uint64 argraw(int n) { struct proc *p = myproc(); switch (n) { case 0: return p-\u0026gt;trapframe-\u0026gt;a0; case 1: return p-\u0026gt;trapframe-\u0026gt;a1; case 2: return p-\u0026gt;trapframe-\u0026gt;a2; case 3: return p-\u0026gt;trapframe-\u0026gt;a3; case 4: return p-\u0026gt;trapframe-\u0026gt;a4; case 5: return p-\u0026gt;trapframe-\u0026gt;a5; } panic(\u0026#34;argraw\u0026#34;); return -1; } 因为我们需要获得 mask 值，而 mask 值在 trace() 函数中是第一个，所以我们直接方位 a0 寄存器来得到 mask 的值。\n总结:\n在 xv6 中实现系统调用有如下的几步:\nuser-mode 调用 trace()， trace() 的实现在 usys.S 的汇编中 usys.S 将 user-mode 中调用的系统调用代号填充到 a7 寄存器中 ecall 调用了 kernel/syscall.c 中的 syscall() 函数来执行系统调用，并切换到 supervisor-mode syscall() 函数本身没有定义输入，其从 a7 寄存器中找到系统调用函数的代号，然后再在函数表中查找。 Notes:\n为了过 fork() 测试样例，需要修改 fork() 函数，加入 np-\u0026gt;mask = p-\u0026gt;mask; 来继承 mask。\nsysinfo 有了上面的例子，实现 sysinfo() 就非常的简单了。sysinfo 要求我们统计空闲进程的数量和空闲内存的数量。\n对于进程，xv6 使用了一个数组来存储了所有的进程，每个进程的状态都保存在一个结构体(PCB)中。如果要查询每个进程的状态，我们只需要遍历所有的进程，然后 check 是否是 UNUSED 状态就行了。\n每个进程的状态只能是下面的几种之一:\nenum procstate { UNUSED, SLEEPING, RUNNABLE, RUNNING, ZOMBIE }; 我们需要在 kernel/proc.c 中加入代码:\n// -- modified -- // To get the number of free processes. uint64 free_process_num() { uint64 res = 0; struct proc* tmp; for (tmp = proc; tmp \u0026lt; \u0026amp;proc[NPROC]; tmp++) { acquire(\u0026amp;tmp-\u0026gt;lock); if (tmp-\u0026gt;state != UNUSED) res++; release(\u0026amp;tmp-\u0026gt;lock); } return res; } 需要注意的是: 在统计状态的时候，必须对当前的 PCB 加锁，防止出现冲突(free_process_num)运行过程中变成了 UNUSED 或者其他\n而可用空间判断则是在 kernel/kalloc.c 文件中定义了一个链表，每个链表都指向上一个可用空间，这个kmem就是一个保存最后链表的变量。\nstruct run { struct run *next; }; struct { struct spinlock lock; struct run *freelist; } kmem; kmem.freelist 永远指向最后一个可用页，那我们只要顺着这个链表往前走，直到 NULL 为止。\n在 kernel/kalloc.c 中加入代码:\nuint64 free_mem_num() { uint64 res = 0; struct run *page = kmem.freelist; while(page != 0) { res ++; page = page-\u0026gt;next; } return res * PGSIZE; } 对于统计空闲内存和进程的代码，kernel/def.h 中声明。 之后，我们在 kernel/sysproc.c 中实现 sysinfo() 函数如下:\nuint64 sys_sysinfo() { uint64 address; struct sysinfo info; struct proc *p = myproc(); argaddr(0, \u0026amp;address); if (address \u0026lt; 0) return -1; info.freemem = free_mem_num(); info.nproc = free_process_num(); if (copyout(p-\u0026gt;pagetable, address, (char*)\u0026amp;info, sizeof(info)) \u0026lt; 0) return -1; return 0; } 这里 copyout 是吧页表中的内容 copy 到 user-mode 的进程信息中去。我们通过 argaddr(0, \u0026amp;address); 拿到 info 的地址，把内核态的内存 copy 到用户态去。\n","permalink":"https://chenghuawang.github.io/keep-moving-forward/tech/xv6_lab2_syscall/","summary":"MIT 6.S081 Lab2 website\n为了完成 Syscall 作业，需要阅读:\nXV6-book, Chapter 2, Sections 4.3 and 4.4\nfiles: user/user.h, kernel/proc.c kernel/proc.h, kernel/syscall.c kernel/syscall.h\nsystem call tracing system call tracing 需要我们补充 kernel 中的一些程序，将某程序中指定的 system call 打印出来。当然，这需要我们新增一个 system_trace 系统调用函数。题中，给定的 tracing 程序以 trace [system-call-number] [cmd] 的方式运行。我们首先去看 user/trace.c 中的内容，看看 system call number 是怎么传入系统调用的。\nuser/trace.c 的程序如下所示:\nint main(int argc, char *argv[]) { int i; char *nargv[MAXARG]; if(argc \u0026lt; 3 || (argv[1][0] \u0026lt; \u0026#39;0\u0026#39; || argv[1][0] \u0026gt; \u0026#39;9\u0026#39;)){ fprintf(2, \u0026#34;Usage: %s mask command\\n\u0026#34;, argv[0]); exit(1); } if (trace(atoi(argv[1])) \u0026lt; 0) { fprintf(2, \u0026#34;%s: trace failed\\n\u0026#34;, argv[0]); exit(1); } for(i = 2; i \u0026lt; argc \u0026amp;\u0026amp; i \u0026lt; MAXARG; i++){ nargv[i-2] = argv[i]; } exec(nargv[0], nargv); exit(0); } 我们发现这个程序直接使用了 trace 系统调用来实现。所以接下来的任务是进行 trace 系统调用的实现。再次回顾 Lab 1 中的内容，在 Lab 1 中我们分析系统调用是通过在 usys.","title":"XV6 Lab 2: syscall"},{"content":"MIT 6.S081 Lab1 website\n本章节的实验是为了熟悉 XV6 环境和 interface 而准备的。包含了 interface 调用、多进程编程、Pipeline。\n阅读 book-riscv-ref3 熟悉 XV6 的系统组织形式\nsleep 通过调用 user.h 中的系统调用函数来完成 sleep 程序。主要是为了介绍 系统调用 的工作方式。\n/** * @author chenghua.wang * @brief Lab1-utilities. sleep prog using syscall. * @time Feb 23, 2023 * */ #include \u0026#34;kernel/types.h\u0026#34; #include \u0026#34;kernel/stat.h\u0026#34; #include \u0026#34;user/user.h\u0026#34; int main(int argc, char *argv[]){ if (argc != 2){ printf(\u0026#34;[ error ] you should follow anw integer with sleep prog to indicate ticks times!\\n\u0026#34;); exit(1); // failure } sleep(atoi(argv[1])); // sys call provided by user.h exit(0); // success } 下面以 sleep(int) 系统调用来阐述 XV6 中调用的流程。sleep 函数在 user/user.h 中声明，具体的函数定义则通过其他模块的动态连接最终合成可执行文件 _sleep.\n在 user/usys.S 中 sleep 调用的汇编代码是这样的:\n.global sleep\rsleep:\rli a7, SYS_sleep\recall\rret 这里的 sleep 就是上面那个 sleep 函数的定义，就是说执行 sleep 时，就是执行这段汇编代码。可以看到在用户空间 sleep 函数的任务就是将 SYS_sleep 放入 a7 中，然后中断。SYS_sleep 在 kernel/syscall.h 中宏定义为一个系统调用号。可以看到这个 usys.S 文件也包含了这个头文件。\n// kernel/syscall.h #define SYS_sleep 13 接下来在 kernel/syscall.c 中，可以看到 SYS_sleep 转换为了对应的 sys_sleep:\n// kernel/syscall.c // An array mapping syscall numbers from syscall.h // to the function that handles the system call. static uint64 (*syscalls[])(void) = { [SYS_fork] sys_fork, [SYS_exit] sys_exit, [SYS_wait] sys_wait, [SYS_pipe] sys_pipe, [SYS_read] sys_read, [SYS_kill] sys_kill, [SYS_exec] sys_exec, [SYS_fstat] sys_fstat, [SYS_chdir] sys_chdir, [SYS_dup] sys_dup, [SYS_getpid] sys_getpid, [SYS_sbrk] sys_sbrk, [SYS_sleep] sys_sleep, [SYS_uptime] sys_uptime, [SYS_open] sys_open, [SYS_write] sys_write, [SYS_mknod] sys_mknod, [SYS_unlink] sys_unlink, [SYS_link] sys_link, [SYS_mkdir] sys_mkdir, [SYS_close] sys_close, }; pingpong 使用 pipe 进行父进程和子进程之间的调用。注意管道 0, 1 的关闭。\n/** * @author chenghua.wang * @brief Lab-1-utilities pingpong. Using the pipe syscall. * */ #include \u0026#34;kernel/types.h\u0026#34; #include \u0026#34;kernel/stat.h\u0026#34; #include \u0026#34;user/user.h\u0026#34; char* buf[8]; int main() { int p[2]; pipe(p); // using syscall to create pipe. // 1. child if (fork() == 0){ int id = getpid(); read(p[0], buf, 1); close(p[0]); printf(\u0026#34;%d: received ping\\n\u0026#34;, id); write(p[1], \u0026#34;c\u0026#34;, 1); close(p[1]); } else { // 2. parent int id = getpid(); write(p[1], \u0026#34;p\u0026#34;, 1); close(p[1]); wait(0); read(p[0], buf, 1); close(p[0]); printf(\u0026#34;%d: received pong\\n\u0026#34;, id); } exit(0); } prime 通过管道作为不同进程之间通信的渠道来进行多进程求解素数，实现素数筛。这里因为 XV6 的限制只能开 34 个进程。\n算法的具体流程非常的简单:\n有管道 p_{0} 把数据(2-\u0026gt;35)全都填入\np_{0} 右端连接一个进程，这个进程筛去 p_{0} 中 %2==0的，将剩下的数传递给 p_{1} 管道。\n以此类推。\n只需要注意两次管道的关闭就好了。\n/** * @author chenghua.wang * @brief using concurrency to get primes. * @time Feb 23, 2023 * */ #include \u0026#34;kernel/types.h\u0026#34; #include \u0026#34;kernel/stat.h\u0026#34; #include \u0026#34;user/user.h\u0026#34; int child(int previous_pipe[2]) { int child_pipe[2]; int prime; close(previous_pipe[1]); // child do not need write data to previous pipe. int len = read(previous_pipe[0], \u0026amp;prime, 1); if (!len) { close(previous_pipe[0]); // no data need to read anymore. exit(0); // success. } pipe(child_pipe); printf(\u0026#34;prime %d\\n\u0026#34;, prime); int num; if (fork() == 0) { close(previous_pipe[0]); child(child_pipe); } else { close(child_pipe[0]); while(1) { int len = read(previous_pipe[0], \u0026amp;num, 1); if (len == 0) break; if (num % prime != 0) { write(child_pipe[1], \u0026amp;num, 1); } } close(child_pipe[1]); close(previous_pipe[0]); wait(0); } exit(0); // success } int main() { int parent_pipe[2]; pipe(parent_pipe); if (fork() == 0) { child(parent_pipe); } else { close(parent_pipe[0]); // parent do not need to read data. for (int i = 2; i \u0026lt; 36; ++i) { write(parent_pipe[1], \u0026amp;i, sizeof(int)); } close(parent_pipe[1]); // parent do not need to write anymore. wait(0); // wait for child process done. } exit(0); } find 基本上就是 XV6 中 ls 程序的改版。只不过判断是文件夹的话要递归。\n/** * @author chenghua.wang * @brief Lab1-utilities find * @time Feb 23, 2023 * */ #include \u0026#34;kernel/types.h\u0026#34; #include \u0026#34;kernel/stat.h\u0026#34; #include \u0026#34;user/user.h\u0026#34; #include \u0026#34;kernel/fs.h\u0026#34; // for file system. DIRSIZ, etc. char* fmtname(char *path) { char *p; for (p = path + strlen(path); p \u0026gt;= path \u0026amp;\u0026amp; *p != \u0026#39;/\u0026#39;; --p); p++; return p; } void find(char *directory, char *file_name) { char buf[512], *p; int fd; struct dirent de; struct stat st; // open file. if ((fd = open(directory, 0)) \u0026lt; 0) { fprintf(2, \u0026#34;find: cannot open %s\\n\u0026#34;, directory); return; } // get the state of this file if (fstat(fd, \u0026amp;st) \u0026lt; 0) { fprintf(2, \u0026#34;find: cannot get stat of %s\\n\u0026#34;, directory); close(fd); return; } // find the correct type need to check. // 1. path-\u0026gt; find if file_name is same. // 2. directory-\u0026gt; run find func again. // 3. others(\u0026#39;.\u0026#39;, \u0026#39;..\u0026#39;)-\u0026gt; skip. switch(st.type) { case T_DEVICE: case T_FILE: if (strcmp(file_name, fmtname(directory)) == 0) { printf(\u0026#34;%s\\n\u0026#34;, directory); } break; case T_DIR: strcpy(buf, directory); p = buf + strlen(buf); *p++ = \u0026#39;/\u0026#39;; while(read(fd, \u0026amp;de, sizeof(de)) == sizeof(de)) { if (de.inum == 0 || strcmp(de.name, \u0026#34;..\u0026#34;) == 0 || strcmp(de.name, \u0026#34;.\u0026#34;) == 0) continue; memmove(p, de.name, DIRSIZ); p[DIRSIZ] = 0; find(buf, file_name); } break; } close(fd); } int main(int argc, char *argv[]) { if (argc != 3) { printf(\u0026#34;[ error ] find [directory] [\u0026#39;name\u0026#39;]\\n\u0026#34;); exit(1); // failure } find(argv[1], argv[2]); exit(0); // success } xargs 我想 XV6 这个实验主要是想考察 exec 的作用。xargs 从标准输入得到数据，然后对每一行数据重新填入 argv 再执行。\n/** * @author chenghua.wang * @brief Lab1-utilities xargs. * @time Feb 23, 2023 * */ #include \u0026#34;kernel/types.h\u0026#34; #include \u0026#34;kernel/stat.h\u0026#34; #include \u0026#34;user/user.h\u0026#34; char buf[512]; int main(int argc, char *argv[]) { char *pass_argv[64]; for (int i = 0; i \u0026lt; argc; ++i) { pass_argv[i] = argv[i + 1]; // the terminate 0 is also moved. } for (;;) { // to read a line int i = 0; for(;;++i) { int len = read(0, \u0026amp;buf[i], 1); // read from stdin. if (len == 0 || buf[i] == \u0026#39;\\n\u0026#39;) break; } if (i == 0) break; // exec a line buf[i] = 0; pass_argv[argc - 1] = buf; if (fork() == 0) { exec(pass_argv[0], pass_argv); exit(0); // success } else { wait(0); } } exit(0); // success } ","permalink":"https://chenghuawang.github.io/keep-moving-forward/tech/xv6_lab1_utility/","summary":"MIT 6.S081 Lab1 website\n本章节的实验是为了熟悉 XV6 环境和 interface 而准备的。包含了 interface 调用、多进程编程、Pipeline。\n阅读 book-riscv-ref3 熟悉 XV6 的系统组织形式\nsleep 通过调用 user.h 中的系统调用函数来完成 sleep 程序。主要是为了介绍 系统调用 的工作方式。\n/** * @author chenghua.wang * @brief Lab1-utilities. sleep prog using syscall. * @time Feb 23, 2023 * */ #include \u0026#34;kernel/types.h\u0026#34; #include \u0026#34;kernel/stat.h\u0026#34; #include \u0026#34;user/user.h\u0026#34; int main(int argc, char *argv[]){ if (argc != 2){ printf(\u0026#34;[ error ] you should follow anw integer with sleep prog to indicate ticks times!","title":"XV6 Lab 1: Xv6 and Unix utilities"},{"content":"go back to home\nPaper link\nACM SIGOPS Operating Systems Review, 2010, 44(4): 30-39.\nLast Edit: Jan 19, 2023\nIntroduction 是的，这节内容还是 FT(fault-tolerance)。现在做 FT 主要有两种手段[2]：\nState transfer Primary用来执行所有的任务，Primary发送该机器的所有状态(所有的内存变动，所有的磁盘变动，等)给Replica。State Transfer 虽然听起来非常的简单(实际上做起来也是的，相对于Replicated state machine)，但是需要占用非常大量的网络带宽来实现。\n尽管如此(占用大量的网络资源，导致传输缓慢)，State Transfer是对多处理器友好的一种方式，而Replicated state machine则不是。\nReplicated state machine Client 向 Primary 发送操作和数据(inputs)。Primary把这些操作和数据发送给Replicas，Replicas和Primary都会执行这些指令，都会受到这些数据(inputs)，所有的指令都以同样的顺序执行，只要Primary和Replicas初始的状态是一致的，那么二者就一直保持着同步(也意味着deterministic)。\n在并行化的应用中，State transfer 是首选，毕竟在并行的时候，并行顺序对于Replicated state machine来说是异常难同步的。\n主从复制时的挑战[2]：\n要复制什么状态 primary需要等待backup吗 什么时候需要切换到backup 切换的时候异常情况是否能看到 如何提高创建新backup的速度 VM-FT使用的就是Replicated state machine的方法。为了能够捕获数据，并且做出一些软件中断，Primary和Backup都是在VM上运行的，由hypervisor来管理他们。\nArch VM-FT 总体的结构较为简单，论文中是一主一从的结构。VM-FT主要依赖于 Deterministic replay(确定性重放)。VM-FT 通过确定性重放来产生相关的日志条目，但不将日志写入磁盘，而是通过 logging channel 发送给backup(这里指备用机)。backup实时重放日志项。\n因为一切都是在 logging channel 上做同步的。为了容错，必须在 logging channel 上实现严格的容错协议，有以下要求：\nPrimary直到backup接收并确认了和输出相关的日志的时候，才发送输出给外界。(这样做的目的是，只要backup收到了所有的日志条目，即使primary宕机了，backup仍能够重放到客户端最后看到的状态。) Primary和backup的数据都是10。现在客户端发送increase请求，Primary+1并回复给client 11，之后马上宕机了，更糟糕的是Primary发给backup的+1操作也丢包了。这时候backup还是10，并接管了primary的工作，client再次请求+1，又会收到11的回复。这就产生了矛盾。\n如果备机在主机故障后接管，备机将以和主机已经向外界发送的输出完全一致的方式继续运行。 为了确保一次只有一个虚拟机成为主机，避免出现brain split，VMware 在共享存储上执行test and set指令(\u0026ldquo;OSTEP\u0026quot;中提到过)。\n为了保证一定的性能，VM-FT决定在Primary没收到Ack以前，可以继续往下执行，但是这样会拉大和backup之间的距离。所以文中用一定的方法来限制这个gap：如果backup在需要读取下一个日志条目时遇到空日志条目，则停止执行，直到有新的日志条目可用。因为backup没有与外部通信，因此此暂停不会影响clients。相同的，Primary如果在需要写入日志条目时遇到一个满的buffer(虚拟机管理程序维护了一个大的日志缓冲(log buffer)，保存主机和备机的日志。主机会产生日志项到日志缓冲，备机从日志缓冲消费日志。)，则停止执行，直到日志条目被清除为止，显然我们不希望Primary与backup之间状态差距太大，一般来说Primary速度更快。\n缺陷 仅支持单处理器，多核对于Replicated state machine是极其不确定的。\nReference [1] Scales D J, Nelson M, Venkitachalam G. The design of a practical system for fault-tolerant virtual machines[J]. ACM SIGOPS Operating Systems Review, 2010, 44(4): 30-39.\n[2] MIT6.824 Notes. l-vm-tf.txt\n","permalink":"https://chenghuawang.github.io/keep-moving-forward/papers/ft-vm/","summary":"notes of paper, ACM SIGOPS Operating Systems Review, 2010, 44(4): 30-39.","title":"The Design of a Practical System for Fault-Tolerant Virtual Machines"},{"content":"go back to home\nPaper link\nProceedings of the 19th ACM Symposium on Operating Systems Principles, ACM, Bolton Landing, NY (2003), pp. 20-43\nLast Edit: Jan 18, 2023\nGFS 有非常多的东西，这里只写了一些重要的部分。像是snapshot，文件删除，高可用机制，Replica管理等没有具体提及。\nIntroduction GFS是google提出的一个可扩展分布式文件系统，为大型分布式数据密集型应用提供服务。可以在大规模的消费级机器集群上提供不错的容错能力。GFS在设计的时候主要依据6个假设(观察得出的):\n节点失效经常发生。系统由非常多的消费级机器组成，大量用户同时进行访问，这使得节点很容易因为程序bug、磁盘故障、内存故障等原因失效。 存储以大文件为主。每个文件通常100MB或几GB。系统需要支持小文件，但不需要对其进行特殊的优化。 大容量连续读，小容量随机读取是文件系统中的常态。 写入也已大容量为主，小容量无需特殊优化。 支持原子的文件追加操作。使得大量用户可以并行追加文件，而不需要额外的加锁机制。 高吞吐量比低延时更重要 为什么设计一个优秀的分布式存储系统非常的困难:\nPerformance. 当数据量非常大的时候，数据分片(Sharding)是非常重要的。 Fault Tolerance. 但是由于数据在多台服务器上分片。由于多台服务器，整个系统出现故障的概率会大很多。因此需要容错机制 Replication. 复制数据副本到多台服务器上，但是为了用户能够拿到一致的数据，需要考虑一致性 Consistency. 为了一致性，不得不使用网络(极慢的数据交互方式)进行确认和同步。这又会影响性能(Performance)!!! 世界因此变成了一个美妙的环。:-)。这也是分布式的挑战之处。\nGFS讨论了上述的这些主题和在实际生产场景中的应用。\nArchitecture Fig 1. GFS ArchGoole File System. Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung. Proceedings of the 19th ACM Symposium on Operating Systems Principles, ACM, Bolton Landing, NY (2003), pp. 20-43\n一个GFS集群由一个Master节点和若干个Chunk Server节点组成。每个Chunk Server可以被许多个Client访问。GFS Chunk Server作为用户级进程在Linux服务器中运行，并且文件系统本身使用的就是Linux系统的那套。(所以文中说，没有特地的为GFS Chunk Server加入cache功能，因为Linux文件系统已经干了这件事)\nChunk: GFS中的文件在存储的时候会被分割成多个Chunk，每个Chunk的大小为64MB。在Chunk分配的时候，Master会分配一个Handle给Chunk，类似于指针。Chunk在google的实现中，使用3份副本。\nMaster: 维护元数据，记录文件被分割为哪些Chunk、以及这些Chunk的存储位置；它还负责Chunk的迁移、重新平衡(rebalancing)和垃圾回收；此外，Master通过心跳机制与ChunkServer通信，向其传递指令(是的，也是通过心跳机制)，并收集状态；\nClient: 首先向Master询问该文件的Chunk在哪里，Chunk Server位置，再从 Chunk Server获取数据。Client并不会每次都向Master发送数据进行询问，它会cache一部分的数据(不是Chunk的，是某个文件对应的Chunk Server的位置，即Chunk的地址)，并且保持一定的时间。\nChunkServer: 存储Chunk，Client和Chunk Server不会缓存Chunk数据，防止数据出现不一致\nChunk and Chunk Size Chunk大小的选择是非常重要的。通常情况下，Chunk大小为64MB，这比典型的文件系统的block大得多，可以通过惰性空间分配策略，来避免因内部碎片造成的空间浪费。\n不过，较大的Chunk会使得小文件占据额外的存储空间。此外，小文件通常只会占据一个Chunk，当大量客户端访问这个小文件时，这个Chunk容易成为系统的负载热点。\nChunk的大小设置主要考虑这些因素：\n减少Master保存的元数据大小(每个chunck都对应着一份元数据，分割的Chunk太多会导致元数据非常的大。类似的，可以比作内存分页中的页表)，使得可以把元数据全部放在内存中。 减少Client与Master的通信次数，因为对同一个Chunk的多次读写只需要请求一次Chunk信息(Client会暂时的存储这些元信息，类比于内存的Spacial locality特性)。 增大Client操作落到同一个Chunk上的概率。通过保持持久的TCP连接来减少网络上的负载。(类似的，可以类比于内存的Temporal locality特性) Master 为了简化设计，只有一台机器会作为Master存在。Master在内存中存储3种metadat。标记 nv(non-volatile, 非易失) 的数据需要在写入的同时存到磁盘(为了效率，也有批写入的)，标记v的数据，Master会在启动后查询Chunk Server 集群。\nnamespace(目录层次结构)和文件名(nv)\n文件名 -\u0026gt; array of Chunk Handles 的映射(nv)\nChunk Handles -\u0026gt; 版本号(nv)、list of Chunk Servers(v)、primary(v)、lease(v)\nMaster 使用Log和CheckPoints来进行记录而不是使用数据库的形式。Log形式可以在尾部快速添加，相比于数据库的复杂数据结构，Log的速度会更快。并且Log是一种顺序的结构，可以很好的体现出时间线。\n元数据管理[2] 元数据保存在Master内存中使得Master要对元数据作出变更变得极为容易；同时，这也使得Master可以简单高效地周期性扫描整个集群的状态，以实现Chunk回收、迁移、均衡等操作。一个64MB的chunck需要使用掉64KB的空间来存储元数据。\nMaster会把前两类信息(namespace+文件名，文件名到chunck Handles的映射)以日志形式持久化存储在Master的本地磁盘上，并在在其他机器上备份，但是不会持久化保存Chunk Replica的位置信息，而是在集群启动时由Master询问各个Chunk Server其当前所有的Repica。这样做可以省去由于Chunk Server离开集群、更改名字、重启等原因的Master与Chunk Server的同步问题。此后，Master通过心跳包来监控Chunk Server的状态并更新内存中的信息。\n为了保证元数据的可用性，Master在对元数据做任何操作前对会用先写日志的形式将操作进行记录，只有当日志写入完成后才会响应客户端的请求，而这些日志也会备份到多个机器上。日志不仅是元数据的唯一持久化记录，也是定义操作执行顺序的时间线。文件、Chunk和他们的版本信息都由他们的创建时间唯一的永久的标识。\nNamespace管理[2] 在逻辑上，Master并不会根据文件与目录的关系以分层的结构来管理这部分数据，而是单纯地将其表示为从完整路径名到对应文件元数据的映射表，并在路径名上应用前缀压缩以减少内存占用。\n为了管理来自不同客户端的并发请求对Namespace的修改，Master会为Namespace中的每个文件和目录都分配一个读写锁（Read-Write Lock）。由此，对不同Namespace区域的并发请求便可以同时进行。\n所有Master操作在执行前都会需要先获取一系列的锁：通常，当操作涉及某个路径 /d1/d2/\u0026hellip;/dn/leaf时，Master会需要先获取从/d1、/d1/d2到/d1/d2/\u0026hellip;/dn的读锁，然后再根据操作的类型获取 /d1/d2/\u0026hellip;/dn/lead的读锁或写锁。获取父目录的读锁是为了避免父目录在此次操作执行的过程中被重命名或删除。\n由于大量的读写锁可能会造成较高的内存占用，这些锁会在实际需要时才进行创建，并在不再需要时被销毁。此外，所有的锁获取操作也会按照一个相同的顺序进行，以避免发生死锁：锁首先按Namespace树的层级排列，同一层级内则以路径名字典序排列(\u0026ldquo;OSTEP\u0026quot;中chapter32讲述的，使用一定的锁的顺序来避免死锁)。\nLease管理[2] GFS使用租约（lease）机制来保持多个副本间变更顺序的一致性。在客户端对某个Chunk做出变更时，会把该Chunk的Lease交给某个Replica，使其成为Primary：Primary 会负责为这些变更安排一个执行顺序，然后其他Replica便按照相同的顺序执行这些修改。\n设计租约机制的目的是为了最小化Master节点的管理负担。Chunk Lease在初始时会有60秒的超时时间。在未超时前，Primary可以向Master申请延长Chunk Lease的时间；必要时Master也可以直接撤回已分配的Chunk Lease。\nRead and Write 在写文件的时候会涉及到Lease和Version的问题。\nRead Client将文件名 + Offset转为文件名 + Chunk Index，向Master发起请求\nMaster在元数据中查询对应Chunk所在的Chunk Handle + Chunk Locations并返回给Client\nClient将Master返回给它的信息缓存起来，用文件名 + Chunk Index作为 key\nClient会选择网络上最近的Chunk Server通信，并通过 Chunk Handle + Chunk Locations 来读取数据\nWrite Fig 2. Write Control and Data FlowGoole File System. Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung. Proceedings of the 19th ACM Symposium on Operating Systems Principles, ACM, Bolton Landing, NY (2003), pp. 20-43\n写文件可分为 7 步：\nClient向Master询问哪个Chunk Server持有该Chunk的当前租约，以及其他副本的位置。如果没有人有租约，则Master将租约授予它选择的Replica。\nMaster回复谁是Primary和Secondary Replica的位置。Client缓存此数据以备将来更改。仅当Primary变得不可访问或它不再持有租约时，Client才需要再次联系Master。\nClient将数据推送到所有Replica。Client可以按任何顺序执行此操作。每个 Chunk Server都会将数据存储在内部LRU缓冲区缓存中，直到数据被使用或老化。通过将数据流与控制流分离，我们可以通过基于网络拓扑调度不同数据流来提高性能，而不管哪个Chunk Server是Primary。\n一旦Client确认每个Chunk Server都收到数据，Client向Primary发送写请求，Primary可能会收到多个连续的写请求，会先将这些操作的顺序写入本地(以此来避免并发问题，顺序对于其他Replica来说是一致的)。\nPrimary做完写请求后，将写请求和顺序转发给所有的Secondary，让他们以同样的顺序写数据。\nSecondary完成后应答Primary写请求是否成功。\nPrimary应答Client所有的流程是成功还是失败。如果出现失败，Client会重试，但在重试整个写之前，会先重复步骤 3-7\nAtomic Record Appends 文件追加的操作与写文件的操作非常像。\nClient将数据推送到每个Replica，然后将写请求发往Primary。 Primary首先判断将数据追加到Chunk后是否会令CHunk的大小超过上限。如果是，那么Primary会将当前Chunk填充至其大小达到上限，并通知其他Replica执行相同的操作，再响应客户端，通知其应在下一个Chunk上重试该操作。 如果数据能够被放入到当前Chunk中，那么Primary会把数据追加到Chunk中，拿到追加成功返回的偏移量，然后通知其他Replica将数据写入到相同位置中。 最后Primary把偏移量返回给Client。 NOTE！！！\nGFS只确保数据会以一个原子的整体被追加到文件中至少一次。如果一个Replica追加不成功，那么会重试，此时可能会发生在一个Replica中出现两次该文件，但是不论如何，他们的偏移量都是一样的。\n类似于这样的(从上到下看)：\nReplica 1(primary) Replica 2(secondary) Replica 3(secondary) state A A A - B Failed B write B C C C write C B B B try write B again Consistency Fig 3. File Region State After MutationGoole File System. Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung. Proceedings of the 19th ACM Symposium on Operating Systems Principles, ACM, Bolton Landing, NY (2003), pp. 20-43\nGFS是弱一致性的模型。它并不保证一个 chunk 的所有副本是相同的。从文件追加中就可以看出来。\nInconsistent：客户端读取不同的Replica时可能会读取到不同的内容，那这部分文件是不一致的。\nConsistent：所有客户端无论读取哪个Replica都会读取到相同的内容，那这部分文件就是一致的。\nDefined：所有客户端都能看到上一次修改的完整内容，且这部分文件是一致的，那么我们说这部分文件是确定的。\nmaterials check FAQ of MIT6.824 2022 Lecture 3[3]\n为什么需要Lease 如果没有Lease，那么加入Master先指定一个chunkserver为Primary，但是它宕机或者网络延迟了，然后Master重新指定了一个chunk server，当前一个chunk server恢复后，以为自己还是Primary，这样就有两个Primary，会指定不同的次序进行写，那么就违反了一致性。有Lease，那么在第一个chunk server的Lease无效前，不会分配第二个Lease。\nmaster如何选择Primary，为什么需要版本号 对于每个chunk handle，Master在内存中有它的最新版本号，因此通过定期和chunk server交流后知道最新的chunk在哪个chunk server上，选择它为Primary；因此，版本号可以帮助识别最新的Replica数据；此外，有可能Master会看到chunk server有更大的版本号，这可能是因为可能发完租约和最新的版本号后，Master宕机了，没有更新本地的版本号，这种情况下，Master会采用更大的版本号。\nReference [1] Goole File System. Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung. Proceedings of the 19th ACM Symposium on Operating Systems Principles, ACM, Bolton Landing, NY (2003), pp. 20-43\n[2] SOSP'03 The Google File System, from zhihu\n[3] MIT 6.824 2022, distributed system\n","permalink":"https://chenghuawang.github.io/keep-moving-forward/papers/gfs/","summary":"go back to home\nPaper link\nProceedings of the 19th ACM Symposium on Operating Systems Principles, ACM, Bolton Landing, NY (2003), pp. 20-43\nLast Edit: Jan 18, 2023\nGFS 有非常多的东西，这里只写了一些重要的部分。像是snapshot，文件删除，高可用机制，Replica管理等没有具体提及。\nIntroduction GFS是google提出的一个可扩展分布式文件系统，为大型分布式数据密集型应用提供服务。可以在大规模的消费级机器集群上提供不错的容错能力。GFS在设计的时候主要依据6个假设(观察得出的):\n节点失效经常发生。系统由非常多的消费级机器组成，大量用户同时进行访问，这使得节点很容易因为程序bug、磁盘故障、内存故障等原因失效。 存储以大文件为主。每个文件通常100MB或几GB。系统需要支持小文件，但不需要对其进行特殊的优化。 大容量连续读，小容量随机读取是文件系统中的常态。 写入也已大容量为主，小容量无需特殊优化。 支持原子的文件追加操作。使得大量用户可以并行追加文件，而不需要额外的加锁机制。 高吞吐量比低延时更重要 为什么设计一个优秀的分布式存储系统非常的困难:\nPerformance. 当数据量非常大的时候，数据分片(Sharding)是非常重要的。 Fault Tolerance. 但是由于数据在多台服务器上分片。由于多台服务器，整个系统出现故障的概率会大很多。因此需要容错机制 Replication. 复制数据副本到多台服务器上，但是为了用户能够拿到一致的数据，需要考虑一致性 Consistency. 为了一致性，不得不使用网络(极慢的数据交互方式)进行确认和同步。这又会影响性能(Performance)!!! 世界因此变成了一个美妙的环。:-)。这也是分布式的挑战之处。\nGFS讨论了上述的这些主题和在实际生产场景中的应用。\nArchitecture Fig 1. GFS ArchGoole File System. Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung. Proceedings of the 19th ACM Symposium on Operating Systems Principles, ACM, Bolton Landing, NY (2003), pp.","title":"Google File System(GFS)"},{"content":"OSDI'04: 6th Symposium on Operating Systems Design and Implementation\ngo back to home\nPaper link\n什么是 MapReduce 在MapReduce原文中是这么说的\nMapReduce is a programming model and an associated implementation for processing and generating large data sets. [1]\n这是一个较为简单的处理大型问题的分布式编程模型。其产生的原因是因为 google 想要让普通的程序员也能够通过一套简单的编程模型来编写分布式应用程序，以此来处理 google 内部的大数据。然后著名的 Jeff Dean 和 Sanjay Ghemawat 出马搞定了这个问题(在知乎上关于Jeff Dean的轶事:-))。MapReduce 在 google 内部运行很长的时间并且处理了很多业务，但是目前也败下阵来，具体原因参见 MapReduce 缺陷章节(MR早在2004年就出来了，其实是古早的技术了，其没有在性能上做文章，主要在可扩展性上)。\nMapReduce 的设计框架和基本流程 MR实际上是一个非常简单的框架，思路也非常的直白。MapReduce 背后的核心思想是将数据集映射到一个 \u0026lt;key, value\u0026gt; pair的集合中，然后使用相同的键对所有pair进行整合。 整体概念很简单，但是如果你设想下下面的情况，MR实际上非常有用: 基本上所有的数据都能被映射到一个 \u0026lt;key, value\u0026gt; 对中，并且key和value可以是任意类型。\n在论文中MR使用的样例是在超大的文件里做单词统计。这个框架还可以用来做：\n分布式的排序 分布式的搜索 Web链接图遍历 \u0026hellip; Fig 1. MapReduce Archfrom \u0026ldquo;MapReduce: Simplified Data Processing on Large Clusters\u0026rdquo; googleusercontent.com.\nMR 中的Map和Reduce是从函数式编程语言(LISP)中借鉴过来的一个概念。程序员只需要编写Map和Reduce两个函数就能够对某个任务并发处理。在 Fig 1 中，MR主要的流程分为6个部分\n用户程序的MR首先将输入文件分成 M 块，每块通常为 16 MB到 64 MB(可由用户通过参数控制)。 然后，它会在一组机器上启动该程序的多个副本。\n这些副本中有一个控制程序(Master)。其余的程序(Worker)由控制程序(Master)来分配。有 M 个 Map 任务和 R 个 Reduce 任务要分配。Master 挑选空闲的 Worker 并为每个机器分配一个 Map 任务或 Reduce 任务。\n分配了 map 任务的 worker 读取相应的内容(拆分后的文件块)(设想下，如果这里的文件读取需要从数据仓库中拿出，那么整个框架的计算能力极大的被网络所限制了)。它从输入数据中解析出\u0026lt;key, value\u0026gt;对，并将每一对传递给用户定义的 Map 函数。 Map 函数产生的中间\u0026lt;key, value\u0026gt;对被缓存在内存中。\n缓存在机器中的中间\u0026lt;key, value\u0026gt;对周期性地被写入本地磁盘，由分区函数划分为 R 个区域。这些缓冲对在本地磁盘上的位置被传回 Master，Master 负责将这些位置转发给 Reduce Worker。\n当 Master 通知 Reduce Worker 有关这些位置时，它会使用RPC从 Map Worker 的本地磁盘读取缓冲数据。当 Reduce Worker 读取所有中间数据时，它会按中间键对其进行排序，以便将所有出现的相同键组合在一起。排序是因为通常许多不同的键映射到同一个 Reduce 任务。如果中间数据量太大而无法放入内存，则使用外部排序。\nReduce Worker 迭代排序的中间数据，对于遇到的每个唯一中间键，它将键和相应的中间值集传递给用户的 Reduce 函数。Reduce 函数的输出到此 Reduce 分区的最终输出文件。\n当所有的 Map 任务和 Reduce 任务都完成后，Master唤醒用户程序。此时用户程序中的 MR 调用返回给用户代码\n因为网络上的数据交换是非常缓慢的，作者在分发数据的时候尽量保证数据就在这台计算服务器上，而不需要传输数据。在原文中是这样说的：\nWe conserve network bandwidth by taking advantage of the fact that the input data (managed by GFS) is stored on the local disks of the machines that make up our cluster.\nMapReduce 的使用范例 论文中，以词频统计为例子，如论文中的伪代码：\nmap(String key, String value):\r// key: document name\r// value: document contents\rfor each word w in value:\rEmitIntermediate(w, \u0026#34;1\u0026#34;); reduce(String key, Iterator values):\r// key: a word\r// values: a list of counts\rint result = 0;\rfor each v in values:\rresult += ParseInt(v);\rEmit(AsString(result)); MR Fault Tolerance Fault Tolerance 在分布式程序中是非常重要的。MR 主要的贡献点其实就在 scalability 和 fault tolerance。\nWorker Failure Master 会定期的 ping Worker 服务器，如果不通，那么这个 Worker 被设置成 idle，分配给这台机器的任务会被分发给其他的机器。\nMaster Failure Master 因为只有少量的机器，所以发生错误的可能性非常的小。可以通过存储 Master 的状态为 checkpoints 来进行错误时回退和恢复。\nMR 缺陷 MR 的缺陷实际上在 Google I/O 上已经说明了，如下:\nToday at Google I/O, we are demonstrating Google Cloud Dataflow for the first time. Cloud Dataflow is a fully managed service for creating data pipelines that ingest, transform and analyze data in both batch and streaming modes. Cloud Dataflow is a successor to MapReduce, and is based on our internal technologies like Flume and MillWheel.[2]\nMR是一个基于 batch mode 的框架。 用 MR 写复杂的分析 pipeline 太麻烦。 在很多情况下，一次 MR 执行是没法把事情做完的，需要很多个 MR 任务互相组合，这就是 MR pipeline. 在数据流复杂的分析任务中，设计好的 pipeline 达到最高运行效率很困难。\nMR 最大的贡献我认为就是 scalability 和 fault tolerance。在 MillWheel 里，结果可以随着数据流的输入实时的显示出来，而不是到数据流结束时才输出一个结果，而且这样中间结果不需要保存下来。\nMR Lab in MIT6.824 Check the lab-mr page here\n在本实验中，我们将构建一个 MapReduce 系统。 我们将实现一个调用应用程序 Map 和 Reduce 函数并处理读写文件的程序，以及一个将任务分发给 Worker 并处理失败的 Worker 的进程(Master)。在 MR 实验中，使用 Golang 来编码实现。\n我的实现在 WSL Ubuntu20.04 上 go版本 1.18。\n这次的要求貌似比以前更难了，这次需要 Worker 主动向 Master 请求任务，当任务超时后，需要 Master 来重新分配这个任务。\n总体思路其实也非常的简单，worker 和 coordinator 的交互都围绕着 RPC 展开，故先从 RPC 的定义开始。因为深受事件轮询编程模式的\u0026rsquo;荼毒\u0026rsquo;，我把 worker 和 coordinator 之间的交互过程以事件的形式来进行处理。首先定义事件的类型。\n// src/mr/rpc.go type TaskTp = int const ( TpRequireTask = iota TpMapTaskDone TpReduceTaskDone TpSendMapTask TpSendReduceTask TpTaskAllDone TpWait ) 再来考虑 RPC 需要在 worker 和 coordinator 中传递什么信息？不论何种信息，首先都需要一个时间戳来记录 require 和 reply 对。其次每个 worker 都需要知道一共有多少个 Map 和 Reduce 任务。每个 require 需要跟上当前 worker 的任务编号。当然，因为是事件驱动的角度编写的，所以每条 require 和 reply 都需要有一个事件类型记录。\n// src/mr/rpc.go type RequireMsg struct { Stamp int64 MsgFlag TaskTp TaskID int } type ReplyMsg struct { Stamp int64 MsgFlag TaskTp TaskID int NumReduceWorkers int NumMapWorkers int Content string } 对于 coordinator，其作用像是一个状态机，需要维护所有的 worker 的状态，并能做到回退(Fault Tolerance)。Coordinator 的定义如下\n// src/mr/coordinator.go type Coordinator struct { nMapWorkers int nReduceWorkers int nMapWorking int nReduceWorking int MapWorkerState []int64 ReduceWorkerState []int64 FilesContent []string MapWorkerPool chan int ReduceWorkerPool chan int IsDone bool GlobalLock *sync.Cond // to make sure the rpc visit is atomic. } coordinator 需要处理 RPC 的请求，对于不同的事件进行反应。因为需要超时处理，所以在这里，每一个 worker 都会有一个协程来进行相应的计时和 id 回收。\n// src/mr/coordinator.go func (c *Coordinator) ProcessEvents(args *RequireMsg, reply *ReplyMsg) error { c.GlobalLock.L.Lock() tmpBool := c.IsDone c.GlobalLock.L.Unlock() if tmpBool { reply.MsgFlag = TpTaskAllDone reply.Stamp = args.Stamp return nil } switch args.MsgFlag { case TpMapTaskDone: c.GlobalLock.L.Lock() if c.MapWorkerState[args.TaskID] == args.Stamp { c.MapWorkerState[args.TaskID] = 1 c.nMapWorking-- } c.GlobalLock.L.Unlock() case TpReduceTaskDone: c.GlobalLock.L.Lock() if c.ReduceWorkerState[args.TaskID] == args.Stamp { c.ReduceWorkerState[args.TaskID] = 1 c.nReduceWorking-- } if c.nReduceWorking == 0 { c.IsDone = true } c.GlobalLock.L.Unlock() case TpRequireTask: if len(c.MapWorkerPool) \u0026gt; 0 { // State 1. Send Map Task to worker. reply.Stamp = args.Stamp reply.TaskID = \u0026lt;-c.MapWorkerPool reply.MsgFlag = TpSendMapTask reply.NumMapWorkers = c.nMapWorkers reply.NumReduceWorkers = c.nReduceWorkers reply.Content = c.FilesContent[reply.TaskID] c.GlobalLock.L.Lock() c.MapWorkerState[reply.TaskID] = args.Stamp c.GlobalLock.L.Unlock() go func(id int) { time.Sleep(10 * time.Second) c.GlobalLock.L.Lock() if c.MapWorkerState[id] != 1 { // run out of 10 secs. recycle this id to id pool. c.MapWorkerPool \u0026lt;- id } c.GlobalLock.L.Unlock() }(reply.TaskID) return nil } else { c.GlobalLock.L.Lock() nMapOnWorking := c.nMapWorking nReduceOnWorking := c.nReduceWorking c.GlobalLock.L.Unlock() // State 2. All map worker is on working. wait. if nMapOnWorking != 0 { reply.Stamp = args.Stamp reply.TaskID = -1 reply.MsgFlag = TpWait reply.Content = \u0026#34;Just Wait !!! Map Worker !!!\u0026#34; reply.NumMapWorkers = c.nMapWorkers reply.NumReduceWorkers = c.nReduceWorkers } else { // State 3. Map is Done. Send Reduce task to works. if len(c.ReduceWorkerPool) \u0026gt; 0 { reply.Stamp = args.Stamp reply.TaskID = \u0026lt;-c.ReduceWorkerPool reply.MsgFlag = TpSendReduceTask reply.Content = \u0026#34;Reduce no need to handle this\u0026#34; reply.NumMapWorkers = c.nMapWorkers reply.NumReduceWorkers = c.nReduceWorkers c.GlobalLock.L.Lock() c.ReduceWorkerState[reply.TaskID] = args.Stamp c.GlobalLock.L.Unlock() go func(id int) { time.Sleep(10 * time.Second) c.GlobalLock.L.Lock() if c.ReduceWorkerState[id] != 1 { // run out of 10 secs. resolved this id to id pool. c.ReduceWorkerPool \u0026lt;- id } c.GlobalLock.L.Unlock() }(reply.TaskID) } else { // State 4. All reduce worker is on working. wait. if nReduceOnWorking != 0 { reply.Stamp = args.Stamp reply.TaskID = -1 reply.MsgFlag = TpWait reply.Content = \u0026#34;Just Wait !!! Reduce Worker !!!\u0026#34; reply.NumMapWorkers = c.nMapWorkers reply.NumReduceWorkers = c.nReduceWorkers } } } } default: return nil } return nil } 同样的，在 worker 中的流程也是不断的循环产生事件，处理事件\n// src/mr/worker.go func Worker(mapf func(string, string) []KeyValue, reducef func(string, []string) string) { for true { ThisStamp := time.Now().Unix() Req := RequireMsg{} Req.Stamp = ThisStamp Req.MsgFlag = TpRequireTask Req.TaskID = -1 Rep := ReplyMsg{} ok := call(\u0026#34;Coordinator.ProcessEvents\u0026#34;, \u0026amp;Req, \u0026amp;Rep) if ok \u0026amp;\u0026amp; Rep.Stamp == Req.Stamp { switch Rep.MsgFlag { case TpSendMapTask: doMap(mapf, \u0026amp;Rep) case TpSendReduceTask: doReduce(reducef, \u0026amp;Rep) case TpWait: time.Sleep(time.Second) case TpTaskAllDone: return default: return } } } return } doMap(mapf, \u0026amp;Rep) 和 doReduce(reducef, \u0026amp;Rep) 就是非常简单的逻辑编写了。Map需要把文件分拆到N=numReduce块中，总共产生N * M(reduce num, map num)块文件。Reduce worker从自己对应的编号中取出M块进行排序和合并。\n结果顺利通过，没有过多的延迟(因为10s的判断)产生。\nFig 2. test mr Reference [1] \u0026ldquo;MapReduce: Simplified Data Processing on Large Clusters\u0026rdquo; googleusercontent.com.\n[2] Google cloud blog about MapReduce\u0026rsquo;s successor\n","permalink":"https://chenghuawang.github.io/keep-moving-forward/papers/mapreduce/","summary":"OSDI'04: 6th Symposium on Operating Systems Design and Implementation\ngo back to home\nPaper link\n什么是 MapReduce 在MapReduce原文中是这么说的\nMapReduce is a programming model and an associated implementation for processing and generating large data sets. [1]\n这是一个较为简单的处理大型问题的分布式编程模型。其产生的原因是因为 google 想要让普通的程序员也能够通过一套简单的编程模型来编写分布式应用程序，以此来处理 google 内部的大数据。然后著名的 Jeff Dean 和 Sanjay Ghemawat 出马搞定了这个问题(在知乎上关于Jeff Dean的轶事:-))。MapReduce 在 google 内部运行很长的时间并且处理了很多业务，但是目前也败下阵来，具体原因参见 MapReduce 缺陷章节(MR早在2004年就出来了，其实是古早的技术了，其没有在性能上做文章，主要在可扩展性上)。\nMapReduce 的设计框架和基本流程 MR实际上是一个非常简单的框架，思路也非常的直白。MapReduce 背后的核心思想是将数据集映射到一个 \u0026lt;key, value\u0026gt; pair的集合中，然后使用相同的键对所有pair进行整合。 整体概念很简单，但是如果你设想下下面的情况，MR实际上非常有用: 基本上所有的数据都能被映射到一个 \u0026lt;key, value\u0026gt; 对中，并且key和value可以是任意类型。\n在论文中MR使用的样例是在超大的文件里做单词统计。这个框架还可以用来做：\n分布式的排序 分布式的搜索 Web链接图遍历 \u0026hellip; Fig 1.","title":"MapReduce"},{"content":"","permalink":"https://chenghuawang.github.io/keep-moving-forward/about/news/","summary":"","title":"🎉News🎉"},{"content":"Hi👋, I\u0026rsquo;m Chenghua Wang, currently a postgraduate CS student at BUPT.\nI\u0026rsquo;m interested in AI\u0026amp;Sys. I\u0026rsquo;ve struggled for almost one and half years on computer vision(Dehazing, Multi-label classification. July 1 2021 -\u0026gt; Dec 31 2022) contact me: chenghua.wang.edu@gmail.com\nResearch Interests Machine Learning Infrastructure\nML Compiler NNCV(ZJGSU, BS dissertation), it contains an Aten-lang auto-parallel language(using Polyhedra Algorithms) and a set of deep learning compilation pipelines(Based on MLIR). Distributed and Parallel System Design Computer Vision\nlow-level tasks(ZJGSU, Image dehazing) multi-label multi-class classification object detection Research Experience Multimedia And Computer Vision Laboratory, ZJGSU. July 1 2021 -\u0026gt; Dec 31 2022. Image Dehazing Medical Image Processing(Palm-print, multi-class multi-label classification) Projects [Projects(click to expand)]\rnncv\nNeural Network inference\u0026amp;compile toolchain for Computer Vision.\nA Project of my BS dissertation\nAn Aten language support Polyhedral model for CPU(X86 with AVX2) target. A small pipeline which use tiling and vectorization method to optimize model from torch. Support X86(with AVX2) and partial Tensor Core instruction. using command below to compile a model from torch:\n$nncv-c -target HostWParallel -split-params res18.mlir -o optimized.mlir call from C++ side:\n#include \u0026#34;libnncv/DataType.hpp\u0026#34; #include \u0026#34;libnncv/SystemIo.hpp\u0026#34; using namespace nncv::rt; MemRefFlatArray params(dataType::kFloat32); params.read(\u0026#34;model.bin\u0026#34;) optimized_res18_forward(/*dst*/..., /*src*/..., params.get()); mgloria\nA Matrix lib based on SIMD and expression template(Lazy Compute). On CPU/GPU. Currently, GPU functionality is not fully implemented\n#include \u0026#34;mgloria/core.hpp\u0026#34; inline void __test_tensor_basic_OP__() { using namespace mgloria; InitTensorComputeMachine\u0026lt;CPU\u0026gt;(0); struct ReLU { MGLORIA_INLINE_NORMAL static float Do(float t) { if (t \u0026gt; 0.f) return t; return 0.f; } }; auto __stream__ = NewStream\u0026lt;CPU\u0026gt;(0); Tensor\u0026lt;CPU, 3\u0026gt; A = NewTensor(makeShape3d(2, 3, 5), true, 2.f, true, __stream__); Tensor\u0026lt;CPU, 3\u0026gt; B = NewTensor(makeShape3d(2, 3, 5), true, 1.f, true, __stream__); Tensor\u0026lt;CPU, 3\u0026gt; C = NewTensor(makeShape3d(2, 3, 5), true, 5.f, true, __stream__); A = B + C; // A = expr::implicit_dot(B, C); A = expr::Func\u0026lt;ReLU\u0026gt;(A); std::cout \u0026lt;\u0026lt; A; std::cout \u0026lt;\u0026lt; B; std::cout \u0026lt;\u0026lt; C; FreeStream(__stream__); ShutdownTensorComputeMachine\u0026lt;CPU\u0026gt;(0); } covalent bond\nA tool for managing distributed database, gathering/cleaning data, etc. covalentBond(cb) using Op graph concept to allow users to easily build query logic and data manipulation logic. We took a lot of inspiration from torch and bound most of cb\u0026rsquo;s operators in lua, making it very easy for users to construct and overload the basic behavior of graphs. This project is for 2022-2023 Fall, SE lecture.\ndaydream engine\nA render lib. Compatible with ShaderToy, can switch between 2d and 3d rendering modes. For 2022-2023 Fall, CG lecture. This lib is shown as a 3d editor below(this editor is also a part of daydream lib):\nkeep-moving-forward\nSome notes of papars I read, and some tech report. Including topics: Distributed system, Machine learning system(compiler), HPC, AI\nimage-dehazing-the-end\nA tech report of my image dehazing research from Jan 1 2022 to Dec 31 2022\npic 1(a) real-world id=3\rpic 1(b) dehazed id=3\rpic 2(a) real-world id=5\rpic 2(b) dehazed id=5 pic 3(a) real-world id=104\rpic 3(b) dehazed id=104 pic 4(a) real-world id=8\rpic 4(b) dehazed id=8 pic 5(a) real-world id=12\rpic 5(b) dehazed id=12 Languages \u0026amp; Skills Languages used: c, c++, python, golang, asm(x86, risc-v), cuda, glsl, lua, dart\nSkills: MLIR, pytorch, paddlepaddle, opencv, opengl, mysql, redis, flutter, nginx, django\n","permalink":"https://chenghuawang.github.io/keep-moving-forward/about/about/","summary":"Hi👋, I\u0026rsquo;m Chenghua Wang, currently a postgraduate CS student at BUPT.\nI\u0026rsquo;m interested in AI\u0026amp;Sys. I\u0026rsquo;ve struggled for almost one and half years on computer vision(Dehazing, Multi-label classification. July 1 2021 -\u0026gt; Dec 31 2022) contact me: chenghua.wang.edu@gmail.com\nResearch Interests Machine Learning Infrastructure\nML Compiler NNCV(ZJGSU, BS dissertation), it contains an Aten-lang auto-parallel language(using Polyhedra Algorithms) and a set of deep learning compilation pipelines(Based on MLIR). Distributed and Parallel System Design Computer Vision","title":"About"},{"content":" 日期 更新内容 2024-01-01 文章创建 2024-08-20 1. 根据最近的AI\u0026amp;Sys发展情况做了跟进 2. 对几个AI\u0026amp;Sys分支给出了学习路径和必读的文献 本文是笔者在学习AI\u0026amp;Sys的过程中梳理出来的。本文默认读者掌握了：\n基础的深度学习知识，对计算机视觉(完成大部分cs231n lab)有一定的了解。 能熟练使用Python、C++完成中型的项目。 对操作系统、体系结构有一定的了解(大部分CSAPP lab完成，理解了OSTEP书中的大部分知识/完成MIT 6.S081 XV6 lab大部分内容)。 对 CUDA 编程模型有了解，不要求使用。 本文只记录入门需要看的书籍/课程/论文/项目等，暂时不包括更加深入的内容。\n1. AI\u0026amp;Sys / MLSys / LLMSys的基础内容 1.1 课程 TinyML and Efficient Deep Learning Computing, MIT han lab\nMIT 6.5940\n韩松老师主讲的课程\n课程质量很高，在B站有FAll 2023的视屏。推荐入门的同学可以先看看这个，可以带你参观整个MLSys相关的大部分领域（偏向算法）。\nCS559E, cs.washington：\ncs.washington CSE559M\n完整的讲述了MLSys大部分的领域（偏向Sys），类似一个综述类型的课程，可以带领你了解完大部分MLSys的领域。\n缺点是没有视屏，只有一些资料。\nLarge Language Model Systems, CMU\nCMU 11868\n与LLM联系的更加紧密一点，如RAG，大模型Serving。还有一些Sys上的，如GPU just-in-time compilation、Communication Efficient Distributed Training等。\n推荐先看韩松老师的课程来获得一个全局的视角（主要是补齐大模型的一些基本算法知识，大部分是推理上的，训练的知识是缺失的）。然后其他的课程可以选择看看，或者直接看您想从事方向的论文。\n1.2 阅读材料 笔者总结了一些常识性的阅读材料，仅供入门阅读，可能会有重复，读者可以选择一些来阅读。阅读材料比较多，阅读的时候应该详略得当，明确主攻哪块方向。\nLLM Basics\nRequired:\nAttention Is All You Need RoFormer: Enhanced Transformer with Rotary Position Embedding Optional:\nLlama 2: Open Foundation and Fine-Tuned Chat Models The Llama 3 Herd of Models Fine-Tuning：\nRequired:\nLoRA: Low-Rank Adaptation of Large Language Models Parameter-Efficient Transfer Learning for NLP The Power of Scale for Parameter-Efficient Prompt Tuning Optional:\nQLORA: E cient Finetuning of Quantized LLMs Quantization：\nRequired:\nTinyML and Efficient Deep Learning Computing Quantization(Part 1 and 2) GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration Optional:\nQuantization Algorithms - Distiller Documentation. From Intel AI Lab SpinQuant: LLM quantization with learned rotations Pruning / Sparsification：\nRequired:\nOptional:\nKV Cache Optimization：\nRequired:\nEfficient Memory Management for Large Language Model Serving with PagedAttention Prompt Cache: Modular Attention Reuse for Low-Latency Inference Optional:\nSGLang: Efficient Execution of Structured Language Model Programs Attention Acceleration：\nRequired:\nFrom Online Softmax to FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning Simple Hardware-Efficient Long Convolutions for Sequence Modeling Optional:\nFlash-Decoding for long-context inference FlashDecoding++: Faster Large Language Model Inference on GPUs Parallel Decoding：\nRequired:\nFast Inference from Transformers via Speculative Decoding Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads Optional:\nEAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty ML Compiler：\nRequired:\nTVM: An Automated End-to-End Optimizing Compiler for Deep Learning MLIR: A Compiler Infrastructure for the End of Moore\u0026rsquo;s Law Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations Optional:\nRammer: Enabling Holistic Deep Learning Compiler Optimizations with rTasks TASO: optimizing deep learning computation with automatic generation of graph substitutions Tensor Program Optimization with Probabilistic Programs LLM On Device Training：\nRequired:\nOptional:\nLLM Training Parallelism / Mem Optmization：\nRequired:\nOptional:\nLarge Scale LLM Serving / Inference：\nRequired:\nOptional:\nHardware Arch：\nRequired:\nOptional:\n1.3 项目 2. 基础技能 2.1 硬件相关 2.2 高性能计算 1. 前置知识 对于前置知识，默认已经通过了 MIT 6.S081 即以上难度的 OS 课程历练；有 Deep Learning 方面的课程基础或者科研经历；在体系结构上有一定的了解(从 CSAPP 到计组学完)。对于 C++ 编程较为熟练，能够使用 CMake 构建中型项目；能够使用 Pybind，对于 Python 高级编程较为熟练。在编译工具链(Clang, LLVM) 上有一定的了解，能够使用。对 CUDA 编程模型有了解，不要求使用。\n1.1 Courses 1.1.1 CMU15418 Parallel computing Note: 2023 的视频没有公开，目前能够找到的最新的视频是 2018 年的，这个领域发展较快，初次入门还是选择 CS267。\nSpring 2023 Homepage\n并行计算入门课程，Lab 工作量非常的巨大。涉及现代多处理器，SIMD，分布式通讯协议MPI，GPU加速CUDA编程，异构计算，同步，Cache，等。\n1.1.2 UCB CS267 Applications of Parallel Computers Spring 2022 Homepage\n1.1.3 Stanford 143: Compilers Homepage\n现在有很多的自动并行做法想要使用编译技术来统一的生成调度代码和优化后的 kernel，编译技术是值得学习的。但是这门课是传统编译，和 MLIR 那一套的后端是有共性但是不是一致的，建议只看编译前端部分，后端部分可以较为简略的来看。\n1.2 Tools 1.2.1 CUDA 2023-04-18, CUDA: NSight System, link 2. Machine Learning System 2023-05-02, 浅析机器学习中的并行模型和自动并行方法, link A1. 相关领域的文章 cs.washington CSE559M 罗列了基本的阅读资料 ","permalink":"https://chenghuawang.github.io/keep-moving-forward/about/hpc_ai/","summary":"日期 更新内容 2024-01-01 文章创建 2024-08-20 1. 根据最近的AI\u0026amp;Sys发展情况做了跟进 2. 对几个AI\u0026amp;Sys分支给出了学习路径和必读的文献 本文是笔者在学习AI\u0026amp;Sys的过程中梳理出来的。本文默认读者掌握了：\n基础的深度学习知识，对计算机视觉(完成大部分cs231n lab)有一定的了解。 能熟练使用Python、C++完成中型的项目。 对操作系统、体系结构有一定的了解(大部分CSAPP lab完成，理解了OSTEP书中的大部分知识/完成MIT 6.S081 XV6 lab大部分内容)。 对 CUDA 编程模型有了解，不要求使用。 本文只记录入门需要看的书籍/课程/论文/项目等，暂时不包括更加深入的内容。\n1. AI\u0026amp;Sys / MLSys / LLMSys的基础内容 1.1 课程 TinyML and Efficient Deep Learning Computing, MIT han lab\nMIT 6.5940\n韩松老师主讲的课程\n课程质量很高，在B站有FAll 2023的视屏。推荐入门的同学可以先看看这个，可以带你参观整个MLSys相关的大部分领域（偏向算法）。\nCS559E, cs.washington：\ncs.washington CSE559M\n完整的讲述了MLSys大部分的领域（偏向Sys），类似一个综述类型的课程，可以带领你了解完大部分MLSys的领域。\n缺点是没有视屏，只有一些资料。\nLarge Language Model Systems, CMU\nCMU 11868\n与LLM联系的更加紧密一点，如RAG，大模型Serving。还有一些Sys上的，如GPU just-in-time compilation、Communication Efficient Distributed Training等。\n推荐先看韩松老师的课程来获得一个全局的视角（主要是补齐大模型的一些基本算法知识，大部分是推理上的，训练的知识是缺失的）。然后其他的课程可以选择看看，或者直接看您想从事方向的论文。\n1.2 阅读材料 笔者总结了一些常识性的阅读材料，仅供入门阅读，可能会有重复，读者可以选择一些来阅读。阅读材料比较多，阅读的时候应该详略得当，明确主攻哪块方向。\nLLM Basics","title":"AI \u0026 Sys 入门"},{"content":" 目前头脑还是一片荒芜\n","permalink":"https://chenghuawang.github.io/keep-moving-forward/about/thingking/","summary":"目前头脑还是一片荒芜","title":"思考"},{"content":"Fundamental 2024-08-10, [Fundamental]From Online Softmax to Flash Attention V3 2024-08-11, [Fundamental] 旋转位置编码(RoPE) 2024-08-19, [Fundamental] 模型量化 2024-08-21, [Fundamental] FlashDecoding Series MIT6.S081, XV6 Lab 2023-02-23, XV6 lab 1 Utilities 2023-02-25, XV6 lab 2 syscall 2023-03-02, XV6 lab 3 page table 2023-03-12, XV6 lab 4 traps 2023-03-17, XV6 lab 5 copy on write Tools 2023-04-18, CUDA: NSight System 2024-10-08, Xnnpack 使用指南 2024-10-12, Roofline Model Parallel 2023-05-02, 浅析机器学习中的并行模型和自动并行方法 Framework 2024-06-28, mllm框架浅析(一)-以QWen0.5B为例 2024-08-13, mllm框架浅析(二)-QNN-Backend Kernel Impls 2024-09-17, Q8_0 @ Q4_0_4 GEMM/GEMV in llama.cpp ","permalink":"https://chenghuawang.github.io/keep-moving-forward/about/tech_posts/","summary":"Fundamental 2024-08-10, [Fundamental]From Online Softmax to Flash Attention V3 2024-08-11, [Fundamental] 旋转位置编码(RoPE) 2024-08-19, [Fundamental] 模型量化 2024-08-21, [Fundamental] FlashDecoding Series MIT6.S081, XV6 Lab 2023-02-23, XV6 lab 1 Utilities 2023-02-25, XV6 lab 2 syscall 2023-03-02, XV6 lab 3 page table 2023-03-12, XV6 lab 4 traps 2023-03-17, XV6 lab 5 copy on write Tools 2023-04-18, CUDA: NSight System 2024-10-08, Xnnpack 使用指南 2024-10-12, Roofline Model Parallel 2023-05-02, 浅析机器学习中的并行模型和自动并行方法 Framework 2024-06-28, mllm框架浅析(一)-以QWen0.5B为例 2024-08-13, mllm框架浅析(二)-QNN-Backend Kernel Impls 2024-09-17, Q8_0 @ Q4_0_4 GEMM/GEMV in llama.","title":"技术相关"},{"content":"Fundamental 2024-08-21, [Fundamental] FlashDecoding Series\n2024-08-19, [Fundamental] 模型量化\n2024-08-11, [Fundamental] 旋转位置编码(RoPE)\n2024-08-10, [Fundamental]From Online Softmax to Flash Attention V3\nDistributed System 2023-01-19, The Design of a Practical System for Fault-Tolerant Virtual Machines\n2023-01-18, Google File System(GFS)\n2023-01-17, MapReduce\n量化 2024-08-15, ✅[April-May 2024] 模型量化之 🥕Quarot \u0026amp; SpinQuant\n里程碑，旋转矩阵缓解Outliers\n2024-06-25, ✅[Oct 2023] 模型量化之QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\nMLSys2024， MoE量化\n2024-05-25, ✅[April 2024] 模型量化之AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration\nMLSys 2024 Best Paper\nKV Cache/Prompt Cache/Attention Acceleration 2024-08-10, [Fundamental]From Online Softmax to Flash Attention V3\n里程碑，FA 1-3\n2024-06-21, ✅[April 2024] Prompt Cache: Modular Attention Reuse for Low-Latency Inference\nMLSys 2024，prompt cache优化\nEdge 2024-06-17, ✅[Mar 2024] Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs\n边缘Transformer部署优化 from OPPO\n","permalink":"https://chenghuawang.github.io/keep-moving-forward/about/paper_posts/","summary":"Fundamental 2024-08-21, [Fundamental] FlashDecoding Series\n2024-08-19, [Fundamental] 模型量化\n2024-08-11, [Fundamental] 旋转位置编码(RoPE)\n2024-08-10, [Fundamental]From Online Softmax to Flash Attention V3\nDistributed System 2023-01-19, The Design of a Practical System for Fault-Tolerant Virtual Machines\n2023-01-18, Google File System(GFS)\n2023-01-17, MapReduce\n量化 2024-08-15, ✅[April-May 2024] 模型量化之 🥕Quarot \u0026amp; SpinQuant\n里程碑，旋转矩阵缓解Outliers\n2024-06-25, ✅[Oct 2023] 模型量化之QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\nMLSys2024， MoE量化\n2024-05-25, ✅[April 2024] 模型量化之AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration","title":"论文解析"}]