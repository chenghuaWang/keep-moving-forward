<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>✅[Mar 2024] Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs - Ubios Home</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="✅[Mar 2024] Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs" />
<meta property="og:description" content="Li L, Qian S, Lu J, et al. Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs. arxiv preprint arxiv:2403.20041, 2024.
无代码，技术报告
⭐️⭐️⭐️
http://arxiv.org/abs/2403.20041
1. 背景 &amp; 动机 这篇论文是OPPO AI Center发表的，其提出了Transformer-Lite框架来缓解移动设备GPU上部署大型语言模型（LLM）时存在的性能问题。大型语言模型（如ChatGLM2 6B和Gemma 2B）被广泛应用于智能助手、文本摘要、翻译和多模态任务等，但它们在移动设备上的部署面临着几个关键问题：
计算能力和内存带宽的需求：LLMs需要大量的计算能力和内存带宽，这些资源在移动设备上是有限的。 用户体验：当前的在端侧设备上部署LLM的方法通常有着较慢的推理速度，这会严重影响用户体验。 成本：在云上大规模部署LLM有着可观的成本，而且随着移动设备性能的不断提升，本地部署LLM不仅可以减少与云部署相关的高成本，还可以扩大LLM在移动设备上的应用前景以及保护用户隐私。 为了解决上述问题。这篇文章文章提出了以下优化技术：
符号表达式的方法：支持动态形状模型推理，包括动态形状推导、内存重用和执行调度等。 算子优化和执行优先级设置：加快推理速度并减少手机延迟。 FP4量化方法：称为M0E4，减少dequantize开销，使得矩阵乘法更高效。 基于子张量的技巧：避免了在LLM推理后复制KV缓存的内存压力。 作者团队还实现了一个名为Transformer-Lite的移动推理引擎，该引擎与高通和联发科处理器兼容，并通过一系列实验评估了其性能。通过这些技术，Transformer-Lite引擎在prefill和decoding方面相比基于CPU的FastLLM和基于GPU的MLC-LLM取得了显著的速度提升。
2. 方法 2.1 对于动态形状推理的符号表达方法 不同于多数静态尺寸的CV模型，LLM 是一种动态形状输入的场景，每次迭代的输入形状都会发生变化。这导致模型中一些激活张量的形状发生变化，给内存重用和算子性能优化带来了巨大挑战。 例如，将形状[“sumN-N”,1,2,128]与axis 0上的[“N”,1,2,128]连接，输出形状为[“sumN”,1,2,128]。为此，作者团队利用类似于 Nimble 和 DISC 的方法，将深度学习模型中的算子分为两类：张量计算算子和形状计算算子。后一类算子包含形状算子和那些其输入取决于形状算子结果的算子。形状计算算子在 CPU 上执行，计算形状信息。相反，负责计算激活的张量和权重的张量的计算算子则在 GPU 上执行。
Fig 1. 形状推理作者团队使用了SymPy包且结合了ONNX来实现了这一能力。
对于内存复用，实现内存复用的先决条件是获得张量之间的内存大小关系，这在静态形状推理中很简单。符号表达式便于轻松确定这种关系。首先，计算每个张量的内存大小，即元素数乘以数据类型的字节数，从而为每个张量的内存大小创建一个符号表达式。**然后结合减法和除法来辨别张量之间的内存大小关系。**例如，用 “N ”*4096 除以 “N ”32128，得到的整数为 1，表示大小相等，可以重复使用内存。另一方面，当 “N” * 4096 与 “sumN” * 2 * 128 相比较时，减法和除法都会产生另一个符号表达式，而不是一个确定的整数。因此，除非探索所有潜在的输入形状，否则它们之间的大小关系是不确定的，这意味着无法进行内存重用。 作者团队使用了OpenCL工具来实现了内存复用。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chenghuawang.github.io/keep-moving-forward/papers/transformer-lite/" /><meta property="article:section" content="Papers" />
<meta property="article:published_time" content="2024-06-17T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-06-17T00:00:00+00:00" />

		<meta itemprop="name" content="✅[Mar 2024] Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs">
<meta itemprop="description" content="Li L, Qian S, Lu J, et al. Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs. arxiv preprint arxiv:2403.20041, 2024.
无代码，技术报告
⭐️⭐️⭐️
http://arxiv.org/abs/2403.20041
1. 背景 &amp; 动机 这篇论文是OPPO AI Center发表的，其提出了Transformer-Lite框架来缓解移动设备GPU上部署大型语言模型（LLM）时存在的性能问题。大型语言模型（如ChatGLM2 6B和Gemma 2B）被广泛应用于智能助手、文本摘要、翻译和多模态任务等，但它们在移动设备上的部署面临着几个关键问题：
计算能力和内存带宽的需求：LLMs需要大量的计算能力和内存带宽，这些资源在移动设备上是有限的。 用户体验：当前的在端侧设备上部署LLM的方法通常有着较慢的推理速度，这会严重影响用户体验。 成本：在云上大规模部署LLM有着可观的成本，而且随着移动设备性能的不断提升，本地部署LLM不仅可以减少与云部署相关的高成本，还可以扩大LLM在移动设备上的应用前景以及保护用户隐私。 为了解决上述问题。这篇文章文章提出了以下优化技术：
符号表达式的方法：支持动态形状模型推理，包括动态形状推导、内存重用和执行调度等。 算子优化和执行优先级设置：加快推理速度并减少手机延迟。 FP4量化方法：称为M0E4，减少dequantize开销，使得矩阵乘法更高效。 基于子张量的技巧：避免了在LLM推理后复制KV缓存的内存压力。 作者团队还实现了一个名为Transformer-Lite的移动推理引擎，该引擎与高通和联发科处理器兼容，并通过一系列实验评估了其性能。通过这些技术，Transformer-Lite引擎在prefill和decoding方面相比基于CPU的FastLLM和基于GPU的MLC-LLM取得了显著的速度提升。
2. 方法 2.1 对于动态形状推理的符号表达方法 不同于多数静态尺寸的CV模型，LLM 是一种动态形状输入的场景，每次迭代的输入形状都会发生变化。这导致模型中一些激活张量的形状发生变化，给内存重用和算子性能优化带来了巨大挑战。 例如，将形状[“sumN-N”,1,2,128]与axis 0上的[“N”,1,2,128]连接，输出形状为[“sumN”,1,2,128]。为此，作者团队利用类似于 Nimble 和 DISC 的方法，将深度学习模型中的算子分为两类：张量计算算子和形状计算算子。后一类算子包含形状算子和那些其输入取决于形状算子结果的算子。形状计算算子在 CPU 上执行，计算形状信息。相反，负责计算激活的张量和权重的张量的计算算子则在 GPU 上执行。
Fig 1. 形状推理作者团队使用了SymPy包且结合了ONNX来实现了这一能力。
对于内存复用，实现内存复用的先决条件是获得张量之间的内存大小关系，这在静态形状推理中很简单。符号表达式便于轻松确定这种关系。首先，计算每个张量的内存大小，即元素数乘以数据类型的字节数，从而为每个张量的内存大小创建一个符号表达式。**然后结合减法和除法来辨别张量之间的内存大小关系。**例如，用 “N ”*4096 除以 “N ”32128，得到的整数为 1，表示大小相等，可以重复使用内存。另一方面，当 “N” * 4096 与 “sumN” * 2 * 128 相比较时，减法和除法都会产生另一个符号表达式，而不是一个确定的整数。因此，除非探索所有潜在的输入形状，否则它们之间的大小关系是不确定的，这意味着无法进行内存重用。 作者团队使用了OpenCL工具来实现了内存复用。"><meta itemprop="datePublished" content="2024-06-17T00:00:00+00:00" />
<meta itemprop="dateModified" content="2024-06-17T00:00:00+00:00" />
<meta itemprop="wordCount" content="142">
<meta itemprop="keywords" content="LLM Server,Edge,LLM," />
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans+SC:400,700">

	<link rel="stylesheet" href="/keep-moving-forward/css/style.css">
	

	<link rel="shortcut icon" href="/keep-moving-forward/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		
<header class="header">
	<div class="container header__container">
		
	<div class="logo logo--mixed" >
		<a class="logo__link" href="/keep-moving-forward/" title="Ubios Home" rel="home" >
			
			<div class="logo__item logo__text" >
					<div class="logo__title" >Ubios Home</div>
					<div class="logo__tagline">Remember brick walls let us show our dedication. They are there to separate us from the people who don&#39;t really want to achieve their childhood dreams. --Randy Pausch</div>
				</div>
		</a>
	</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/keep-moving-forward/about/about/">
				
				<span class="menu__text">About</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/keep-moving-forward/about/hpc_ai/">
				
				<span class="menu__text">HPC &amp; AI 入坑</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/keep-moving-forward/about/lecture_notes/">
				
				<span class="menu__text">Lecture-Notes</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/keep-moving-forward/about/paper_posts/">
				
				<span class="menu__text">Paper-Notes</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/keep-moving-forward/about/tech_posts/">
				
				<span class="menu__text">Tech-Posts</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/keep-moving-forward/about/thinking/">
				
				<span class="menu__text">Thinking</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/keep-moving-forward/about/news/">
				
				<span class="menu__text">🎉News🎉</span>
				
			</a>
		</li>
	</ul>
</nav>

	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			


<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">✅[Mar 2024] Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs</h1>
			<p class="post__lead">Transformer-Lite from OPPO</p>
			<div class="post__meta meta"><div class="meta__item-author meta__item">
	<svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2 0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class="meta__text">chenghua.wang</span>
</div>
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class="meta__text" datetime="2024-06-17T00:00:00Z">2024-06-17</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg><span class="meta__text"><a class="meta__link" href="/keep-moving-forward/categories/aisys/" rel="category">AI&amp;Sys</a>
	</span>
</div></div>
		</header>

		
<div class="post__toc toc">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
		<nav id="TableOfContents">
  <ul>
    <li><a href="#1-背景--动机">1. 背景 &amp; 动机</a></li>
    <li><a href="#2-方法">2. 方法</a>
      <ul>
        <li><a href="#21-对于动态形状推理的符号表达方法">2.1 对于动态形状推理的符号表达方法</a></li>
        <li><a href="#22-算子优化和执行优先级设置">2.2 算子优化和执行优先级设置</a></li>
        <li><a href="#23-fp4量化方法m0e4">2.3 FP4量化方法：M0E4</a></li>
        <li><a href="#24-基于subtensor来缓解kv-cache压力">2.4 基于subtensor来缓解KV Cache压力</a></li>
      </ul>
    </li>
    <li><a href="#3-实验">3. 实验</a>
      <ul>
        <li><a href="#31-各个llm上的性能">3.1 各个LLM上的性能</a></li>
        <li><a href="#32-和fastllm和mlc-llm比较">3.2 和FastLLM和MLC-LLM比较</a></li>
      </ul>
    </li>
  </ul>
</nav>
	</div>
</div><div class="content post__content clearfix">
			<p>Li L, Qian S, Lu J, et al. Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs. arxiv preprint arxiv:2403.20041, 2024.</p>
<p>无代码，技术报告</p>
<p>⭐️⭐️⭐️</p>
<p><a href="http://arxiv.org/abs/2403.20041">http://arxiv.org/abs/2403.20041</a></p>
<hr>
<h1 id="1-背景--动机">1. 背景 &amp; 动机</h1>
<p>这篇论文是OPPO AI Center发表的，其提出了Transformer-Lite框架来缓解移动设备GPU上部署大型语言模型（LLM）时存在的性能问题。大型语言模型（如ChatGLM2 6B和Gemma 2B）被广泛应用于智能助手、文本摘要、翻译和多模态任务等，但它们在移动设备上的部署面临着几个关键问题：</p>
<ol>
<li><strong>计算能力和内存带宽的需求</strong>：LLMs需要大量的计算能力和内存带宽，这些资源在移动设备上是有限的。</li>
<li><strong>用户体验</strong>：当前的在端侧设备上部署LLM的方法通常有着较慢的推理速度，这会严重影响用户体验。</li>
<li><strong>成本</strong>：在云上大规模部署LLM有着可观的成本，而且随着移动设备性能的不断提升，本地部署LLM不仅可以减少与云部署相关的高成本，还可以扩大LLM在移动设备上的应用前景以及保护用户隐私。</li>
</ol>
<p>为了解决上述问题。这篇文章文章提出了以下优化技术：</p>
<ul>
<li><strong>符号表达式的方法</strong>：支持动态形状模型推理，包括动态形状推导、内存重用和执行调度等。</li>
<li><strong>算子优化和执行优先级设置</strong>：加快推理速度并减少手机延迟。</li>
<li><strong>FP4量化方法</strong>：称为M0E4，减少dequantize开销，使得矩阵乘法更高效。</li>
<li><strong>基于子张量的技巧</strong>：避免了在LLM推理后复制KV缓存的内存压力。</li>
</ul>
<p>作者团队还实现了一个名为Transformer-Lite的移动推理引擎，该引擎与高通和联发科处理器兼容，并通过一系列实验评估了其性能。通过这些技术，Transformer-Lite引擎在prefill和decoding方面相比基于CPU的FastLLM和基于GPU的MLC-LLM取得了显著的速度提升。</p>
<h1 id="2-方法">2. 方法</h1>
<h2 id="21-对于动态形状推理的符号表达方法">2.1 对于动态形状推理的符号表达方法</h2>
<p>不同于多数静态尺寸的CV模型，LLM 是一种动态形状输入的场景，每次迭代的输入形状都会发生变化。这导致模型中一些激活张量的形状发生变化，给<strong>内存重用</strong>和<strong>算子性能优化</strong>带来了巨大挑战。
例如，将形状[“sumN-N”,1,2,128]与axis 0上的[“N”,1,2,128]连接，输出形状为[“sumN”,1,2,128]。为此，作者团队利用类似于 Nimble 和 DISC 的方法，将深度学习模型中的算子分为两类：张量计算算子和形状计算算子。后一类算子包含形状算子和那些其输入取决于形状算子结果的算子。形状计算算子在 CPU 上执行，计算形状信息。相反，负责计算激活的张量和权重的张量的计算算子则在 GPU 上执行。</p>
<div align="center"> 
<img src="/keep-moving-forward/imgs/transformer_lite/shape-infer.png" width = "80%"/>
<br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Fig 1. 形状推理</div>
</div>
<p>作者团队使用了SymPy包且结合了ONNX来实现了这一能力。</p>
<p>对于内存复用，实现内存复用的先决条件是获得张量之间的内存大小关系，这在静态形状推理中很简单。符号表达式便于轻松确定这种关系。首先，计算每个张量的内存大小，即元素数乘以数据类型的字节数，从而为每个张量的内存大小创建一个符号表达式。**然后结合减法和除法来辨别张量之间的内存大小关系。**例如，用 “N ”*4096 除以 “N ”<em>32</em>128，得到的整数为 1，表示大小相等，可以重复使用内存。另一方面，当 “N” * 4096 与 “sumN” * 2 * 128 相比较时，减法和除法都会产生另一个符号表达式，而不是一个确定的整数。因此，除非探索所有潜在的输入形状，否则它们之间的大小关系是不确定的，这意味着无法进行内存重用。
作者团队使用了OpenCL工具来实现了内存复用。</p>
<p>通常，LLM的每次推理迭代都需要更新输入形状。在形状更新过程中，需要重新计算动态张量的形状，并更新具有动态形状输入或输出张量的算子参数。因此，频繁的输入形状更新通常会导致不可忽略的性能开销。为了避免这个问题，作者团队采用了Mask机制，在解码阶段将模型输入序列长度填充为64或128的倍数。这种方法可以在实际计数超过填充长度时更新形状，从而使每次推理迭代的形状更新时间开销可以忽略不计。</p>
<h2 id="22-算子优化和执行优先级设置">2.2 算子优化和执行优先级设置</h2>
<p>作者实现了半精度和4bit量化的混合精度矩阵乘法。此外，prefill和decoding阶段对矩阵乘法的形状是不同的。在prefill阶段，矩阵乘法进行矩阵-矩阵计算，而在decoding阶段，这些运算符被简化为向量-矩阵计算。作者对这两种方式的乘法都做了优化，文中没有涉及到算子优化的细节。</p>
<p>GPU在执行LLM时还需要负责用户界面渲染，LLM的执行可能会造成界面渲染延迟。为了解决这个问题，作者利用高通和ARM提供的OpenCL扩展，将深度学习模型算子的执行优先级设置为最低级别。这种方法有效地缓解了移动设备的渲染延迟。</p>
<h2 id="23-fp4量化方法m0e4">2.3 FP4量化方法：M0E4</h2>
<p>在传统的量化方法中，如GPTQ和AWQ，模型权重从浮点数被量化为4位整数，以减少模型大小和内存带宽需求。然而，为了保持高精度，计算的时候通常使用半精度进行。在模型推理时，需要将量化的权重dequantize为浮点数以执行矩阵乘法，这个过程涉及到将整数转换为浮点数，会引起显著的性能开销。<strong>在将整数转换为浮点数的过程中需要使用复杂的算法或指令来实现。因此，dequantize会引发不可忽略的性能开销。</strong></p>
<p>M0E4中的&quot;M0&quot;表示指数部分使用0位。这意味着指数部分被设置为一个常量值，通常是用于表示半精度浮点数的最小正指数。&ldquo;E4&quot;表示尾数部分使用4位。这些4位用于存储量化的尾数信息，这与浮点数表示中的尾数（或称为小数部分）相对应。</p>
<p>作者采用group-wise量化，并通过缩放和偏置系数将每个group的张量(表示为w0)转换为一个新的浮点张量(表示为w1)。并且确保所有w1元素都在$[2^n, 2^{n+1} - eps]$的范围内。$n$的取值为1，2。在这个范围内，就可以保证这些元素的指数部分是一致的。</p>
<p>为了提高量化的准确性，作者为舍入分离了一个额外的位(表示为y2，如图所示):y1 = min (y1+ y2,15)。这里，15是一个4位无符号整数所能表示的最大值。</p>
<div align="center"> 
<img src="/keep-moving-forward/imgs/transformer_lite/quant.png" width = "80%"/>
<br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Fig 2. M0E4 量化</div>
</div>
<p>在dequantize时候，用如下公式即可</p>
<p>$$
\text{w1_half} = \text{as_half} (\text{CONST_EXP_BIN_PART} | (\text{w1_fp4} \langle \langle \text{CONST_BIT_SHIFT_NUM}))
$$</p>
<h2 id="24-基于subtensor来缓解kv-cache压力">2.4 基于subtensor来缓解KV Cache压力</h2>
<p>一张图就可以说明，结合SWA应该可以工作的不错。</p>
<div align="center"> 
<img src="/keep-moving-forward/imgs/transformer_lite/kvcache.png" width = "80%"/>
<br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Fig 3. KV Cache 优化</div>
</div>
<p><strong>发现这个做法不如vLLM种的Page Attention来的统一、简洁。</strong></p>
<h1 id="3-实验">3. 实验</h1>
<h2 id="31-各个llm上的性能">3.1 各个LLM上的性能</h2>
<div align="center"> 
<img src="/keep-moving-forward/imgs/transformer_lite/cp1.png" width = "80%"/>
<br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Fig 4. 各个LLM上的性能</div>
</div>
<h2 id="32-和fastllm和mlc-llm比较">3.2 和FastLLM和MLC-LLM比较</h2>
<div align="center"> 
<img src="/keep-moving-forward/imgs/transformer_lite/cp2.png" width = "80%"/>
<br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Fig 5. 和MLC-LLM比较</div>
</div>
<div align="center"> 
<img src="/keep-moving-forward/imgs/transformer_lite/cp3.png" width = "80%"/>
<br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Fig 6. 和FastLLM比较</div>
</div>
		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/keep-moving-forward/tags/llm-server/" rel="tag">LLM Server</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/keep-moving-forward/tags/edge/" rel="tag">Edge</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/keep-moving-forward/tags/llm/" rel="tag">LLM</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>

<div class="authorbox clearfix">
	<figure class="authorbox__avatar">
		<img alt="chenghua.wang avatar" src="/keep-moving-forward/img/Cornell_box.png" class="avatar" height="90" width="90">
	</figure>
	<div class="authorbox__header">
		<span class="authorbox__name">About chenghua.wang</span>
	</div>
	<div class="authorbox__description">
		Currently working on AI&amp;Sys, CV (low-level) and LLM topics.
	</div>
</div>

<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/keep-moving-forward/papers/awq/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">✅[April 2024] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/keep-moving-forward/papers/prompt_cache/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">✅[April 2024] Prompt Cache: Modular Attention Reuse for Low-Latency Inference</p>
		</a>
	</div>
</nav>


			</div>
			<aside class="sidebar"><div class="widget-search widget">
	<form class="widget-search__form" role="search" method="get" action="https://google.com/search">
		<label>
			<input class="widget-search__field" type="search" placeholder="SEARCH…" value="" name="q" aria-label="SEARCH…">
		</label>
		<input class="widget-search__submit" type="submit" value="Search">
		<input type="hidden" name="don&#39;t use this search" value="don&#39;t use this searchhttps://chenghuawang.github.io/keep-moving-forward/">
	</form>
</div>
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/tech/mllm-qwen/">mllm框架浅析-以QWen0.5B为例</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/papers/mlsys2024-qmoe/">✅[Oct 2023] QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/papers/prompt_cache/">✅[April 2024] Prompt Cache: Modular Attention Reuse for Low-Latency Inference</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/papers/transformer-lite/">✅[Mar 2024] Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/papers/awq/">✅[April 2024] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/tech/x86_avx_sgemm_6x16/">【施工中】6xKx16 SGEMM Kernel on X86-AVX</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/tech/introduction_mldistri/">浅析机器学习中的并行模型和自动并行方法</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/tech/cuda_nsight_system/">CUDA: NSight System</a></li>
		</ul>
	</div>
</div>
<div class="widget-taglist widget">
	<h4 class="widget__title">Tags</h4>
	<div class="widget__content">
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/cuda/" title="CUDA">CUDA (1)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/distributed-system/" title="Distributed System">Distributed System (1)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/edge/" title="Edge">Edge (1)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/kernel/" title="Kernel">Kernel (1)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/lecture/" title="Lecture">Lecture (5)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/llm/" title="LLM">LLM (5)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/llm-cache-optimize/" title="LLM Cache Optimize">LLM Cache Optimize (1)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/llm-server/" title="LLM Server">LLM Server (5)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/mlsys-2024/" title="MLSys 2024">MLSys 2024 (3)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/quantization/" title="Quantization">Quantization (2)</a>
	</div>
</div>
<div class="toc__block_div">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
		<nav id="TableOfContents">
  <ul>
    <li><a href="#1-背景--动机">1. 背景 &amp; 动机</a></li>
    <li><a href="#2-方法">2. 方法</a>
      <ul>
        <li><a href="#21-对于动态形状推理的符号表达方法">2.1 对于动态形状推理的符号表达方法</a></li>
        <li><a href="#22-算子优化和执行优先级设置">2.2 算子优化和执行优先级设置</a></li>
        <li><a href="#23-fp4量化方法m0e4">2.3 FP4量化方法：M0E4</a></li>
        <li><a href="#24-基于subtensor来缓解kv-cache压力">2.4 基于subtensor来缓解KV Cache压力</a></li>
      </ul>
    </li>
    <li><a href="#3-实验">3. 实验</a>
      <ul>
        <li><a href="#31-各个llm上的性能">3.1 各个LLM上的性能</a></li>
        <li><a href="#32-和fastllm和mlc-llm比较">3.2 和FastLLM和MLC-LLM比较</a></li>
      </ul>
    </li>
  </ul>
</nav>
	</div>
</div>

</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 chenghua.wang.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/keep-moving-forward/js/menu.js"></script>




<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { fonts: ["TeX"] }
  });
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async>
</script>
</body>
</html>