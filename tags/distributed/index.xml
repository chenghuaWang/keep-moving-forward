<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Distributed on Ubios Home</title>
    <link>http://localhost:1313/keep-moving-forward/tags/distributed/</link>
    <description>Recent content in Distributed on Ubios Home</description>
    <generator>Hugo -- 0.131.0</generator>
    <language>en</language>
    <copyright>chenghua.wang</copyright>
    <lastBuildDate>Mon, 26 Aug 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/keep-moving-forward/tags/distributed/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[Fundamental] 分布式训练基础</title>
      <link>http://localhost:1313/keep-moving-forward/tech/fundamental_distributed_training/</link>
      <pubDate>Mon, 26 Aug 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/keep-moving-forward/tech/fundamental_distributed_training/</guid>
      <description>0x00 前沿和阅读材料 本文是对分布式训练的复习记录，是对浅析机器学习中的并行模型和自动并行方法这篇文章的补充和进一步的梳理。本文的行文是参考MIT 6.5940: TinyML and Efficient Deep Learning Computing的slides来进行的。
MIT 6.5940: TinyML and Efficient Deep Learning Computing 0x01 分布式训练中的并行方法 数据并行（Data Parallelism） Fig 1. 数据并行Figures credit from MIT 6.5940: TinyML and Efficient Deep Learning Computing
将数据分割成几份，每一份分配给不同的GPU进行计算。每个GPU共享相同的ML Model，在一次Backward之后，通过Reduce操作合并每个GPU上的梯度。典型的算法就是李沐老师的Parameter Server，在下文中会提及。
流水线并行（Data Parallelism） 张量并行（Tensor Parallelism） 0x0x LLM 中的分布式训练方法 </description>
    </item>
  </channel>
</rss>
