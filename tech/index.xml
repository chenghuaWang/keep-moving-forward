<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Technique on Ubios Home</title>
    <link>https://chenghuawang.github.io/keep-moving-forward/tech/</link>
    <description>Recent content in Technique on Ubios Home</description>
    <generator>Hugo -- 0.131.0</generator>
    <language>en</language>
    <copyright>chenghua.wang</copyright>
    <lastBuildDate>Tue, 17 Sep 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://chenghuawang.github.io/keep-moving-forward/tech/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Q8_0 @ Q4_0_4 GEMM/GEMV in llama.cpp</title>
      <link>https://chenghuawang.github.io/keep-moving-forward/tech/q8_0_q4_0_4_gemm_in_llamacpp/</link>
      <pubDate>Tue, 17 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://chenghuawang.github.io/keep-moving-forward/tech/q8_0_q4_0_4_gemm_in_llamacpp/</guid>
      <description>GEMM/GEMV in MLLM Slides
在本文中我以mllm的实现为例。mllm中的大部分混合精度的矩阵乘法是从llama.cpp中更改过来的。我们先来看下Q8_0和Q4_0代表什么。Huggingface的Doc中给出了一张表，大家可以去看一下：GGUF Quantization Type，我在这里也截图给出来
Fig 1. Q8_0和Q4_0含义GGUF Quantization Type FROM Huggingface
对于量化操作不是很熟悉的读者可以看下我之前的blog: [Fundamental] 模型量化
在mllm中，Q8_0和Q4_0的实现是这样的：
typedef struct { mllm_fp16_t d; // delta int8_t qs[QK8_0]; // quants QK8_0 = 32 } block_q8_0; // QK4_0 = 32 typedef struct { mllm_fp16_t d; // delta uint8_t qs[QK4_0 / 2]; // nibbles / quants } block_q4_0; 而Q4_0x4实际上就是将4个Q4_0打包成一组，这样在GEMM的时候可以利用起指令并行性。
我们首先来看下GEMV问题定义，然后再推广到GEMM上。我们有两个矩阵，分别是A($1 \times nr$), B($nc \times nr$)，矩阵乘法后的结果是C$1 \times nc$。一个不是非常恰当的图例如下图所示：
Fig 2. Q8_0和Q4_0x4的GEMV 为了更好的理解怎么分块，我们先来看下Q4_0x4的数据排布是怎么样的：Q4_0x4实际上是在$nc$的方向上以4分块，在$nr$的方向上以32分块，最终得到的block形状如下图所示：
Fig 3. Q8_0和Q4_0x4的GEMV Tiled 我们在$nc$的方向上以4分块，在$nr$的方向上以32分块，将gemv拆解成一个更小的子问题。</description>
    </item>
    <item>
      <title>[Fundamental] FlashDecoding Series</title>
      <link>https://chenghuawang.github.io/keep-moving-forward/tech/fundamental_flashdecoding_series/</link>
      <pubDate>Wed, 21 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://chenghuawang.github.io/keep-moving-forward/tech/fundamental_flashdecoding_series/</guid>
      <description>0x00 前沿和阅读材料 FlashDecoding系列的文章是对FA在推理场景下的改进，目前包含两篇文章：
Flash-Decoding for long-context inference, Torch团队的Blog FlashDecoding++: Faster Large Language Model Inference on GPUs 我们知道，在FA2中特别对Seq方向做了并行化，但是在推理的时候Seq=1。此时，并不能占用满GPU的全部的SM，导致性能损失，FlashDecoding就是对此的优化。
0x01 FlashDecoding 在解码过程中，生成的每个新Token都需要考虑之前的所有Token，以便进行注意力计算。
在训练的时候，Attention这一算子已使用FlashAttentionV2算法进行了优化，其瓶颈在于读写中间结果（例如 Q @ K^T）的内存带宽不足。但是，这些优化并不能直接应用于推理，因为推理的时候不在是内存带宽的瓶颈。在训练过程中，FlashAttention 会在Batch和Seq两个维度上进行并行处理。在推理过程中，Seq=1，这意味着，如果Batch大小小于 GPU 上的SM数量（A100 为 108），则这个Attention操作只会使用 GPU 的一小部分！在使用长上下文时尤其如此。当Batch大小为 1 时，FlashAttention 将使用不到 1%的 GPU！
Fig 1. Regular AttentionFlash-Decoding for long-context inference
为此我们不难想到可以实用Split-K的方法来使得Attention在推理的时候也有很好的并行性。如下图所示：
Fig 1. Split-K AttentionFlash-Decoding for long-context inference
非常的好理解，但是这里需要注意的是，在最后的Reduce Op这里还是要做Online Softmax的，所以在SRAM里面保存的东西是比原来多的，除了Output，还有exp Sum和Max。
0x02 FlashDecoding++ flashdecoding++不是meta官方出品的。
FA中，求解Max需要遍历迭代，之后的子块依赖于之前的子块。Safe-softmax的计算公式中，需要先求每行x的最大值，然后减去这个max(x)之后，再做softmax以防止数值溢出。FlashDecoding++提出的创新点就是，我们可以实用一个先验的$\phi$来作为max值，只要它能让数值稳定就可以了。从Safe Softmax的公式上来看，无论是$\phi$还是max(x)，他们的结果是一致的，我们需要追求的是数值上的稳定与否。
FlashDecoding++认为一个合理的先验值 $\phi$，可以直接从数据集中进行统计获得。对于不同的模型，这个先验值也是不一样的。在实现的时候，FlashDecoding++还使用了Fallback的思路，当出现数值溢出的时候，使用传统的FlashDecoding。
那为什么FalshDecoding++能异步，而FlashDecoding不行呢？
在FalshDecoding Split-K分成的几个区间内，还是使用的FA2的方法来计算，但是FA2的一次迭代是依赖于上一次迭代的结果的，也就是需要rescale。但是FlashDecoding++不需要，它大致上是这样的：
$$\begin{aligned} &amp;\ell^{(1)}=\mathrm{rowsum}\Big(e^{\mathbf{S}^{(1)}-\phi}\Big)\in\mathbb{R}^{B_{r}} \\ &amp;\tilde{\mathbf{O}}^{(1)}=e^{\mathbf{S}^{(1)}-\phi}\mathbf{V}^{(1)}\in\mathbb{R}^{B_{r}\times d} \\ &amp;\ell^{(2)}=\mathrm{rowsum}\left(e^{\mathbf{S}^{(2)}-\phi}\right) \\ &amp;\tilde{\mathbf{O}}^{(2)}=e^{s^{(2)}-\phi}\mathbf{V}^{(2)} \\ &amp;\mathbf{O}^{(2)}=\mathrm{diag}\left(\ell^{(1)}+\ell^{(2)}\right)^{-1}(\tilde{\mathbf{O}}^{(1)}+\tilde{\mathbf{O}}^{(2)})=\mathbf{O} \end{aligned}$$ </description>
    </item>
    <item>
      <title>[Fundamental] 模型量化</title>
      <link>https://chenghuawang.github.io/keep-moving-forward/tech/fundamental_quantization/</link>
      <pubDate>Mon, 19 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://chenghuawang.github.io/keep-moving-forward/tech/fundamental_quantization/</guid>
      <description>RoPE from Fundamental Series</description>
    </item>
    <item>
      <title>mllm框架浅析(二)-QNN-Backend</title>
      <link>https://chenghuawang.github.io/keep-moving-forward/tech/mllm-qwen-2/</link>
      <pubDate>Tue, 13 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://chenghuawang.github.io/keep-moving-forward/tech/mllm-qwen-2/</guid>
      <description>以Qwen0.5B为例解析mllm的基本实现，NPU Backend</description>
    </item>
    <item>
      <title>[Fundamental] 旋转位置编码(RoPE)</title>
      <link>https://chenghuawang.github.io/keep-moving-forward/tech/fundamental_rope/</link>
      <pubDate>Sun, 11 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://chenghuawang.github.io/keep-moving-forward/tech/fundamental_rope/</guid>
      <description>RoPE from Fundamental Series</description>
    </item>
    <item>
      <title>[Fundamental] From Online Softmax to Flash Attention V3</title>
      <link>https://chenghuawang.github.io/keep-moving-forward/tech/fundamental_from_online_softmax_to_flash_attentionv3/</link>
      <pubDate>Sat, 10 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://chenghuawang.github.io/keep-moving-forward/tech/fundamental_from_online_softmax_to_flash_attentionv3/</guid>
      <description>Flash Attention from Fundamental Series</description>
    </item>
    <item>
      <title>mllm框架浅析(一)-以QWen0.5B为例</title>
      <link>https://chenghuawang.github.io/keep-moving-forward/tech/mllm-qwen/</link>
      <pubDate>Fri, 28 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://chenghuawang.github.io/keep-moving-forward/tech/mllm-qwen/</guid>
      <description>以Qwen0.5B为例解析mllm的基本实现，CPU Backend</description>
    </item>
    <item>
      <title>【施工中】6xKx16 SGEMM Kernel on X86-AVX</title>
      <link>https://chenghuawang.github.io/keep-moving-forward/tech/x86_avx_sgemm_6x16/</link>
      <pubDate>Fri, 16 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://chenghuawang.github.io/keep-moving-forward/tech/x86_avx_sgemm_6x16/</guid>
      <description>How to impl high performance 6xKx16 micro kernel</description>
    </item>
    <item>
      <title>浅析机器学习中的并行模型和自动并行方法</title>
      <link>https://chenghuawang.github.io/keep-moving-forward/tech/introduction_mldistri/</link>
      <pubDate>Tue, 02 May 2023 00:00:00 +0000</pubDate>
      <guid>https://chenghuawang.github.io/keep-moving-forward/tech/introduction_mldistri/</guid>
      <description>数据并行、模型并行、流水并行、专家混合</description>
    </item>
    <item>
      <title>CUDA: NSight System</title>
      <link>https://chenghuawang.github.io/keep-moving-forward/tech/cuda_nsight_system/</link>
      <pubDate>Tue, 18 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://chenghuawang.github.io/keep-moving-forward/tech/cuda_nsight_system/</guid>
      <description>Usage of Nsight System</description>
    </item>
    <item>
      <title>XV6 Lab 5: Copy On Write</title>
      <link>https://chenghuawang.github.io/keep-moving-forward/tech/xv6_lab5_cow/</link>
      <pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://chenghuawang.github.io/keep-moving-forward/tech/xv6_lab5_cow/</guid>
      <description>xv6 中的 fork() 系统调用将父进程的所有用户空间内存复制到子进程中。如果父进程很大，复制可能需要很长的时间。更糟糕的是，这项工作通常是无用的：fork() 之后通常是子进程的 exec()，它放弃了复制的内存，没有使用复制过来的大部分内存。如果父代和子代都使用了复制的页面，并且其中一个或两个都写了它，那么这个复制才是真正需要的。
所以写时复制(copy on write, cow) 这个技术变得十分的重要。在这个 lab 中，我们需要实现一个写时复制的 fork()。我们需要修改原本的 fork() 程序中做内存申请的模块，同时我们还需要实现 usertrap 来在写时(在实际写的时候碰到 cow flag，抛出分页错误)的时候处理这个 trap。
因为一个程序很可能被 fork() 了很多的分支，所以我们需要一个计数器来确定这个 page 是要被释放还是保留。
一般来说，一个正常运行的程序在写时复制的时候会有下面几个流程:
fork() 出一个子进程 child。 child 拥有父进程的页的引用，并且页的 flag 有 cow 标识。 并且要把 页 引用 ++ child 需要向自己的页中写入新的数据. 此时需要重新分配内存，页引用 &amp;ndash;。 当 child 返回的时候，可能有内存需要由 kernel 转换到 user。 当父进程销毁的时候，如果计数器为 0，则销毁，反之，不变。 每一页都需要一个计数器来进行计数，所以我们需要在 kernel 中加入一个数组来记录，查阅 xv6 book，我们发现有如下的图:
Fig 1. memlayoutXV6 手册
我们需要在 kernel data 之后的区域内申请一块内存来作为计数器存储的数组。我们可以在 kalloc.c 中实现。
// ./kernel/kalloc.c struct { struct spinlock lock; struct run *freelist; // lab 5 uint* ref_cnt; struct spinlock ref_cnt_lock; char * pa_start; } kmem; 在这个 kmem 结构体中加入了一个 ref_cnt_lock 来保证计数器的正确引用。一个 ref_cnt pointer 来指示 ref array 的起始位置，使用 pa_start 来表示实际的 free memory 的起始位置。我们还需要修改初始化程序来正确的申请计数器数组，并且对 free memory 填充上一个初始值。</description>
    </item>
    <item>
      <title>XV6 Lab 4: Traps</title>
      <link>https://chenghuawang.github.io/keep-moving-forward/tech/xv6_lab4_trap/</link>
      <pubDate>Sun, 05 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://chenghuawang.github.io/keep-moving-forward/tech/xv6_lab4_trap/</guid>
      <description>RISC-V assembly Which registers contain arguments to functions? For example, which register holds 13 in main&amp;rsquo;s call to printf?
Register: a0, a1, a2&amp;hellip;, a7 for integer arguments. Register fa0, fa1, fa2&amp;hellip;, fa7 for float arguments.
Register a2 holds 13 when we call printf().
Where is the call to function f in the assembly code for main? Where is the call to g? (Hint: the compiler may inline functions.)
Compiler inlined f(8) and g() in printf() function.</description>
    </item>
    <item>
      <title>XV6 Lab 3: Page Table</title>
      <link>https://chenghuawang.github.io/keep-moving-forward/tech/xv6_lab3_pagetable/</link>
      <pubDate>Thu, 02 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://chenghuawang.github.io/keep-moving-forward/tech/xv6_lab3_pagetable/</guid>
      <description>MIT 6.S081 Lab3 website
做 Lab 3 需要提前阅读 XV6 book，了解 RISC-V SR39 的地址格式，并且实验中大量用到了页表的准换函数，需要查阅 XV6 手册。不过，熟记 RISC-V 的地址说实在的没有什么用处，通过这个实验理解页表的工作方式并且 hands on 才是真的。
Speed up system calls 目前有很多的操作系统(Linux)在用户区和内核区之间共享一块数据(Read-Only for user)，这样用户在进行系统调用的时候就不需要陷入内核态后，由内核态拷贝数据进用户态，而是将数据写在这个共享的区块内。这样可以加快操作系统的运行速度(毕竟大部分系统调用需要的内存消耗是很小的，内存的消耗在当今已经不是问题)。
在本实验中我们需要使用 ugetpid() 来进行加速获得进程的 pid。
首先就是为每一个进程创建一个页表作为共享内存区块。我们发现在 kernel\memlayout.h 中已经为我们定义好了需要的数据结构:
struct usyscall { int pid; // Process ID }; 那么我们只需要在 kernel/proc.c /proc.h 中加入代码，来实现进程创建时创建页表，销毁时销毁页表的动作就行了。
在 proc.h 中，我们需要在进程的 PCB 中加入新的数据结构:
struct proc { ... struct usyscall *usyscall; // using read only shared data to accelerate. ... } 为了让进程的正常创建和释放，我们需要向进程创建和销毁函数中加入对应的页表操作代码。</description>
    </item>
    <item>
      <title>XV6 Lab 2: syscall</title>
      <link>https://chenghuawang.github.io/keep-moving-forward/tech/xv6_lab2_syscall/</link>
      <pubDate>Sat, 25 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://chenghuawang.github.io/keep-moving-forward/tech/xv6_lab2_syscall/</guid>
      <description>MIT 6.S081 Lab2 website
为了完成 Syscall 作业，需要阅读:
XV6-book, Chapter 2, Sections 4.3 and 4.4
files: user/user.h, kernel/proc.c kernel/proc.h, kernel/syscall.c kernel/syscall.h
system call tracing system call tracing 需要我们补充 kernel 中的一些程序，将某程序中指定的 system call 打印出来。当然，这需要我们新增一个 system_trace 系统调用函数。题中，给定的 tracing 程序以 trace [system-call-number] [cmd] 的方式运行。我们首先去看 user/trace.c 中的内容，看看 system call number 是怎么传入系统调用的。
user/trace.c 的程序如下所示:
int main(int argc, char *argv[]) { int i; char *nargv[MAXARG]; if(argc &amp;lt; 3 || (argv[1][0] &amp;lt; &amp;#39;0&amp;#39; || argv[1][0] &amp;gt; &amp;#39;9&amp;#39;)){ fprintf(2, &amp;#34;Usage: %s mask command\n&amp;#34;, argv[0]); exit(1); } if (trace(atoi(argv[1])) &amp;lt; 0) { fprintf(2, &amp;#34;%s: trace failed\n&amp;#34;, argv[0]); exit(1); } for(i = 2; i &amp;lt; argc &amp;amp;&amp;amp; i &amp;lt; MAXARG; i++){ nargv[i-2] = argv[i]; } exec(nargv[0], nargv); exit(0); } 我们发现这个程序直接使用了 trace 系统调用来实现。所以接下来的任务是进行 trace 系统调用的实现。再次回顾 Lab 1 中的内容，在 Lab 1 中我们分析系统调用是通过在 usys.</description>
    </item>
    <item>
      <title>XV6 Lab 1: Xv6 and Unix utilities</title>
      <link>https://chenghuawang.github.io/keep-moving-forward/tech/xv6_lab1_utility/</link>
      <pubDate>Thu, 23 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://chenghuawang.github.io/keep-moving-forward/tech/xv6_lab1_utility/</guid>
      <description>MIT 6.S081 Lab1 website
本章节的实验是为了熟悉 XV6 环境和 interface 而准备的。包含了 interface 调用、多进程编程、Pipeline。
阅读 book-riscv-ref3 熟悉 XV6 的系统组织形式
sleep 通过调用 user.h 中的系统调用函数来完成 sleep 程序。主要是为了介绍 系统调用 的工作方式。
/** * @author chenghua.wang * @brief Lab1-utilities. sleep prog using syscall. * @time Feb 23, 2023 * */ #include &amp;#34;kernel/types.h&amp;#34; #include &amp;#34;kernel/stat.h&amp;#34; #include &amp;#34;user/user.h&amp;#34; int main(int argc, char *argv[]){ if (argc != 2){ printf(&amp;#34;[ error ] you should follow anw integer with sleep prog to indicate ticks times!</description>
    </item>
  </channel>
</rss>
