<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>浅析机器学习中的并行模型和自动并行方法 - Ubios Home</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="浅析机器学习中的并行模型和自动并行方法" />
<meta property="og:description" content="人工智能领域的许多最新进展都围绕着大规模神经网络展开，但训练大规模神经网络是一项艰巨的工程和研究挑战，需要协调GPU集群来执行单个同步计算。随着集群数和模型规模的增长，机器学习从业者开发了多项技术，让机器学习模型能在多个GPU上进行并行模型训练。
乍一看，这些并行技术令人生畏，但只需对计算结构进行一些假设，这些技术就会变得清晰：从某些角度来看，这也只是从 A 到 B 传递并不透明的位，就像数据包在网络交换机之间传递一样。
各种 Parallel 模型 不同的并行技术将训练过程划分为不同的维度，包括：
数据并行（Data Parallelism）在不同的GPU上运行同一批数据的不同子集 DP（Data Parallel） DDP（Distributed Data Parallel） FSDP（Fully Shared Data Parallel） 流水并行（Pipeline Parallelism）在不同的GPU上运行模型的不同层 模型并行（Tensor Parallelism）将单个数学运算（如矩阵乘法）拆分到不同的GPU上运行 专家混合（Mixture-of-Experts）只用模型每一层中的一小部分来处理数据。 Note：Tensor Parallelism 翻译成模型并行可能并不是非常的恰当🤣
1. 并行模型 1.1 数据并行 （Data Parallesim） 数据并行是指将相同的参数复制到多个工作节点上，并为每个工作节点分配不同的数据子集同时进行处理。每个工作节点拥有完整的神经网络模型，每次训练仅将一批数据输入模型，进行前向传播、计算误差、反向传播，最后进行参数的更新。储了参数的更新，其余的操作都是互相独立的，所以可以在多个节点上进行并发的执行。
每个工作节点都有自己的模型和输入，当属于自己的模型参数推理完成后（产生了梯度参数），所有的工作节点会把参数（梯度参数）发给一个 Master，这个 Master 会把所有节点传进来的参数做融合，通过这些梯度参数更新生成新的模型参数，然后把这个模型的参数再发送给每个工作节点，拱他们进行下一轮的计算。
在 Pytorch 中提供了DP（Data Parallel）、DDP（Distributed Data Parallel）、FSDP（Fully Shared Data Parallel）三种不同的数据并行方法。
1.1.1 DP（Data Parallel）Parameter Server DP 使用了 Parameter Server（PS） 作为理论依据。PS 结构是李沐老师提出来的方法，由server节点和worker节点组成。
[点击折叠] 李沐老师的 PS 讲解 Server 节点的主要功能是初始化和保存模型参数、接受worker节点计算出的局部梯度、汇总计算全局梯度，并更新模型参数。
Parameter Server Worker 节点的主要功能是各自保存部分训练数据，初始化模型，从 Server 节点拉取（Pull）最新的模型参数，再读取参数，根据训练数据计算局部梯度，上传（Push）给 Server 节点。ps：李沐老师说这个 Pull 和 Push 叫法来源于 Git。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chenghuawang.github.io/keep-moving-forward/tech/introduction_mldistri/" /><meta property="article:section" content="Tech" />
<meta property="article:published_time" content="2023-05-02T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-05-02T00:00:00+00:00" />

		<meta itemprop="name" content="浅析机器学习中的并行模型和自动并行方法">
<meta itemprop="description" content="人工智能领域的许多最新进展都围绕着大规模神经网络展开，但训练大规模神经网络是一项艰巨的工程和研究挑战，需要协调GPU集群来执行单个同步计算。随着集群数和模型规模的增长，机器学习从业者开发了多项技术，让机器学习模型能在多个GPU上进行并行模型训练。
乍一看，这些并行技术令人生畏，但只需对计算结构进行一些假设，这些技术就会变得清晰：从某些角度来看，这也只是从 A 到 B 传递并不透明的位，就像数据包在网络交换机之间传递一样。
各种 Parallel 模型 不同的并行技术将训练过程划分为不同的维度，包括：
数据并行（Data Parallelism）在不同的GPU上运行同一批数据的不同子集 DP（Data Parallel） DDP（Distributed Data Parallel） FSDP（Fully Shared Data Parallel） 流水并行（Pipeline Parallelism）在不同的GPU上运行模型的不同层 模型并行（Tensor Parallelism）将单个数学运算（如矩阵乘法）拆分到不同的GPU上运行 专家混合（Mixture-of-Experts）只用模型每一层中的一小部分来处理数据。 Note：Tensor Parallelism 翻译成模型并行可能并不是非常的恰当🤣
1. 并行模型 1.1 数据并行 （Data Parallesim） 数据并行是指将相同的参数复制到多个工作节点上，并为每个工作节点分配不同的数据子集同时进行处理。每个工作节点拥有完整的神经网络模型，每次训练仅将一批数据输入模型，进行前向传播、计算误差、反向传播，最后进行参数的更新。储了参数的更新，其余的操作都是互相独立的，所以可以在多个节点上进行并发的执行。
每个工作节点都有自己的模型和输入，当属于自己的模型参数推理完成后（产生了梯度参数），所有的工作节点会把参数（梯度参数）发给一个 Master，这个 Master 会把所有节点传进来的参数做融合，通过这些梯度参数更新生成新的模型参数，然后把这个模型的参数再发送给每个工作节点，拱他们进行下一轮的计算。
在 Pytorch 中提供了DP（Data Parallel）、DDP（Distributed Data Parallel）、FSDP（Fully Shared Data Parallel）三种不同的数据并行方法。
1.1.1 DP（Data Parallel）Parameter Server DP 使用了 Parameter Server（PS） 作为理论依据。PS 结构是李沐老师提出来的方法，由server节点和worker节点组成。
[点击折叠] 李沐老师的 PS 讲解 Server 节点的主要功能是初始化和保存模型参数、接受worker节点计算出的局部梯度、汇总计算全局梯度，并更新模型参数。
Parameter Server Worker 节点的主要功能是各自保存部分训练数据，初始化模型，从 Server 节点拉取（Pull）最新的模型参数，再读取参数，根据训练数据计算局部梯度，上传（Push）给 Server 节点。ps：李沐老师说这个 Pull 和 Push 叫法来源于 Git。"><meta itemprop="datePublished" content="2023-05-02T00:00:00+00:00" />
<meta itemprop="dateModified" content="2023-05-02T00:00:00+00:00" />
<meta itemprop="wordCount" content="482">
<meta itemprop="keywords" content="Distributed System," />
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans+SC:400,700">

	<link rel="stylesheet" href="/keep-moving-forward/css/style.css">
	

	<link rel="shortcut icon" href="/keep-moving-forward/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		
<header class="header">
	<div class="container header__container">
		
	<div class="logo logo--mixed">
		<a class="logo__link" href="/keep-moving-forward/" title="Ubios Home" rel="home">
			<div class="logo__item logo__imagebox">
					<img class="logo__img" src="/keep-moving-forward/img/inspace.gif">
				</div><div class="logo__item logo__text">
					<div class="logo__title">Ubios Home</div>
					<div class="logo__tagline">Whomst has summoned the almighty one?</div>
				</div>
		</a>
	</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/keep-moving-forward/about/about/">
				
				<span class="menu__text">About</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/keep-moving-forward/about/hpc_ai/">
				
				<span class="menu__text">HPC &amp; AI 入坑</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/keep-moving-forward/about/lecture_notes/">
				
				<span class="menu__text">Lecture-Notes</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/keep-moving-forward/about/paper_posts/">
				
				<span class="menu__text">Paper-Notes</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/keep-moving-forward/about/tech_posts/">
				
				<span class="menu__text">Tech-Posts</span>
				
			</a>
		</li>
	</ul>
</nav>

	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			


<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">浅析机器学习中的并行模型和自动并行方法</h1>
			<p class="post__lead">数据并行、模型并行、流水并行、专家混合</p>
			<div class="post__meta meta"><div class="meta__item-author meta__item">
	<svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2 0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class="meta__text">chenghua.wang</span>
</div>
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class="meta__text" datetime="2023-05-02T00:00:00Z">2023-05-02</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg><span class="meta__text"><a class="meta__link" href="/keep-moving-forward/categories/hpc&#43;ai/" rel="category">HPC&#43;AI</a>
	</span>
</div></div>
		</header>

		
<div class="post__toc toc">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
		<nav id="TableOfContents">
  <ul>
    <li><a href="#1-并行模型">1. 并行模型</a>
      <ul>
        <li><a href="#11-数据并行-data-parallesim">1.1 数据并行 （Data Parallesim）</a>
          <ul>
            <li><a href="#111-dpdata-parallelparameter-server">1.1.1 DP（Data Parallel）Parameter Server</a></li>
            <li><a href="#112-ddpdistributed-data-parallelring-all-reduce">1.1.2 DDP（Distributed Data Parallel）Ring-All-Reduce</a></li>
            <li><a href="#113-fsdpfully-shared-data-parallel">1.1.3 FSDP（Fully Shared Data Parallel）</a></li>
          </ul>
        </li>
        <li><a href="#12-流水并行pipeline-parallesim">1.2 流水并行（Pipeline Parallesim）</a></li>
        <li><a href="#13-模型并行tensor-parallesim">1.3 模型并行（Tensor Parallesim）</a></li>
        <li><a href="#14-混合并行mixture-of-experts">1.4 混合并行（Mixture-of-Experts）</a></li>
      </ul>
    </li>
    <li><a href="#2-自动并行方法">2. 自动并行方法</a>
      <ul>
        <li><a href="#21-exploring-hidden-dimensions-in-parallelizing-convolutional-neural-networks-9">2.1 Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks [9]</a></li>
      </ul>
    </li>
  </ul>
</nav>
	</div>
</div><div class="content post__content clearfix">
			<p>人工智能领域的许多最新进展都围绕着大规模神经网络展开，但训练大规模神经网络是一项艰巨的工程和研究挑战，需要协调GPU集群来执行单个同步计算。随着集群数和模型规模的增长，机器学习从业者开发了多项技术，让机器学习模型能在多个GPU上进行并行模型训练。</p>
<p>乍一看，这些并行技术令人生畏，但只需对计算结构进行一些假设，这些技术就会变得清晰：从某些角度来看，这也只是从 A 到 B 传递并不透明的位，就像数据包在网络交换机之间传递一样。</p>
<div align="center"> 
<img src="/keep-moving-forward/imgs/introduction_distributed_ml/Untitled.png" width = "100%"/>
<br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">各种 Parallel 模型</div>
</div>
<p>不同的并行技术将训练过程划分为不同的维度，包括：</p>
<ul>
<li><strong>数据并行（Data Parallelism）在不同的GPU上运行同一批数据的不同子集</strong>
<ul>
<li>DP（Data Parallel）</li>
<li>DDP（Distributed Data Parallel）</li>
<li>FSDP（Fully Shared Data Parallel）</li>
</ul>
</li>
<li><strong>流水并行（Pipeline Parallelism）在不同的GPU上运行模型的不同层</strong></li>
<li><strong>模型并行（Tensor Parallelism）将单个数学运算（如矩阵乘法）拆分到不同的GPU上运行</strong></li>
<li><strong>专家混合（Mixture-of-Experts）只用模型每一层中的一小部分来处理数据。</strong></li>
</ul>
<p>Note：Tensor Parallelism 翻译成模型并行可能并不是非常的恰当🤣</p>
<h1 id="1-并行模型">1. 并行模型</h1>
<h2 id="11-数据并行-data-parallesim">1.1 数据并行 （Data Parallesim）</h2>
<p>数据并行是指将相同的参数复制到多个工作节点上，并为每个工作节点分配不同的数据子集同时进行处理。每个工作节点拥有完整的神经网络模型，每次训练仅将一批数据输入模型，进行前向传播、计算误差、反向传播，最后进行参数的更新。储了参数的更新，其余的操作都是互相独立的，所以可以在多个节点上进行并发的执行。</p>
<p>每个工作节点都有自己的模型和输入，当属于自己的模型参数推理完成后（产生了梯度参数），所有的工作节点会把参数（梯度参数）发给一个 Master，这个 Master 会把所有节点传进来的参数做融合，通过这些梯度参数更新生成新的模型参数，然后把这个模型的参数再发送给每个工作节点，拱他们进行下一轮的计算。</p>
<p>在 Pytorch 中提供了DP（Data Parallel）、DDP（Distributed Data Parallel）、FSDP（Fully Shared Data Parallel）三种不同的数据并行方法。</p>
<h3 id="111-dpdata-parallelparameter-server">1.1.1 DP（Data Parallel）Parameter Server</h3>
<p>DP 使用了 Parameter Server（PS） 作为理论依据。PS 结构是李沐老师提出来的方法，由server节点和worker节点组成。</p>
<details open><summary>[点击折叠] 李沐老师的 PS 讲解</summary>
<div align="center">
<vedio>
 <source src="">
</vedio>
<iframe src="//player.bilibili.com/player.html?aid=895626974&bvid=BV1YA4y197G8&cid=577032881&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height=480 width=720 autoplay="false"> </iframe>
</div>
</details>
<p>Server 节点的主要功能是初始化和保存模型参数、接受worker节点计算出的局部梯度、汇总计算全局梯度，并更新模型参数。</p>
<div align="center"> 
<img src="/keep-moving-forward/imgs/introduction_distributed_ml/Untitled%201.png" width = "80%"/>
<br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Parameter Server</div>
</div>
<p>Worker 节点的主要功能是各自保存部分训练数据，初始化模型，从 Server 节点拉取（Pull）最新的模型参数，再读取参数，根据训练数据计算局部梯度，上传（Push）给 Server 节点。ps：李沐老师说这个 Pull 和 Push 叫法来源于 Git。</p>
<p>在这个模式下，会有负载不均衡的情况出现。因为在 Server 需要大量的显存来保存从 Woker 到来的梯度参数，而 Server 还需要将更新后的参数广播到每一个 Worker 上，那么 Server 的网络带宽就变得是非重要了。随着 Worker 数目的增多，网络的需求会更加大。并且 Server 作为最终处理所有 Worker 传来的梯度参数的机器，其计算消耗时间也是制约其他 Worker 不间断工作的瓶颈。</p>
<h3 id="112-ddpdistributed-data-parallelring-all-reduce">1.1.2 DDP（Distributed Data Parallel）Ring-All-Reduce</h3>
<p>DDP 主要基于 Ring-All-Reduce 算法，该算法主要分两步：</p>
<ol>
<li>share-reduce：会逐步交换彼此的梯度并融合，最后每个 GPU 都会包含完整融合梯度的一部分。</li>
<li>share-only：GPU 会逐步交换彼此不完整的融合梯度，最后所有 GPU 都会得到完整的融合梯度</li>
</ol>
<div align="center"> 
<img src="/keep-moving-forward/imgs/introduction_distributed_ml/Untitled%202.png" width = "60%"/>
<br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Ring-All-Reduce 示意图</div>
</div>
<p>在 share-reduce 阶段，每个进程 p 将数据发送给进程 (p+1) % p，其中 % 是模运算符。所以进程A会向进程 B 发送。这就是创建类似于环形队列状态的的原因。此外，长度为 n 的数组被划分成 p 块，这些块中的每一个都将以 i 为索引。它看起来会是这样的：</p>
<div align="center"> 
<img src="/keep-moving-forward/imgs/introduction_distributed_ml/Untitled%203.png" width = "60%"/>
<br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Ring-All-Reduce 1</div>
</div>
<p>第一个 share-reduce 操作进程 A 向进程 B 发送 a0，进程 B 将向进程 C 发送 b1，等等。类似这样：</p>
<div align="center"> 
<img src="/keep-moving-forward/imgs/introduction_distributed_ml/Untitled%204.png" width = "60%"/>
<br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Ring-All-Reduce 2</div>
</div>
<p>然后，当每个进程收到前一个进程的数据时，它就会应用 reduce operation，然后继续将其再次发送到环中的下一个进程。如果 reduce operation 是 sum。它看起来就会像这样：</p>
<div align="center"> 
<img src="/keep-moving-forward/imgs/introduction_distributed_ml/Untitled%205.png" width = "60%"/>
<br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Ring-All-Reduce 3</div>
</div>
<p>然后，再进一步：</p>
<div align="center"> 
<img src="/keep-moving-forward/imgs/introduction_distributed_ml/Untitled%206.png" width = "60%"/>
<br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Ring-All-Reduce 4</div>
</div>
<p>第二步，shared-only，这个操作非常的简单，只是以环形方式共享数据，而不应用 reduce operation。这就把每个进程中的每个分块的结果合并起来。</p>
<div align="center"> 
<img src="/keep-moving-forward/imgs/introduction_distributed_ml/Untitled%207.png" width = "60%"/>
<br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Ring-All-Reduce 5</div>
</div>
<p>Ring-All-Reduce 有这些优点：</p>
<ol>
<li>负载分散在每个 Worker 上，通讯时间基本是一致的。并且不需要通过 Server 分发全模型的参数到每个 Worker 上。</li>
<li>使用 ring-all-reduce 的方式进行通讯，随着 Worker 数量 N 增加，总传输量恒定。也就是理论上，随着 Worker 数量的增加，ring all-reduce 有线性性能拓展能力。</li>
</ol>
<blockquote>
<p>DP 和 DDP 之间的区别</p>
<table>
<thead>
<tr>
<th style="text-align:center">Data Parallel</th>
<th style="text-align:center">Distributed Data Parallel</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">更多的开销；模型在每次参数聚合后需要被复制，并在每次前向传递后被销毁（因为只需要梯度参数被传回就够了）。</td>
<td style="text-align:center">模型只会被复制一次</td>
</tr>
<tr>
<td style="text-align:center">只支持单个节点的 Master</td>
<td style="text-align:center">可以拓展到多台机器上</td>
</tr>
<tr>
<td style="text-align:center">单进程多线程的实现方式</td>
<td style="text-align:center">采用多进程的方式</td>
</tr>
</tbody>
</table>
</blockquote>
<h3 id="113-fsdpfully-shared-data-parallel">1.1.3 FSDP（Fully Shared Data Parallel）</h3>
<p>DDP 用起来简单方便，<strong>但是要求整个模型能加载一个GPU上</strong>，这使得大模型的训练需要使用额外复杂的设置进行模型拆分。Pytorch 的 FSDP 从 DeepSpeed ZeRO [8] 以及 FairScale 的 FSDP 中获取灵感，<strong>打破模型分片的障碍</strong>（包括模型参数，梯度，优化器状态），同时仍然保持了数据并行的简单性。</p>
<p>FSDP 是对模型参数、优化器状态和梯度进行分片，可以选择将部分模型参数卸载到 CPU 中。FSDP 是在 DDP 的基础上，将 All-Reduce 操作分解为独立的 Reduce-Scatter 和 All-gather 的操作这种方式能够使得每个工作节点仅需存储一个参数和优化器状态的分片。FSDP 非常类似于 <strong>ZeRO-3</strong>。</p>
<div align="center"> 
<img src="/keep-moving-forward/imgs/introduction_distributed_ml/Untitled%208.png" width = "100%"/>
<br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">FSDP workflow [3]</div>
</div>
<p>如 FSDP 完全流程图所示，通过 All-gather 操作可以从其他工作节点获取模型权重来进行前向传播和反向传播。Reduce-Scatter 操作聚合局部梯度，并在各工作的节点上分片，还可以选择将梯度卸载到CPU 中，等需要时再从 CPU 中加载回来。</p>
<div align="center"> 
<img src="/keep-moving-forward/imgs/introduction_distributed_ml/Untitled%209.png" width = "80%"/>
<br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Reduce-Scatter and All-gather</div>
</div>
<p>原本的 All-Reduce 操作可以拆分成 Reduce-Scatter 和 All-gather 操作。在进行Reduce-Scatter 时，gradients 在相同块的各个 ranks 中被加起来，在进行 All-gather 时，每个 Worker 上可用的聚合梯度分片的部分可供所有 Worker 使用。</p>
<h2 id="12-流水并行pipeline-parallesim">1.2 流水并行（Pipeline Parallesim）</h2>
<p>通过流水线机制进行并行训练，我们将模型的参数分配到GPU上。每个GPU只持有一部分参数，因此，同一个模型在每个GPU上消耗的内存成比例减小。</p>
<p>将大模型分为若干份连续的layer很简单。然而，各层的输入和输出之间存在着顺序上的依赖性，所以一个简单的实现会导致大量的GPU空闲时间，因为 Worker 在等待前一个 Worker 的输出来用作其输入。这些等待时间块被称为 <strong>&ldquo;气泡(bubble)&rdquo;</strong>，而在这些等待的过程中，空闲的机器本可以继续进行计算。。</p>
<div align="center"> 
<img src="/keep-moving-forward/imgs/introduction_distributed_ml/Untitled%2010.png" width = "100%"/>
<br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Naive way for Streaming [1]</div>
</div>
<p>为了减少气泡的开销，我们可以重用数据并行的思想，通过让每个 Worker 一次只处理一个数据元素的子集，使我们能够巧妙地将新的计算与等待时间重叠(overlapping)起来。核心思想是将一个批次分成多个微批(microbatches)；每个微批的处理速度应该是成比例的，每个 Worker 在下一个微批可用时就开始工作，从而加速流的执行。有了足够的微批，Worker 可以在大部分时间内被利用，在进程的开始和结束时，气泡最小。梯度是跨微批的平均数，只有在所有微批完成后才会对参数进行更新。</p>
<p>模型被分割的节点数的数量通常被称为流水线深度(Pipline depth)。</p>
<p>在前向传递期间，Worker 只需要将其 Layer 的输出（称为激活）发送给下一个 Worker；在反向传递期间，它只将这些激活的梯度发送给前一个 Worker 。如何安排这些传递以及如何在微批中聚集梯度，有很大的设计空间。 GPipe让每个工作者连续处理前向和反向的传递，然后在最后同步聚合来自多个微批的梯度。而 PipeDream 则安排每个 Worker 交替地处理前向和反向 Stream。</p>
<div align="center"> 
<img src="/keep-moving-forward/imgs/introduction_distributed_ml/Untitled%2011.png" width = "100%"/>
<br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">GPipe and PipeDream [1]</div>
</div>
<h2 id="13-模型并行tensor-parallesim">1.3 模型并行（Tensor Parallesim）</h2>
<p>流水并行将一个模型按层“垂直”拆分。也可以在一个 layer 内 “水平” 分割某些操作，这通常被称为模型并行训练。对于许多现代模型（如Transformer）来说，计算的瓶颈是将激活值与大的权重矩阵相乘。矩阵乘法可以被认为是成对的行和列之间的点积：有可能在不同的GPU上计算独立的点积，或者在不同的GPU上计算每个点积的一部分，然后将结果相加。无论采用哪种策略，我们都可以将权重矩阵切成大小均匀的 “块(Shards)”，将每个块放在不同的GPU上。要得到完整矩阵的结果，需要进行通信将不同部分的结果进行整合。</p>
<p>一个例子是 <a href="https://nv-adlr.github.io/MegatronLM">Megatron-LM</a>，它在 Transformer 的 自注意 和 MLP层内并行化矩阵乘法。 <a href="https://arxiv.org/abs/2104.04473">PTD-P</a> 同时使用张量、数据和流水线并行；它的流水线并行将多个不连续的 layer 分配到单设备上运行，以更多网络通信为代价来减少气泡开销。</p>
<p>有时，网络的输入可以在一个维度上进行并行化，相对于交叉通信来说，并行计算的程度很高。 <a href="https://arxiv.org/abs/2205.05198">序列并行</a>就是这样一个想法，一个输入序列在不同时间被分割成多个子集，通过在更细粒度的子集上进行计算，峰值内存消耗可以成比例地减少。</p>
<h2 id="14-混合并行mixture-of-experts">1.4 混合并行（Mixture-of-Experts）</h2>
<p><a href="https://arxiv.org/abs/1701.06538">混合专家（MoE）</a>方法，对于任何一个输入，只有一部分网络被用来计算输出。在有许多套权重的情况下，网络可以在推理时通过门控机制选择使用哪一套。这样就可以在不增加计算成本的情况下增加许多参数。每组权重被称为 &ldquo;专家(experts)&quot;，希望网络能够学会将专门的计算任务分配给每个专家。不同的专家可以托管在不同的GPU上，这也为扩大模型使用的GPU数量提供了一个明确的方法。</p>
<div align="center"> 
<img src="/keep-moving-forward/imgs/introduction_distributed_ml/moe.svg" width = "100%"/>
<br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">混合专家（MoE）Illustration of a mixture-of-experts (MoE) layer. Only 2 out of the n experts are selected by the gating network. (Image adapted from: [Shazeer et al., 2017](https://arxiv.org/abs/1701.06538))[1]</div>
</div>
<p><a href="https://arxiv.org/abs/2006.16668">GShard</a>将 MoE Transformer 的规模扩大到6000亿个参数，其中MoE layers被拆分到多个TPU上，其他layers是完全重复的。 <a href="https://arxiv.org/abs/2101.03961">Switch Transformer</a> 通过将一个输入只路由到一个专家，将模型规模扩展到数万亿的参数，甚至有更高的稀疏度。</p>
<blockquote>
<p>难点：</p>
<ul>
<li>现在的设备，比如GPU，比较擅长做运算，不擅长做分支。</li>
<li>大批量是训练模型的必须，但是这种方式下会导致每个小模型的样本数较少，无法训练得到好的模型。</li>
<li>为了控制稀疏性，可能需要在loss上去做些改进，来保障小模型上的均衡负载。</li>
</ul>
<p><a href="https://github.com/davidmrau/mixture-of-experts">Moe Github, code</a></p>
</blockquote>
<h1 id="2-自动并行方法">2. 自动并行方法</h1>
<h2 id="21-exploring-hidden-dimensions-in-parallelizing-convolutional-neural-networks-9">2.1 Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks [9]</h2>
<p><a href="https://link.zhihu.com/?target=https%3A//cs.stanford.edu/~zhihao/papers/icml18.pdf">Get This Paper</a></p>
<p>这篇文章勾勒了自动并行的基本框架，很多解决自动并行的工作都是这样一个流程。</p>
<hr>
<p><strong>Reference:</strong></p>
<p>[1] <a href="https://openai.com/research/techniques-for-training-large-neural-networks">Techniques for training large neural networks (openai.com)</a></p>
<p>[2] <a href="https://zhuanlan.zhihu.com/p/624687216">先进编译实验室-自动并行-并行划分 - 知乎 (zhihu.com)</a></p>
<p>[3] <a href="https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html?highlight=fsdp">Getting Started with Fully Sharded Data Parallel(FSDP)</a></p>
<p>[4] <a href="https://github.com/chenzomi12/DeepLearningSystem">chenzomi12/DeepLearningSystem: Deep Learning System core principles introduction. (github.com)</a></p>
<p>[5] <a href="https://developer.nvidia.com/blog/deep-learning-nutshell-core-concepts/#layer">Deep Learning in a Nutshell: Core Concepts | NVIDIA Technical Blog</a></p>
<p>[6] <a href="https://towardsdatascience.com/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da">Visual intuition on ring-Allreduce for distributed Deep Learning | by Edir Garcia Lazo | Towards Data Science</a></p>
<p>[7] <a href="https://zhuanlan.zhihu.com/p/485208899#:~:text=Pytorch%20%E7%9A%84FSDP%E6%98%AF%E4%B8%80%E7%A7%8D%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E7%9A%84%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%EF%BC%8C,%E5%AE%83%E5%AE%9E%E9%99%85%E4%B8%8A%E5%B0%B1%E6%98%AFZeRO-3%20%EF%BC%8C%E4%BC%A0%E7%BB%9F%E7%9A%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E4%BC%9A%E5%9C%A8%E6%AF%8F%E4%B8%AAGPU%E4%B8%8A%E7%BB%B4%E6%8A%A4%E4%B8%80%E4%BB%BD%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%EF%BC%8C%E6%A2%AF%E5%BA%A6%EF%BC%8C%E4%BC%98%E5%8C%96%E5%99%A8%E7%8A%B6%E6%80%81%E7%9A%84%E5%89%AF%E6%9C%AC%EF%BC%8C%E4%BD%86%E6%98%AFFSDP%E5%B0%86%E8%BF%99%E4%BA%9B%E7%8A%B6%E6%80%81%E5%88%86%E7%89%87%E5%88%B0%E6%89%80%E6%9C%89%E7%9A%84%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8Cworker%E4%B8%AD%EF%BC%8C%20%E5%B9%B6%E4%B8%94%E5%8F%AF%E4%BB%A5%E9%80%89%E6%8B%A9%E5%B0%86%E5%88%86%E7%89%87%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E5%8D%B8%E8%BD%BD%E5%88%B0CPU%E4%B8%8A%E3%80%82">数据并行Deep-dive: 从DP 到 Fully Sharded Data Parallel （FSDP）完全分片数据并行 - 知乎 (zhihu.com)</a></p>
<p>[8] <a href="https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/">DeepSpeed: Extreme-scale model training for everyone - Microsoft Research</a></p>
<p>[9] <a href="https://link.zhihu.com/?target=https%3A//cs.stanford.edu/~zhihao/papers/icml18.pdf">Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks</a></p>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/keep-moving-forward/tags/distributed-system/" rel="tag">Distributed System</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>

<div class="authorbox clearfix">
	<figure class="authorbox__avatar">
		<img alt="chenghua.wang avatar" src="/keep-moving-forward/img/Cornell_box.png" class="avatar" height="90" width="90">
	</figure>
	<div class="authorbox__header">
		<span class="authorbox__name">About chenghua.wang</span>
	</div>
	<div class="authorbox__description">
		Learning and developing Machine learning infrastructure now. Did some works in Computer Vision and Graphics.
	</div>
</div>

<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/keep-moving-forward/tech/how_to_optimize_convlution/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">How to Optimize Convolution</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/keep-moving-forward/tech/intro_to_iree/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">浅析 IREE</p>
		</a>
	</div>
</nav>


			</div>
			<aside class="sidebar"><div class="widget-search widget">
	<form class="widget-search__form" role="search" method="get" action="https://google.com/search">
		<label>
			<input class="widget-search__field" type="search" placeholder="SEARCH…" value="" name="q" aria-label="SEARCH…">
		</label>
		<input class="widget-search__submit" type="submit" value="Search">
		<input type="hidden" name="don&#39;t use this search" value="don&#39;t use this searchhttps://chenghuawang.github.io/keep-moving-forward/">
	</form>
</div>
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/tech/intro_to_iree/">浅析 IREE</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/tech/introduction_mldistri/">浅析机器学习中的并行模型和自动并行方法</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/tech/how_to_optimize_convlution/">How to Optimize Convolution</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/tech/cuda_nsight_system/">CUDA: NSight System</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/about/hpc_ai/">HPC &amp; AI 入坑</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/tech/xv6_lab5_cow/">XV6 Lab 5: Copy On Write</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/tech/xv6_lab4_trap/">XV6 Lab 4: Traps</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/tech/xv6_lab3_pagetable/">XV6 Lab 3: Page Table</a></li>
		</ul>
	</div>
</div>
<div class="widget-taglist widget">
	<h4 class="widget__title">Tags</h4>
	<div class="widget__content">
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/compiler/" title="Compiler">Compiler (1)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/cuda/" title="CUDA">CUDA (2)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/distributed-system/" title="Distributed System">Distributed System (2)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/lecture/" title="Lecture">Lecture (5)</a>
	</div>
</div>
<div class="toc__block_div">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
		<nav id="TableOfContents">
  <ul>
    <li><a href="#1-并行模型">1. 并行模型</a>
      <ul>
        <li><a href="#11-数据并行-data-parallesim">1.1 数据并行 （Data Parallesim）</a>
          <ul>
            <li><a href="#111-dpdata-parallelparameter-server">1.1.1 DP（Data Parallel）Parameter Server</a></li>
            <li><a href="#112-ddpdistributed-data-parallelring-all-reduce">1.1.2 DDP（Distributed Data Parallel）Ring-All-Reduce</a></li>
            <li><a href="#113-fsdpfully-shared-data-parallel">1.1.3 FSDP（Fully Shared Data Parallel）</a></li>
          </ul>
        </li>
        <li><a href="#12-流水并行pipeline-parallesim">1.2 流水并行（Pipeline Parallesim）</a></li>
        <li><a href="#13-模型并行tensor-parallesim">1.3 模型并行（Tensor Parallesim）</a></li>
        <li><a href="#14-混合并行mixture-of-experts">1.4 混合并行（Mixture-of-Experts）</a></li>
      </ul>
    </li>
    <li><a href="#2-自动并行方法">2. 自动并行方法</a>
      <ul>
        <li><a href="#21-exploring-hidden-dimensions-in-parallelizing-convolutional-neural-networks-9">2.1 Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks [9]</a></li>
      </ul>
    </li>
  </ul>
</nav>
	</div>
</div>

</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 chenghua.wang.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/keep-moving-forward/js/menu.js"></script>




<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { fonts: ["TeX"] }
  });
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async>
</script>
</body>
</html>