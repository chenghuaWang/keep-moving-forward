<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>mllmæ¡†æ¶æµ…æ-ä»¥QWen0.5Bä¸ºä¾‹ - Ubios Home</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="mllmæ¡†æ¶æµ…æ-ä»¥QWen0.5Bä¸ºä¾‹" />
<meta property="og:description" content="ç¬”è€…æœ€è¿‘åœ¨åšä¸€äº›mllmç›¸å…³çš„å·¥ä½œï¼Œä¹¦å†™æ­¤æ–‡å¯¹mllmæ¡†æ¶è¿›è¡Œæ¢³ç†æ€»ç»“ï¼Œå®šæœ‰ä¸å°‘çº°æ¼ï¼Œè¯·è¯»è€…ç«‹å³æŒ‡å‡ºï¼Œè°¢è°¢ã€‚mllmç›®å‰åœ¨åšä¸€äº›å…¶ä»–å·¥ä½œï¼Œè¿™ç¯‡æ–‡ç« çš„ä¹¦å†™æ—¶é—´ä¸ºå‘å¸ƒæ—¶é—´ã€‚åœ¨mllmçš„å…¶ä»–å·¥ä½œåˆå¹¶è¿›ä¸»ä»“åº“åï¼Œæœ¬æ–‡è¿˜ä¼šè¿›ä¸€æ­¥çš„è·Ÿè¿›ã€‚è¯»è€…è¯·æ³¨æ„æœ¬æ–‡çš„æ—¶æ•ˆæ€§ã€‚
1. ç®€ä»‹ mllmæ˜¯ä¸€æ¬¾é€‚ç”¨äºç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¾å¤‡çš„å¿«é€Ÿã€è½»é‡çš„å¤šæ¨¡æ€LLMæ¨ç†å¼•æ“ã€‚
å®Œå…¨çš„C/C&#43;&#43;å®ç°ï¼Œæ— ç¬¬ä¸‰æ–¹ä¾èµ– é’ˆå¯¹fuyu-8Bç­‰å¤šæ¨¡æ€LLMè¿›è¡Œäº†ä¼˜åŒ– æ”¯æŒARM NEONå’ŒX86 AVX2å‘é‡æŒ‡ä»¤ æ”¯æŒ4 bitså’Œ6 bitsæ•´æ•°é‡åŒ– æœ¬æ–‡å°†æ›´å¤šçš„ä»¥å·¥ç¨‹çš„è§†è§’æ¥è§£æmllmæ¡†æ¶ï¼Œåœ¨è¡Œæ–‡è¿‡ç¨‹ä¸­ï¼Œæœ¬æ–‡ä¼šå°†mllmä¸å…¶ä»–æ¡†æ¶çš„è®¾è®¡æ–¹æ³•åšå¯¹æ¯”ã€‚æ¥ä¸‹æ¥ï¼Œæœ¬æ–‡å°†ä¼šç”¨é¡¹ç›®ç»„ç»‡ç»“æ„ã€æ¡†æ¶æ‰§è¡Œæµç¨‹ã€è‡ªå®šä¹‰Op/Layerã€Tokenizerå’Œå¦‚ä½•æ”¯æŒæ–°æ¨¡å‹äº”ä¸ªç« èŠ‚æ¥è¯¦ç»†æè¿°mllmæ¡†æ¶çš„å„é¡¹ç‰¹æ€§å’Œæ€»ä½“ç»“æ„ã€‚è¯»è€…å¯ä»¥æŠŠè¯¥æ–‡ç« åšmllmçš„ä½¿ç”¨æ–‡æ¡£ã€‚åœ¨æœ€åï¼Œæœ¬æ–‡å°†ä¼šæŒ‡å‡ºmllmçš„ä¸è¶³ä¹‹å¤„å’Œå¯ä»¥å°è¯•è·Ÿè¿›çš„å·¥ä½œã€‚
åœ¨å¼€å§‹æ­£å¼è§£æmllmä¹‹å‰ï¼Œè¯»è€…å¯ä»¥å…ˆcloneä¸‹mllmçš„ä»£ç åº“ï¼Œä»¥ä¾¿äºè·Ÿè¿›åˆ†ææµç¨‹ã€‚mllmä¸ä¾èµ–äºgit submoduleï¼Œé¡¹ç›®é…ç½®èµ·æ¥å¾ˆæ–¹ä¾¿ï¼Œç›®å‰mllmå¯ä»¥åœ¨linuxä¸Šä½¿ç”¨Clang/GCCç¼–è¯‘å™¨è¿›è¡Œç¼–è¯‘ã€‚ç›®å‰mllmæ”¯æŒçš„ç›®æ ‡è®¾å¤‡ä½“ç³»ç»“æ„æ˜¯X86å’ŒArmã€‚
git clone https://github.com/UbiquitousLearning/mllm mllmå›¢é˜Ÿå°†æ‰€æœ‰LLMç›¸å…³çš„vocabæ–‡ä»¶éƒ½æ”¾åœ¨äº†gitä»“åº“ä¸­ï¼ˆè¿™ä¸ªå…¶å®å¯ä»¥ç§»åŠ¨åˆ°HuggingFaceçš„ä»“åº“ä¸Šï¼‰ï¼ŒLLMé‡åŒ–åçš„æ¨¡å‹æ–‡ä»¶éƒ½å­˜å‚¨åœ¨HuggingFaceä¸Šï¼Œè¯»è€…å¯ä»¥åœ¨https://huggingface.co/mllmTeamä¸Šæ‰¾åˆ°mllmæä¾›çš„æ¨¡å‹æ–‡ä»¶ã€‚
2. æ¡†æ¶æ‰§è¡Œæµç¨‹ 2.1 ä»¥ä¸¤å±‚Linearå±‚è¿è¡Œä¸ºä¾‹ é¦–å…ˆï¼Œè€ƒè™‘ä¸‹é¢çš„ä»£ç ï¼Œå®šä¹‰äº†ä¸¤ä¸ªLinear Layersï¼Œå¹¶ä¸”è¾“å…¥$X$é€šè¿‡ä¸¤ä¸ªLinear Layersæ¥å¾—åˆ°è¾“å‡ºï¼š
class TwoLinear final : public Module { public: TwoLinear() = default; TwoLinear() { linear1 = Linear(in_f, out_f, /*bias*/true, &#34;linear1&#34;); linear2 = Linear(out_f, out_f, /*bias*/true, &#34;linear2&#34;); } std::vector&lt;Tensor&gt; Forward(std::vector&lt;Tensor&gt; inputs, std::vector&lt;std::any&gt; args) override { x = inputs[0]; x = linear1(x); x = linear2(x); return x; } private: Layer linear1; Layer linear2; } TwoLinear tl; 2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chenghuawang.github.io/keep-moving-forward/tech/mllm-qwen/" /><meta property="article:section" content="Tech" />
<meta property="article:published_time" content="2024-06-28T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-06-28T00:00:00+00:00" />

		<meta itemprop="name" content="mllmæ¡†æ¶æµ…æ-ä»¥QWen0.5Bä¸ºä¾‹">
<meta itemprop="description" content="ç¬”è€…æœ€è¿‘åœ¨åšä¸€äº›mllmç›¸å…³çš„å·¥ä½œï¼Œä¹¦å†™æ­¤æ–‡å¯¹mllmæ¡†æ¶è¿›è¡Œæ¢³ç†æ€»ç»“ï¼Œå®šæœ‰ä¸å°‘çº°æ¼ï¼Œè¯·è¯»è€…ç«‹å³æŒ‡å‡ºï¼Œè°¢è°¢ã€‚mllmç›®å‰åœ¨åšä¸€äº›å…¶ä»–å·¥ä½œï¼Œè¿™ç¯‡æ–‡ç« çš„ä¹¦å†™æ—¶é—´ä¸ºå‘å¸ƒæ—¶é—´ã€‚åœ¨mllmçš„å…¶ä»–å·¥ä½œåˆå¹¶è¿›ä¸»ä»“åº“åï¼Œæœ¬æ–‡è¿˜ä¼šè¿›ä¸€æ­¥çš„è·Ÿè¿›ã€‚è¯»è€…è¯·æ³¨æ„æœ¬æ–‡çš„æ—¶æ•ˆæ€§ã€‚
1. ç®€ä»‹ mllmæ˜¯ä¸€æ¬¾é€‚ç”¨äºç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¾å¤‡çš„å¿«é€Ÿã€è½»é‡çš„å¤šæ¨¡æ€LLMæ¨ç†å¼•æ“ã€‚
å®Œå…¨çš„C/C&#43;&#43;å®ç°ï¼Œæ— ç¬¬ä¸‰æ–¹ä¾èµ– é’ˆå¯¹fuyu-8Bç­‰å¤šæ¨¡æ€LLMè¿›è¡Œäº†ä¼˜åŒ– æ”¯æŒARM NEONå’ŒX86 AVX2å‘é‡æŒ‡ä»¤ æ”¯æŒ4 bitså’Œ6 bitsæ•´æ•°é‡åŒ– æœ¬æ–‡å°†æ›´å¤šçš„ä»¥å·¥ç¨‹çš„è§†è§’æ¥è§£æmllmæ¡†æ¶ï¼Œåœ¨è¡Œæ–‡è¿‡ç¨‹ä¸­ï¼Œæœ¬æ–‡ä¼šå°†mllmä¸å…¶ä»–æ¡†æ¶çš„è®¾è®¡æ–¹æ³•åšå¯¹æ¯”ã€‚æ¥ä¸‹æ¥ï¼Œæœ¬æ–‡å°†ä¼šç”¨é¡¹ç›®ç»„ç»‡ç»“æ„ã€æ¡†æ¶æ‰§è¡Œæµç¨‹ã€è‡ªå®šä¹‰Op/Layerã€Tokenizerå’Œå¦‚ä½•æ”¯æŒæ–°æ¨¡å‹äº”ä¸ªç« èŠ‚æ¥è¯¦ç»†æè¿°mllmæ¡†æ¶çš„å„é¡¹ç‰¹æ€§å’Œæ€»ä½“ç»“æ„ã€‚è¯»è€…å¯ä»¥æŠŠè¯¥æ–‡ç« åšmllmçš„ä½¿ç”¨æ–‡æ¡£ã€‚åœ¨æœ€åï¼Œæœ¬æ–‡å°†ä¼šæŒ‡å‡ºmllmçš„ä¸è¶³ä¹‹å¤„å’Œå¯ä»¥å°è¯•è·Ÿè¿›çš„å·¥ä½œã€‚
åœ¨å¼€å§‹æ­£å¼è§£æmllmä¹‹å‰ï¼Œè¯»è€…å¯ä»¥å…ˆcloneä¸‹mllmçš„ä»£ç åº“ï¼Œä»¥ä¾¿äºè·Ÿè¿›åˆ†ææµç¨‹ã€‚mllmä¸ä¾èµ–äºgit submoduleï¼Œé¡¹ç›®é…ç½®èµ·æ¥å¾ˆæ–¹ä¾¿ï¼Œç›®å‰mllmå¯ä»¥åœ¨linuxä¸Šä½¿ç”¨Clang/GCCç¼–è¯‘å™¨è¿›è¡Œç¼–è¯‘ã€‚ç›®å‰mllmæ”¯æŒçš„ç›®æ ‡è®¾å¤‡ä½“ç³»ç»“æ„æ˜¯X86å’ŒArmã€‚
git clone https://github.com/UbiquitousLearning/mllm mllmå›¢é˜Ÿå°†æ‰€æœ‰LLMç›¸å…³çš„vocabæ–‡ä»¶éƒ½æ”¾åœ¨äº†gitä»“åº“ä¸­ï¼ˆè¿™ä¸ªå…¶å®å¯ä»¥ç§»åŠ¨åˆ°HuggingFaceçš„ä»“åº“ä¸Šï¼‰ï¼ŒLLMé‡åŒ–åçš„æ¨¡å‹æ–‡ä»¶éƒ½å­˜å‚¨åœ¨HuggingFaceä¸Šï¼Œè¯»è€…å¯ä»¥åœ¨https://huggingface.co/mllmTeamä¸Šæ‰¾åˆ°mllmæä¾›çš„æ¨¡å‹æ–‡ä»¶ã€‚
2. æ¡†æ¶æ‰§è¡Œæµç¨‹ 2.1 ä»¥ä¸¤å±‚Linearå±‚è¿è¡Œä¸ºä¾‹ é¦–å…ˆï¼Œè€ƒè™‘ä¸‹é¢çš„ä»£ç ï¼Œå®šä¹‰äº†ä¸¤ä¸ªLinear Layersï¼Œå¹¶ä¸”è¾“å…¥$X$é€šè¿‡ä¸¤ä¸ªLinear Layersæ¥å¾—åˆ°è¾“å‡ºï¼š
class TwoLinear final : public Module { public: TwoLinear() = default; TwoLinear() { linear1 = Linear(in_f, out_f, /*bias*/true, &#34;linear1&#34;); linear2 = Linear(out_f, out_f, /*bias*/true, &#34;linear2&#34;); } std::vector&lt;Tensor&gt; Forward(std::vector&lt;Tensor&gt; inputs, std::vector&lt;std::any&gt; args) override { x = inputs[0]; x = linear1(x); x = linear2(x); return x; } private: Layer linear1; Layer linear2; } TwoLinear tl; 2."><meta itemprop="datePublished" content="2024-06-28T00:00:00+00:00" />
<meta itemprop="dateModified" content="2024-06-28T00:00:00+00:00" />
<meta itemprop="wordCount" content="1622">
<meta itemprop="keywords" content="LLM Server,LLM," />
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans+SC:400,700">

	<link rel="stylesheet" href="/keep-moving-forward/css/style.css">
	

	<link rel="shortcut icon" href="/keep-moving-forward/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		
<header class="header">
	<div class="container header__container">
		
	<div class="logo logo--mixed" >
		<a class="logo__link" href="/keep-moving-forward/" title="Ubios Home" rel="home" >
			
			<div class="logo__item logo__text" >
					<div class="logo__title" >Ubios Home</div>
					<div class="logo__tagline">Remember brick walls let us show our dedication. They are there to separate us from the people who don&#39;t really want to achieve their childhood dreams. --Randy Pausch</div>
				</div>
		</a>
	</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/keep-moving-forward/about/about/">
				
				<span class="menu__text">About</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/keep-moving-forward/about/hpc_ai/">
				
				<span class="menu__text">HPC &amp; AI å…¥å‘</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/keep-moving-forward/about/lecture_notes/">
				
				<span class="menu__text">Lecture-Notes</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/keep-moving-forward/about/paper_posts/">
				
				<span class="menu__text">Paper-Notes</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/keep-moving-forward/about/tech_posts/">
				
				<span class="menu__text">Tech-Posts</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/keep-moving-forward/about/thinking/">
				
				<span class="menu__text">Thinking</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/keep-moving-forward/about/news/">
				
				<span class="menu__text">ğŸ‰NewsğŸ‰</span>
				
			</a>
		</li>
	</ul>
</nav>

	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			


<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">mllmæ¡†æ¶æµ…æ-ä»¥QWen0.5Bä¸ºä¾‹</h1>
			<p class="post__lead">mllm framework</p>
			<div class="post__meta meta"><div class="meta__item-author meta__item">
	<svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2 0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class="meta__text">chenghua.wang</span>
</div>
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class="meta__text" datetime="2024-06-28T00:00:00Z">2024-06-28</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg><span class="meta__text"><a class="meta__link" href="/keep-moving-forward/categories/aisys/" rel="category">AI&amp;Sys</a>
	</span>
</div></div>
		</header>

		
<div class="post__toc toc">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
		<nav id="TableOfContents">
  <ul>
    <li><a href="#1-ç®€ä»‹">1. ç®€ä»‹</a></li>
    <li><a href="#2-æ¡†æ¶æ‰§è¡Œæµç¨‹">2. æ¡†æ¶æ‰§è¡Œæµç¨‹</a>
      <ul>
        <li><a href="#21-ä»¥ä¸¤å±‚linearå±‚è¿è¡Œä¸ºä¾‹">2.1 ä»¥ä¸¤å±‚Linearå±‚è¿è¡Œä¸ºä¾‹</a>
          <ul>
            <li><a href="#211-åŠ è½½å‚æ•°">2.1.1 åŠ è½½å‚æ•°</a></li>
            <li><a href="#212moduleçš„operatoræ˜¯å¦‚ä½•è°ƒç”¨forwardå‡½æ•°çš„">2.1.2Moduleçš„Operator()æ˜¯å¦‚ä½•è°ƒç”¨Forwardå‡½æ•°çš„ï¼Ÿ</a></li>
            <li><a href="#213-linearå±‚çš„æ‰§è¡Œ">2.1.3 Linearå±‚çš„æ‰§è¡Œ</a></li>
          </ul>
        </li>
        <li><a href="#22-æ€»ç»“">2.2 æ€»ç»“</a></li>
      </ul>
    </li>
    <li><a href="#3-å¦‚ä½•ç¼–å†™opä¸è‡ªå®šä¹‰layer">3. å¦‚ä½•ç¼–å†™Opä¸è‡ªå®šä¹‰Layer</a>
      <ul>
        <li><a href="#31-æ–°å¢å¯¹åº”backendçš„opæ–‡ä»¶">3.1 æ–°å¢å¯¹åº”Backendçš„Opæ–‡ä»¶</a></li>
        <li><a href="#32-opå‚æ•°è‡ªå®šä¹‰">3.2 Opå‚æ•°è‡ªå®šä¹‰</a></li>
        <li><a href="#33-é‡è½½å‡½æ•°">3.3 é‡è½½å‡½æ•°</a></li>
        <li><a href="#34-opæ˜¯å¦‚ä½•è¢«æ³¨å†Œå’Œåˆ›å»ºçš„">3.4 Opæ˜¯å¦‚ä½•è¢«æ³¨å†Œå’Œåˆ›å»ºçš„ï¼Ÿ</a>
          <ul>
            <li><a href="#341-åœ¨backendä¸­æ³¨å†Œop">3.4.1 åœ¨Backendä¸­æ³¨å†ŒOp</a></li>
            <li><a href="#342-åœ¨layerhppä¸­åŠ å…¥å¯¹åº”çš„op-layer">3.4.2 åœ¨Layer.hppä¸­åŠ å…¥å¯¹åº”çš„Op Layer</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#4-tokenizer">4. Tokenizer</a></li>
    <li><a href="#5-å¦‚ä½•å¯¹æ–°æ¨¡å‹è¿›è¡Œæ”¯æŒ">5. å¦‚ä½•å¯¹æ–°æ¨¡å‹è¿›è¡Œæ”¯æŒ</a>
      <ul>
        <li><a href="#51-ç”Ÿæˆmllmæ”¯æŒçš„vocabå’Œæ¨¡å‹å‚æ•°">5.1 ç”Ÿæˆmllmæ”¯æŒçš„vocabå’Œæ¨¡å‹å‚æ•°</a>
          <ul>
            <li><a href="#511-æ¨¡å‹è½¬æ¢">5.1.1 æ¨¡å‹è½¬æ¢</a></li>
            <li><a href="#512-vocabè½¬æ¢">5.1.2 Vocabè½¬æ¢</a></li>
            <li><a href="#513-é‡åŒ–">5.1.3 é‡åŒ–</a></li>
          </ul>
        </li>
        <li><a href="#52-configuration">5.2 Configuration</a></li>
        <li><a href="#53-tokenization">5.3 Tokenization</a></li>
        <li><a href="#54-modeling">5.4 Modeling</a>
          <ul>
            <li><a href="#541-åˆ›å»ºè¯¥moduleéœ€è¦ä½¿ç”¨çš„layers">5.4.1 åˆ›å»ºè¯¥Moduleéœ€è¦ä½¿ç”¨çš„Layers</a></li>
            <li><a href="#542-é‡è½½forwardå‰å‘æ¨ç†å‡½æ•°">5.4.2 é‡è½½Forwardå‰å‘æ¨ç†å‡½æ•°</a></li>
          </ul>
        </li>
        <li><a href="#55-è¿è¡Œ">5.5 è¿è¡Œ</a></li>
      </ul>
    </li>
    <li><a href="#6-mllmæ¡†æ¶çš„ä¸è¶³">6. mllmæ¡†æ¶çš„ä¸è¶³</a>
      <ul>
        <li><a href="#61-benchmark">6.1 Benchmark</a></li>
        <li><a href="#62-å¯¹äºç§»åŠ¨ç«¯llmæ¨ç†çš„ç‰¹å®šä¼˜åŒ–">6.2 å¯¹äºç§»åŠ¨ç«¯LLMæ¨ç†çš„ç‰¹å®šä¼˜åŒ–</a></li>
        <li><a href="#63-æ˜“ç”¨æ€§">6.3 æ˜“ç”¨æ€§</a></li>
      </ul>
    </li>
    <li><a href="#a1-qwenæ¨¡å‹å®šä¹‰">A1. Qwenæ¨¡å‹å®šä¹‰</a></li>
  </ul>
</nav>
	</div>
</div><div class="content post__content clearfix">
			<hr>
<p>ç¬”è€…æœ€è¿‘åœ¨åšä¸€äº›mllmç›¸å…³çš„å·¥ä½œï¼Œä¹¦å†™æ­¤æ–‡å¯¹mllmæ¡†æ¶è¿›è¡Œæ¢³ç†æ€»ç»“ï¼Œå®šæœ‰ä¸å°‘çº°æ¼ï¼Œè¯·è¯»è€…ç«‹å³æŒ‡å‡ºï¼Œè°¢è°¢ã€‚mllmç›®å‰åœ¨åšä¸€äº›å…¶ä»–å·¥ä½œï¼Œè¿™ç¯‡æ–‡ç« çš„ä¹¦å†™æ—¶é—´ä¸ºå‘å¸ƒæ—¶é—´ã€‚åœ¨mllmçš„å…¶ä»–å·¥ä½œåˆå¹¶è¿›ä¸»ä»“åº“åï¼Œæœ¬æ–‡è¿˜ä¼šè¿›ä¸€æ­¥çš„è·Ÿè¿›ã€‚è¯»è€…è¯·æ³¨æ„æœ¬æ–‡çš„æ—¶æ•ˆæ€§ã€‚</p>
<hr>
<h1 id="1-ç®€ä»‹">1. ç®€ä»‹</h1>
<p><strong>mllm</strong>æ˜¯ä¸€æ¬¾é€‚ç”¨äº<strong>ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¾å¤‡</strong>çš„å¿«é€Ÿã€è½»é‡çš„å¤šæ¨¡æ€LLMæ¨ç†å¼•æ“ã€‚</p>
<ul>
<li>å®Œå…¨çš„C/C++å®ç°ï¼Œæ— ç¬¬ä¸‰æ–¹ä¾èµ–</li>
<li>é’ˆå¯¹fuyu-8Bç­‰å¤šæ¨¡æ€LLMè¿›è¡Œäº†ä¼˜åŒ–</li>
<li>æ”¯æŒARM NEONå’ŒX86 AVX2å‘é‡æŒ‡ä»¤</li>
<li>æ”¯æŒ4 bitså’Œ6 bitsæ•´æ•°é‡åŒ–</li>
</ul>
<p>æœ¬æ–‡å°†æ›´å¤šçš„ä»¥å·¥ç¨‹çš„è§†è§’æ¥è§£æmllmæ¡†æ¶ï¼Œåœ¨è¡Œæ–‡è¿‡ç¨‹ä¸­ï¼Œæœ¬æ–‡ä¼šå°†mllmä¸å…¶ä»–æ¡†æ¶çš„è®¾è®¡æ–¹æ³•åšå¯¹æ¯”ã€‚æ¥ä¸‹æ¥ï¼Œæœ¬æ–‡å°†ä¼šç”¨<strong>é¡¹ç›®ç»„ç»‡ç»“æ„ã€æ¡†æ¶æ‰§è¡Œæµç¨‹ã€è‡ªå®šä¹‰Op/Layerã€Tokenizerå’Œå¦‚ä½•æ”¯æŒæ–°æ¨¡å‹</strong>äº”ä¸ªç« èŠ‚æ¥è¯¦ç»†æè¿°mllmæ¡†æ¶çš„å„é¡¹ç‰¹æ€§å’Œæ€»ä½“ç»“æ„ã€‚è¯»è€…å¯ä»¥æŠŠè¯¥æ–‡ç« åšmllmçš„ä½¿ç”¨æ–‡æ¡£ã€‚<strong>åœ¨æœ€åï¼Œæœ¬æ–‡å°†ä¼šæŒ‡å‡ºmllmçš„ä¸è¶³ä¹‹å¤„å’Œå¯ä»¥å°è¯•è·Ÿè¿›çš„å·¥ä½œ</strong>ã€‚</p>
<hr>
<p>åœ¨å¼€å§‹æ­£å¼è§£æmllmä¹‹å‰ï¼Œè¯»è€…å¯ä»¥å…ˆcloneä¸‹mllmçš„ä»£ç åº“ï¼Œä»¥ä¾¿äºè·Ÿè¿›åˆ†ææµç¨‹ã€‚mllmä¸ä¾èµ–äºgit submoduleï¼Œé¡¹ç›®é…ç½®èµ·æ¥å¾ˆæ–¹ä¾¿ï¼Œç›®å‰mllmå¯ä»¥åœ¨linuxä¸Šä½¿ç”¨Clang/GCCç¼–è¯‘å™¨è¿›è¡Œç¼–è¯‘ã€‚ç›®å‰mllmæ”¯æŒçš„ç›®æ ‡è®¾å¤‡ä½“ç³»ç»“æ„æ˜¯X86å’ŒArmã€‚</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>git clone https://github.com/UbiquitousLearning/mllm
</span></span></code></pre></div><p>mllmå›¢é˜Ÿå°†æ‰€æœ‰LLMç›¸å…³çš„vocabæ–‡ä»¶éƒ½æ”¾åœ¨äº†gitä»“åº“ä¸­ï¼ˆè¿™ä¸ªå…¶å®å¯ä»¥ç§»åŠ¨åˆ°HuggingFaceçš„ä»“åº“ä¸Šï¼‰ï¼ŒLLMé‡åŒ–åçš„æ¨¡å‹æ–‡ä»¶éƒ½å­˜å‚¨åœ¨HuggingFaceä¸Šï¼Œè¯»è€…å¯ä»¥åœ¨<a href="https://huggingface.co/mllmTeam">https://huggingface.co/mllmTeam</a>ä¸Šæ‰¾åˆ°mllmæä¾›çš„æ¨¡å‹æ–‡ä»¶ã€‚</p>
<h1 id="2-æ¡†æ¶æ‰§è¡Œæµç¨‹">2. æ¡†æ¶æ‰§è¡Œæµç¨‹</h1>
<h2 id="21-ä»¥ä¸¤å±‚linearå±‚è¿è¡Œä¸ºä¾‹">2.1 ä»¥ä¸¤å±‚Linearå±‚è¿è¡Œä¸ºä¾‹</h2>
<p>é¦–å…ˆï¼Œè€ƒè™‘ä¸‹é¢çš„ä»£ç ï¼Œå®šä¹‰äº†ä¸¤ä¸ªLinear Layersï¼Œå¹¶ä¸”è¾“å…¥$X$é€šè¿‡ä¸¤ä¸ªLinear Layersæ¥å¾—åˆ°è¾“å‡ºï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">TwoLinear</span> <span style="color:#66d9ef">final</span> <span style="color:#f92672">:</span> <span style="color:#66d9ef">public</span> Module {
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">public</span><span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>    TwoLinear() <span style="color:#f92672">=</span> <span style="color:#66d9ef">default</span>;
</span></span><span style="display:flex;"><span>    TwoLinear() {
</span></span><span style="display:flex;"><span>        linear1 <span style="color:#f92672">=</span> Linear(in_f, out_f, <span style="color:#75715e">/*bias*/</span>true, <span style="color:#e6db74">&#34;linear1&#34;</span>);
</span></span><span style="display:flex;"><span>        linear2 <span style="color:#f92672">=</span> Linear(out_f, out_f, <span style="color:#75715e">/*bias*/</span>true, <span style="color:#e6db74">&#34;linear2&#34;</span>);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;</span> Forward(std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;</span> inputs, std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>std<span style="color:#f92672">::</span>any<span style="color:#f92672">&gt;</span> args) <span style="color:#66d9ef">override</span> {
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> inputs[<span style="color:#ae81ff">0</span>];
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> linear1(x);
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> linear2(x);
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">private</span><span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>    Layer linear1;
</span></span><span style="display:flex;"><span>    Layer linear2;
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>TwoLinear tl;
</span></span></code></pre></div><h3 id="211-åŠ è½½å‚æ•°">2.1.1 åŠ è½½å‚æ•°</h3>
<p>è¯»è€…å¯ä»¥ä½¿ç”¨ <code>tl.load(path)</code>æ¥åŠ è½½å‚æ•°ã€‚é‚£ä¹ˆmllmæ˜¯å¦‚ä½•å®ç°å‚æ•°åŠ è½½çš„å‘¢ï¼Ÿåœ¨loadå‡½æ•°ä¸­ï¼Œmllmä¼šåˆ›å»ºä¸€ä¸ªParamLoaderï¼Œè¿™ä¸ªParamLoaderæ˜¯Staticçš„ï¼Œåœ¨å…¨å±€å¯ä»¥è®¿é—®ã€‚ç„¶åmllmä¼šè®¾ç½®å¦ä¸€ä¸ªå…¨å±€å‚æ•°doLoadä¸ºTrueï¼Œè¿›è€Œè¿›å…¥æ¨ç†æµç¨‹<code>operator()(tmps, tmpt);</code>ã€‚åœ¨æ¨ç†æµç¨‹ä¸­ï¼Œè¦æ˜¯æ‰§è¡Œå±‚å‘ç°doLoadä¸ºTrueï¼Œé‚£ä¹ˆå°±æ‰§è¡Œæ¯ä¸ªç®—å­å†…å®šä¹‰å¥½çš„loadæŒ‡ä»¤ï¼Œè€Œä¸æ˜¯æ‰§è¡Œæ¯ä¸ªç®—å­çš„åŸæœ¬é€»è¾‘ã€‚
loadçš„æ‰§è¡Œåœ¨<code>Layer.hpp</code>æ–‡ä»¶çš„<code>INIT_OP()</code>ä¸­ã€‚</p>
<h3 id="212moduleçš„operatoræ˜¯å¦‚ä½•è°ƒç”¨forwardå‡½æ•°çš„">2.1.2Moduleçš„Operator()æ˜¯å¦‚ä½•è°ƒç”¨Forwardå‡½æ•°çš„ï¼Ÿ</h3>
<p>å¯¹äºå¸¸è§çš„<code>INPUT_TENSOR</code>ç±»å‹çš„Tensorï¼Œmllmé¦–å…ˆä¼šè®¾ç½®è¿™ä¸ªTensorçš„ç±»å‹ä¸º<code>TENSOR_STATIC_INIT</code>ï¼Œè¿›è¡Œä¸€éForwardæ¨ç†ï¼›ç¬¬ä¸€éForwardæ¨ç†å®Œæ¯•ä»¥åå†æŠŠTensorçš„ç±»å‹è®¾ä¸º<code>TENSOR_STATIC_READY</code>ï¼Œç„¶åè¿›è¡Œç¬¬äºŒéForwardæ¨ç†ã€‚</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">if</span> (inputs[<span style="color:#ae81ff">0</span>].ttype() <span style="color:#f92672">==</span> TensorType<span style="color:#f92672">::</span>INPUT_TENSOR) {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">auto</span> <span style="color:#f92672">&amp;</span>input : inputs) {
</span></span><span style="display:flex;"><span>        input.setTtype(TensorType<span style="color:#f92672">::</span>NORMAL_TENSOR);
</span></span><span style="display:flex;"><span>        input.status() <span style="color:#f92672">=</span> TENSOR_STATIC_INIT;
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span>(input.batch() <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>){
</span></span><span style="display:flex;"><span>            Tensor<span style="color:#f92672">::</span>gph_[input.name()] <span style="color:#f92672">=</span> input;
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    tensor_status <span style="color:#f92672">=</span> TENSOR_STATIC_INIT;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    Forward(inputs, anyArgs);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">auto</span> <span style="color:#f92672">&amp;</span>input : inputs) {
</span></span><span style="display:flex;"><span>        input.status() <span style="color:#f92672">=</span> TENSOR_STATIC_READY;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    tensor_status <span style="color:#f92672">=</span> TENSOR_STATIC_READY;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#a6e22e">Forward</span>(inputs, anyArgs);
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>ç¬¬ä¸€æ¬¡Forwardæ¨ç†çš„ç›®çš„æ˜¯è°ƒç”¨Opå®šä¹‰çš„Reshapeå’ŒSetUpå‡½æ•°ï¼ŒReshapeå‡½æ•°ä¼šæ¨ç†å‡ºè¿™ä¸€æ¬¡æ¨¡å‹æ¨ç†çš„è¿‡ç¨‹ä¸­æ¯ä¸ªTensorçš„å½¢çŠ¶å¤§å°ã€‚SetUpå‡½æ•°ä¼šå¯¹Opéœ€è¦è¾“å‡ºçš„Tensoråšå†…å­˜çš„ç”³è¯·ã€‚
ç¬¬äºŒæ¬¡Forwardæ¨ç†æ‰æ˜¯çœŸæ­£çš„è®¡ç®—ã€‚</p>
<h3 id="213-linearå±‚çš„æ‰§è¡Œ">2.1.3 Linearå±‚çš„æ‰§è¡Œ</h3>
<p>æ¯ä¸ªLayeråœ¨å®ç°çš„æ—¶å€™éƒ½ä¼šé‡è½½<code>operator()</code>ï¼Œæ¯”å¦‚linear layerçš„<code>operator()</code>å‡½æ•°å¦‚ä¸‹ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span>Tensor <span style="color:#f92672">&amp;</span><span style="color:#66d9ef">operator</span>()(Tensor <span style="color:#f92672">&amp;</span>input) {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#a6e22e">_1I1O_OP</span>(input);
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>å…¶ä¸­ï¼Œ<code>_1I1O_OP</code>è¡¨ç¤ºçš„æ„æ€æ˜¯ï¼Œè¿™æ˜¯éœ€è¦ä½¿ç”¨1ä¸ªè¾“å…¥å’Œ1ä¸ªè¾“å‡ºçš„å‡½æ•°æ¥å¤„ç†è¿™ä¸ªç®—å­ã€‚mllmè¿˜æä¾›äº†è®¸å¤šç±»ä¼¼äº<code>_1I1O_OP</code>çš„å‡½æ•°æ¥å¤„ç†ä¸åŒçš„ç®—å­ã€‚</p>
<h2 id="22-æ€»ç»“">2.2 æ€»ç»“</h2>
<p>å¤§ä½“æ¥è¯´ï¼Œmllmä½¿ç”¨äº†ç±»ä¼¼äºçŠ¶æ€æœºçš„å‚æ•°æ¥è®¾ç½®äº†å½“å‰æ¨ç†è¿‡ç¨‹çš„è¿è¡ŒçŠ¶æ€ã€‚æ¯ä¸€æ¬¡éƒ½æ˜¯é€šè¿‡Forwardå‡½æ•°æ¥è¿›è¡Œå…¨æ¨¡å‹çš„éå†ï¼Œåœ¨Opçš„æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œç”¨è¿™äº›è®¾å®šçš„å‚æ•°æ¥åŒºåˆ†æ¯æ¬¡Opéœ€è¦è¡¨ç°çš„è¡Œä¸ºã€‚</p>
<h1 id="3-å¦‚ä½•ç¼–å†™opä¸è‡ªå®šä¹‰layer">3. å¦‚ä½•ç¼–å†™Opä¸è‡ªå®šä¹‰Layer</h1>
<h2 id="31-æ–°å¢å¯¹åº”backendçš„opæ–‡ä»¶">3.1 æ–°å¢å¯¹åº”Backendçš„Opæ–‡ä»¶</h2>
<p>mllmæä¾›äº†<code>src/backends/new_op.py</code>å®ç”¨å·¥å…·æ¥å¸®åŠ©åˆ›å»ºOp Classã€‚è¯¥æ–‡ä»¶ä¼šå¸®åŠ©è¯»è€…åˆ›å»ºä¸‹è¿°åŸºæœ¬å‡½æ•°ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span>ErrorCode <span style="color:#a6e22e">reshape</span>(vector<span style="color:#f92672">&lt;</span>shared_ptr<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;&gt;</span> inputs, vector<span style="color:#f92672">&lt;</span>shared_ptr<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;&gt;</span> outputs) <span style="color:#66d9ef">override</span>;
</span></span><span style="display:flex;"><span>ErrorCode <span style="color:#a6e22e">execute</span>(vector<span style="color:#f92672">&lt;</span>shared_ptr<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;&gt;</span> inputs, vector<span style="color:#f92672">&lt;</span>shared_ptr<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;&gt;</span> outputs) <span style="color:#66d9ef">override</span>;
</span></span><span style="display:flex;"><span>ErrorCode <span style="color:#a6e22e">load</span>(AbstructLoader <span style="color:#f92672">&amp;</span>loader) <span style="color:#66d9ef">override</span>;
</span></span><span style="display:flex;"><span>ErrorCode <span style="color:#a6e22e">free</span>(vector<span style="color:#f92672">&lt;</span>shared_ptr<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;&gt;</span> inputs, vector<span style="color:#f92672">&lt;</span>shared_ptr<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;&gt;</span> outputs) <span style="color:#66d9ef">override</span>;
</span></span><span style="display:flex;"><span>ErrorCode <span style="color:#a6e22e">setUp</span>(vector<span style="color:#f92672">&lt;</span>shared_ptr<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;&gt;</span> inputs, vector<span style="color:#f92672">&lt;</span>shared_ptr<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;&gt;</span> outputs) <span style="color:#66d9ef">override</span>;
</span></span></code></pre></div><h2 id="32-opå‚æ•°è‡ªå®šä¹‰">3.2 Opå‚æ•°è‡ªå®šä¹‰</h2>
<p>æ¯”å¦‚å¯¹äºCPUä¸Šçš„LinearOpï¼Œéœ€è¦<code>in_features</code>ã€<code>out_features</code>å’Œ<code>has_bias</code>ä¸‰ä¸ªå‚æ•°ã€‚é‚£ä¹ˆå¯ä»¥åœ¨4.1è‡ªåŠ¨ç”Ÿæˆçš„classä¸­åŠ å…¥ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CPULinear</span> <span style="color:#66d9ef">final</span> <span style="color:#f92672">:</span> <span style="color:#66d9ef">public</span> Op {
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">private</span><span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> in_features_;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> out_features_;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">bool</span> support_bias_;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> thread_count <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>;
</span></span><span style="display:flex;"><span>    Tensor weight_;
</span></span><span style="display:flex;"><span>    Tensor bias_;
</span></span><span style="display:flex;"><span>};
</span></span></code></pre></div><p>åœ¨CPULinearCreatorä¸­åŠ å…¥ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CPULinearCreator</span> <span style="color:#f92672">:</span> <span style="color:#66d9ef">public</span> CPUBackend<span style="color:#f92672">::</span>Creator {
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">public</span><span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">virtual</span> Op <span style="color:#f92672">*</span>create(OpParam op_param, Backend <span style="color:#f92672">*</span>bn, string name, <span style="color:#66d9ef">int</span> threadCount) <span style="color:#66d9ef">const</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">int</span> in_features <span style="color:#f92672">=</span> op_param[<span style="color:#e6db74">&#34;in_features&#34;</span>];
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">int</span> out_features <span style="color:#f92672">=</span> op_param[<span style="color:#e6db74">&#34;out_features&#34;</span>];
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">int</span> bias <span style="color:#f92672">=</span> op_param[<span style="color:#e6db74">&#34;bias&#34;</span>];
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">new</span> <span style="color:#a6e22e">CPULinear</span>(bn, name, in_features, out_features, (<span style="color:#66d9ef">bool</span>)bias, threadCount);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>};
</span></span></code></pre></div><p>è¯·æ³¨æ„ï¼ŒOpParamæ˜¯ä¸€ä¸ªstring-float mapã€‚</p>
<h2 id="33-é‡è½½å‡½æ•°">3.3 é‡è½½å‡½æ•°</h2>
<p>è¯»è€…éœ€è¦è‡ªè¡Œå®ç°reshapeï¼Œexecuteï¼Œloadï¼Œfreeå‡½æ•°ï¼Œè§†æƒ…å†µé‡è½½setUpå‡½æ•°ã€‚
ä»¥Linear Opä¸ºä¾‹ï¼Œreshapeå‡½æ•°å°±ä¼šé€šè¿‡<code>in_features_</code>å˜é‡æ¥æ£€æŸ¥è¾“å…¥çš„Tensorçš„ç»´åº¦æ˜¯å¦æ­£ç¡®ï¼Œç„¶åå¯¹output Tensoråš<code>outputs[0]-&gt;reshape(inputs[0]-&gt;batch(), inputs[0]-&gt;head(), inputs[0]-&gt;sequence(), out_features_)</code>
åœ¨loadå‡½æ•°ä¸­ï¼Œå®ç°Weightå’ŒBiasçš„åŠ è½½ã€‚
åœ¨executeå‡½æ•°ä¸­ï¼Œå…·ä½“å®ç°çŸ©é˜µä¹˜æ³•ç­‰è®¡ç®—æ“ä½œã€‚
åœ¨freeå‡½æ•°ä¸­é‡Šæ”¾Weightå’ŒBiasã€‚</p>
<h2 id="34-opæ˜¯å¦‚ä½•è¢«æ³¨å†Œå’Œåˆ›å»ºçš„">3.4 Opæ˜¯å¦‚ä½•è¢«æ³¨å†Œå’Œåˆ›å»ºçš„ï¼Ÿ</h2>
<p>åœ¨å®šä¹‰å®ŒæˆOpåï¼Œè¯»è€…è¿˜éœ€è¦æŠŠè¯¥Opæ³¨å†Œåˆ°ç›¸åº”çš„Backendä¸­ï¼Œä»¥åŠå°†OpæŠ½è±¡æˆLayerã€‚</p>
<h3 id="341-åœ¨backendä¸­æ³¨å†Œop">3.4.1 åœ¨Backendä¸­æ³¨å†ŒOp</h3>
<p>ä»¥CPU Backendä¸ºä¾‹ï¼Œè¯»è€…éœ€è¦å†CPUBackendæ–‡ä»¶ä¸­åŠ å…¥<code>addCreator(LINEAR, (CPUBackend::Creator *)(new CPULinearCreator()));</code>å¦‚æœè¿™æ˜¯ä¸€ä¸ªæ–°çš„ç®—å­ï¼Œè¯»è€…è¿˜éœ€è¦åœ¨<code>OpDefined</code>æ–‡ä»¶ä¸­åŠ å…¥æ–°Opçš„Enumé¡¹ã€‚</p>
<h3 id="342-åœ¨layerhppä¸­åŠ å…¥å¯¹åº”çš„op-layer">3.4.2 åœ¨Layer.hppä¸­åŠ å…¥å¯¹åº”çš„Op Layer</h3>
<p>å¦‚Linear Layer:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Linear</span> <span style="color:#66d9ef">final</span> <span style="color:#f92672">:</span> <span style="color:#66d9ef">public</span> Layer {
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">public</span><span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">explicit</span> Linear(<span style="color:#66d9ef">int</span> in_features, <span style="color:#66d9ef">int</span> out_features, <span style="color:#66d9ef">bool</span> bias, std<span style="color:#f92672">::</span>string name) {
</span></span><span style="display:flex;"><span>        param_[<span style="color:#e6db74">&#34;in_features&#34;</span>] <span style="color:#f92672">=</span> in_features;
</span></span><span style="display:flex;"><span>        param_[<span style="color:#e6db74">&#34;out_features&#34;</span>] <span style="color:#f92672">=</span> out_features;
</span></span><span style="display:flex;"><span>        param_[<span style="color:#e6db74">&#34;bias&#34;</span>] <span style="color:#f92672">=</span> (<span style="color:#66d9ef">float</span>)bias;
</span></span><span style="display:flex;"><span>        init(std<span style="color:#f92672">::</span>move(name), OpType<span style="color:#f92672">::</span>LINEAR);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    Tensor <span style="color:#f92672">&amp;</span><span style="color:#66d9ef">operator</span>()(Tensor <span style="color:#f92672">&amp;</span>input) {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#a6e22e">_1I1O_OP</span>(input);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>};
</span></span></code></pre></div><p><strong>å…¶ä¸­ï¼Œåœ¨æ„é€ å‡½æ•°ä¸­çš„</strong><code>**init()**</code><strong>å‡½æ•°å¹¶æ²¡æœ‰åˆ›å»ºè¿™ä¸ªLinearç®—å­ã€‚å®ƒåªæ˜¯è´Ÿè´£ç»™è¿™ä¸ªLinearæŒ‡æ´¾äº†Backendã€‚</strong>
çœŸæ­£çš„ç®—å­åˆ›å»ºè¿˜æ˜¯åœ¨<code>INIT_OP()</code>å‡½æ•°ä¸­ã€‚åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œå®ƒä¼šé€šè¿‡<code>backend_-&gt;opCreate(param_, name_);</code>æ¥åˆ›å»ºç®—å­ã€‚</p>
<h1 id="4-tokenizer">4. Tokenizer</h1>
<p>mllmæä¾›äº†åŸºç¡€çš„Tokenizeræ”¯æŒï¼Œç›®å‰æ”¯æŒBPEå’ŒUnigramä¸¤ç§åˆ†è¯ç®—æ³•ã€‚</p>
<h1 id="5-å¦‚ä½•å¯¹æ–°æ¨¡å‹è¿›è¡Œæ”¯æŒ">5. å¦‚ä½•å¯¹æ–°æ¨¡å‹è¿›è¡Œæ”¯æŒ</h1>
<p>åœ¨mllmä¸­ï¼Œå¯¹æ¨¡å‹ç»„ä»¶ï¼ˆmodelã€Tokenizerã€Configurationï¼‰çš„å®šä¹‰å’ŒHuggingFace Transformeråº“ä¸­çš„å®šä¹‰æ–¹æ³•åŸºæœ¬ä¸€è‡´ã€‚ä»¥æ”¯æŒQWen0.5Bæ¨¡å‹ä¸ºä¾‹ï¼Œéœ€è¦ç¼–å†™ä¸‰ä¸ªæ–‡ä»¶ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>configuration_qwen.cpp
</span></span><span style="display:flex;"><span>modeling_qwen.cpp
</span></span><span style="display:flex;"><span>tokenization_qwen.cpp
</span></span></code></pre></div><p>å…¶ä¸­<code>configuration_qwen.cpp</code>å®šä¹‰äº†Qwen LLMçš„å„ç±»å‚æ•°ï¼Œå¦‚Headæ•°é‡ï¼Œhidden dimç­‰ã€‚<code>modeling_qwen.cpp</code>å®šä¹‰äº†Qwen LLMç½‘ç»œã€‚<code>tokenization_qwen.cpp</code>åŒ…å«äº†å°†å¥å­è½¬åŒ–ä¸ºTokençš„é¢„å¤„ç†è¡Œä¸ºã€‚</p>
<h2 id="51-ç”Ÿæˆmllmæ”¯æŒçš„vocabå’Œæ¨¡å‹å‚æ•°">5.1 ç”Ÿæˆmllmæ”¯æŒçš„vocabå’Œæ¨¡å‹å‚æ•°</h2>
<h3 id="511-æ¨¡å‹è½¬æ¢">5.1.1 æ¨¡å‹è½¬æ¢</h3>
<p>ä½¿ç”¨mllmæä¾›çš„Converterå®ç”¨å·¥å…·æ¥è¿›è¡Œè½¬æ¢ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>cd tools/convertor
</span></span><span style="display:flex;"><span>pip install -r ./requirements.txt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># for one file pytorch model</span>
</span></span><span style="display:flex;"><span>python convert.py --input_model<span style="color:#f92672">=</span>model.pth --output_model<span style="color:#f92672">=</span>model.mllm --type<span style="color:#f92672">=</span>torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># for multi-file pytorch model</span>
</span></span><span style="display:flex;"><span>python convert.py --input_model<span style="color:#f92672">=</span>pytorch_model.bin.index.json --output_model<span style="color:#f92672">=</span>model.mllm --type<span style="color:#f92672">=</span>torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># for one file safetensor model</span>
</span></span><span style="display:flex;"><span>python convert.py --input_model<span style="color:#f92672">=</span>model.bin --output_model<span style="color:#f92672">=</span>model.mllm --type<span style="color:#f92672">=</span>safetensor
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># for multi-file safetensor model</span>
</span></span><span style="display:flex;"><span>python convert.py --input_model<span style="color:#f92672">=</span>model.safetensors.index.json --output_model<span style="color:#f92672">=</span>model.mllm --type<span style="color:#f92672">=</span>safetensor
</span></span></code></pre></div><h3 id="512-vocabè½¬æ¢">5.1.2 Vocabè½¬æ¢</h3>
<p>ä½¿ç”¨mllmæä¾›çš„Converterå®ç”¨å·¥å…·æ¥è¿›è¡Œè½¬æ¢ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>cd tools/convertor
</span></span><span style="display:flex;"><span>python vocab.py --input_file<span style="color:#f92672">=</span>tokenizer.json --output_file<span style="color:#f92672">=</span>vocab.mllm --type<span style="color:#f92672">=</span>Unigram
</span></span></code></pre></div><h3 id="513-é‡åŒ–">5.1.3 é‡åŒ–</h3>
<p>mllmæä¾›äº†é‡åŒ–å·¥å…·ï¼Œè¯¥å·¥å…·æ”¯æŒ4 bitså’Œ6 bitsæ•´æ•°é‡åŒ–ï¼Œä½ å¯ä»¥ä½¿ç”¨ä¸‹è¿°æŒ‡ä»¤æ¥å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œé‡åŒ–</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>cd bin
</span></span><span style="display:flex;"><span>./quantize model.mllm model_q4_0.mllm Q4_K
</span></span></code></pre></div><h2 id="52-configuration">5.2 Configuration</h2>
<p>è®¾ç½®æ–‡ä»¶é‡Œé¢ä¸»è¦å®ç°ä¸¤ä¸ªç±»ï¼Œä¸€ä¸ªæ˜¯<code>QWenNameConfig</code>ï¼Œä¸€ä¸ªæ˜¯<code>QWenConfig</code>ï¼Œå…¶ä¸­<code>QWenNameConfig</code>åŒ…å«<code>QWenConfig</code>ã€‚åœ¨ä¸€ä¸ªmllmæ¨¡å‹å‚æ•°æ–‡ä»¶ä¸­ï¼Œæ¨¡å‹å‚æ•°æ˜¯ä»¥key-valueå¯¹çš„å½¢å¼ç»Ÿä¸€èµ·æ¥çš„ã€‚<code>QWenNameConfig</code>çš„ç›®çš„å°±æ˜¯ç»™å‡ºæ¯ä¸ªå‚æ•°çš„åç§°ï¼Œä»¥ä¾¿äºmllmæ¡†æ¶ç´¢å¼•åˆ°æ­£ç¡®çš„æ¨¡å‹å‚æ•°ã€‚</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">QWenNameConfig</span> <span style="color:#f92672">:</span> <span style="color:#66d9ef">public</span> TransformerNameConfig {
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">public</span><span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">/**
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">     * @brief QWen2 following the hugging face naming method
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">     *
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">     * @param type RoPEType
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">     */</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">void</span> init(RoPEType type <span style="color:#f92672">=</span> RoPEType<span style="color:#f92672">::</span>HFHUBROPE) {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">switch</span> (type) {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">case</span> RoPEType<span style="color:#f92672">::</span>HFHUBROPE: {
</span></span><span style="display:flex;"><span>            blk_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;model.layers.&#34;</span>;
</span></span><span style="display:flex;"><span>            _attn_base_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;self_attn.&#34;</span>;
</span></span><span style="display:flex;"><span>            _ffn_base_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;mlp.&#34;</span>;
</span></span><span style="display:flex;"><span>            _q_proj_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;q_proj&#34;</span>;
</span></span><span style="display:flex;"><span>            _k_proj_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;k_proj&#34;</span>;
</span></span><span style="display:flex;"><span>            _v_proj_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;v_proj&#34;</span>;
</span></span><span style="display:flex;"><span>            _o_proj_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;o_proj&#34;</span>;
</span></span><span style="display:flex;"><span>            _gate_proj_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;gate_proj&#34;</span>;
</span></span><span style="display:flex;"><span>            _up_proj_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;up_proj&#34;</span>;
</span></span><span style="display:flex;"><span>            _down_proj_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;down_proj&#34;</span>;
</span></span><span style="display:flex;"><span>            _attn_norm_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;input_layernorm&#34;</span>;
</span></span><span style="display:flex;"><span>            _ffn_norm_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;post_attention_layernorm&#34;</span>;
</span></span><span style="display:flex;"><span>            token_embd_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;model.embed_tokens&#34;</span>;
</span></span><span style="display:flex;"><span>            post_norm_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;model.norm&#34;</span>;
</span></span><span style="display:flex;"><span>            lm_head_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;lm_head&#34;</span>;
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span>;
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>        ...
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    std<span style="color:#f92672">::</span>string blk_name;
</span></span><span style="display:flex;"><span>    std<span style="color:#f92672">::</span>string token_embd_name;
</span></span><span style="display:flex;"><span>    std<span style="color:#f92672">::</span>string post_norm_name;
</span></span><span style="display:flex;"><span>    std<span style="color:#f92672">::</span>string lm_head_name;
</span></span><span style="display:flex;"><span>    std<span style="color:#f92672">::</span>string _gate_proj_name;
</span></span><span style="display:flex;"><span>};
</span></span></code></pre></div><p>åœ¨<code>QWenConfig</code>ä¸­åˆ™ä¸»è¦å®šä¹‰å„å±‚çš„è¶…å‚æ•°ï¼Œå¦‚ropeçš„thetaå€¼ã€ä¸­é—´å±‚ç»´åº¦å¤§å°ç­‰ï¼Œå¦‚ä¸‹é¢çš„ä»£ç æ‰€ç¤ºï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">QWenConfig</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">explicit</span> <span style="color:#a6e22e">QWenConfig</span>(<span style="color:#66d9ef">int</span> token_limit, string billions <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;0.5B&#34;</span>, RoPEType type <span style="color:#f92672">=</span> RoPEType<span style="color:#f92672">::</span>HFHUBROPE) <span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>        cache_limit(token_limit) {
</span></span><span style="display:flex;"><span>        ...
</span></span><span style="display:flex;"><span>    };
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">float</span> attention_dropout <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> bos_token_id <span style="color:#f92672">=</span> <span style="color:#ae81ff">151643</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> eos_token_id <span style="color:#f92672">=</span> <span style="color:#ae81ff">151643</span>;
</span></span><span style="display:flex;"><span>    std<span style="color:#f92672">::</span>string hidden_act <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;silu&#34;</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> hidden_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">1024</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">float</span> initializer_range <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.02</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> intermediate_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">2816</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> max_position_embeddings <span style="color:#f92672">=</span> <span style="color:#ae81ff">32768</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> max_window_layers <span style="color:#f92672">=</span> <span style="color:#ae81ff">21</span>;
</span></span><span style="display:flex;"><span>    std<span style="color:#f92672">::</span>string model_type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;qwen2&#34;</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> num_attention_heads <span style="color:#f92672">=</span> <span style="color:#ae81ff">16</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> num_hidden_layers <span style="color:#f92672">=</span> <span style="color:#ae81ff">24</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> num_key_value_heads <span style="color:#f92672">=</span> <span style="color:#ae81ff">16</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">double</span> rms_norm_eps <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-6</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">float</span> rope_theta <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000000.0</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> sliding_window <span style="color:#f92672">=</span> <span style="color:#ae81ff">32768</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> vocab_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">151936</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">bool</span> tie_embedding_words <span style="color:#f92672">=</span> false;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> cache_limit;
</span></span><span style="display:flex;"><span>    RoPEType RoPE_type <span style="color:#f92672">=</span> RoPEType<span style="color:#f92672">::</span>HFHUBROPE;
</span></span><span style="display:flex;"><span>    QWenNameConfig names_config;
</span></span><span style="display:flex;"><span>};
</span></span></code></pre></div><h2 id="53-tokenization">5.3 Tokenization</h2>
<p>Tokenizationæ˜¯ä¸€ä¸ªéå¸¸å®¢åˆ¶åŒ–çš„æ­¥éª¤ï¼Œæ¯ä¸ªLLMçš„Tokenizationæ–¹æ³•éƒ½ä¸å°½ç›¸åŒã€‚ä»¥QWenä¸ºä¾‹å­ï¼ŒQWenä½¿ç”¨äº†BBPEæ–¹æ³•ï¼Œé‚£ä¹ˆè¯»è€…åœ¨æ”¯æŒQWenæ¨¡å‹çš„æ—¶å€™ï¼Œå°±è¦ç»™å‡ºå®ç°äº†BBPEçš„Tokenizerã€‚mllmå†…éƒ¨å·²ç»å®ç°ä¸€ä¸ªBPEç®—æ³•ï¼Œè¯»è€…å¯ä»¥å¤ç”¨è¯¥å®ç°æ¥å®ç°è‡ªå·±çš„Tokenizerã€‚</p>
<h2 id="54-modeling">5.4 Modeling</h2>
<p>ä½¿ç”¨mllmæ¡†æ¶æä¾›çš„ç®—å­æ¥å®ç°æ¨¡å‹æ˜¯éå¸¸ç®€å•å’Œä¾¿åˆ©çš„ï¼Œç†Ÿæ‚‰Pytorchçš„è¯»è€…å¯ä»¥å¿«é€Ÿçš„ä¸Šæ‰‹mllmã€‚æœ¬æ–‡åœ¨è¿™é‡Œé»˜è®¤è¯»è€…å¯¹llama/qwen/mistralç­‰å¸¸è§LLMçš„æ¨¡å‹æœ‰ç€åŸºæœ¬çš„äº†è§£ã€‚åœ¨ä¸‹æ–‡ä¸­ï¼Œæœ¬æ–‡ä»¥Attentionæ¨¡å—ä¸ºä¾‹æ¥æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨mllmæ¥æ­å»ºæ¨¡å‹ã€‚
é¦–å…ˆï¼Œæ‰€æœ‰çš„classéœ€è¦ç»§æ‰¿<code>Module</code>çˆ¶ç±»ã€‚<code>Module</code>çˆ¶ç±»æä¾›äº†<code>Forward</code>å‡½æ•°ï¼Œè¯»è€…éœ€è¦é‡è½½è¯¥å‡½æ•°æ¥å®ç°ç›¸åº”çš„è®¡ç®—æµç¨‹ã€‚</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">QWenAttention</span> <span style="color:#66d9ef">final</span> <span style="color:#f92672">:</span> <span style="color:#66d9ef">public</span> Module ...
</span></span></code></pre></div><h3 id="541-åˆ›å»ºè¯¥moduleéœ€è¦ä½¿ç”¨çš„layers">5.4.1 åˆ›å»ºè¯¥Moduleéœ€è¦ä½¿ç”¨çš„Layers</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">QWenAttention</span> <span style="color:#66d9ef">final</span> <span style="color:#f92672">:</span> <span style="color:#66d9ef">public</span> Module {
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">public</span><span style="color:#960050;background-color:#1e0010">ï¼š</span>
</span></span><span style="display:flex;"><span>    QWenAttention() <span style="color:#f92672">=</span> <span style="color:#66d9ef">default</span>;
</span></span><span style="display:flex;"><span>    QWenAttention(<span style="color:#66d9ef">const</span> QWenConfig <span style="color:#f92672">&amp;</span>config, <span style="color:#66d9ef">const</span> QWenNameConfig <span style="color:#f92672">&amp;</span>names, <span style="color:#66d9ef">const</span> string <span style="color:#f92672">&amp;</span>base_name) {
</span></span><span style="display:flex;"><span>        hidden_size <span style="color:#f92672">=</span> config.hidden_size;
</span></span><span style="display:flex;"><span>        num_heads <span style="color:#f92672">=</span> config.num_attention_heads;
</span></span><span style="display:flex;"><span>        head_dim <span style="color:#f92672">=</span> config.hidden_size <span style="color:#f92672">/</span> num_heads;
</span></span><span style="display:flex;"><span>        num_key_value_heads <span style="color:#f92672">=</span> config.num_key_value_heads;
</span></span><span style="display:flex;"><span>        num_key_value_groups <span style="color:#f92672">=</span> num_heads <span style="color:#f92672">/</span> num_key_value_heads;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">// init layers
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        q_proj <span style="color:#f92672">=</span> Linear(hidden_size, num_heads <span style="color:#f92672">*</span> head_dim, true, base_name <span style="color:#f92672">+</span> names._q_proj_name);
</span></span><span style="display:flex;"><span>        k_proj <span style="color:#f92672">=</span> Linear(hidden_size, num_key_value_heads <span style="color:#f92672">*</span> head_dim, true, base_name <span style="color:#f92672">+</span> names._k_proj_name);
</span></span><span style="display:flex;"><span>        v_proj <span style="color:#f92672">=</span> Linear(hidden_size, num_key_value_heads <span style="color:#f92672">*</span> head_dim, true, base_name <span style="color:#f92672">+</span> names._v_proj_name);
</span></span><span style="display:flex;"><span>        o_proj <span style="color:#f92672">=</span> Linear(num_heads <span style="color:#f92672">*</span> head_dim, hidden_size, false, base_name <span style="color:#f92672">+</span> names._o_proj_name);
</span></span><span style="display:flex;"><span>        q_rope <span style="color:#f92672">=</span> RoPE(config.RoPE_type, config.rope_theta, config.max_position_embeddings, base_name <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;q_rope&#34;</span>);
</span></span><span style="display:flex;"><span>        k_rope <span style="color:#f92672">=</span> RoPE(config.RoPE_type, config.rope_theta, config.max_position_embeddings, base_name <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;k_rope&#34;</span>);
</span></span><span style="display:flex;"><span>        k_cache <span style="color:#f92672">=</span> KVCache(num_key_value_groups, config.cache_limit, base_name <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;k_cache&#34;</span>);
</span></span><span style="display:flex;"><span>        v_cache <span style="color:#f92672">=</span> KVCache(num_key_value_groups, config.cache_limit, base_name <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;v_cache&#34;</span>);
</span></span><span style="display:flex;"><span>        mask <span style="color:#f92672">=</span> Causalmask(base_name <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;mask&#34;</span>);
</span></span><span style="display:flex;"><span>        softmax <span style="color:#f92672">=</span> Softmax(DIMENSION, base_name <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;softmax&#34;</span>);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">private</span><span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> hidden_size;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> num_heads;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> head_dim;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> num_key_value_heads;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> num_key_value_groups;
</span></span><span style="display:flex;"><span>    Layer q_proj;
</span></span><span style="display:flex;"><span>    Layer k_proj;
</span></span><span style="display:flex;"><span>    Layer v_proj;
</span></span><span style="display:flex;"><span>    Layer o_proj;
</span></span><span style="display:flex;"><span>    Layer q_rope;
</span></span><span style="display:flex;"><span>    Layer k_rope;
</span></span><span style="display:flex;"><span>    Layer k_cache;
</span></span><span style="display:flex;"><span>    Layer v_cache;
</span></span><span style="display:flex;"><span>    Layer mask;
</span></span><span style="display:flex;"><span>    Layer softmax;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>ç»†å¿ƒçš„è¯»è€…å¯èƒ½å·²ç»å‘ç°äº†ï¼Œåœ¨<code>QWenAttention</code>çš„æ„é€ å‡½æ•°ä¸­ï¼Œåˆ›å»ºæ¯ä¸ªLayerçš„æ—¶å€™éƒ½åœ¨æœ€åä¸€ä¸ªå‚æ•°ä¸Šä¼ é€’äº†Layeråç§°ï¼ˆstd::string typeï¼‰ï¼Œè¿™æ˜¯å› ä¸ºmllmä¾èµ–äºLayerçš„åç§°æ¥å¯»æ‰¾è¯¥Layeræ‰€éœ€è¦çš„å‚æ•°ã€‚</p>
<h3 id="542-é‡è½½forwardå‰å‘æ¨ç†å‡½æ•°">5.4.2 é‡è½½Forwardå‰å‘æ¨ç†å‡½æ•°</h3>
<p>åˆ›å»ºå®Œäº†æ‰€æœ‰æˆ‘ä»¬éœ€è¦çš„Layersä»¥åï¼Œå°±å¯ä»¥ç¼–å†™Forwardå‡½æ•°æ¥å®šä¹‰Attentionæ¨¡å—çš„è®¡ç®—æµç¨‹ï¼ŒForwardå‡½æ•°æ¥æ”¶ä¸€ä¸ªTensor Arrayå’Œä¸€ä¸ªstd::any Arrayï¼Œè¿”å›Tensor Arrayï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span>std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;</span> Forward(std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;</span> inputs, std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>std<span style="color:#f92672">::</span>any<span style="color:#f92672">&gt;</span> args) <span style="color:#66d9ef">override</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> query_states <span style="color:#f92672">=</span> q_proj(inputs[<span style="color:#ae81ff">0</span>]);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> key_states <span style="color:#f92672">=</span> k_proj(inputs[<span style="color:#ae81ff">1</span>]);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> value_states <span style="color:#f92672">=</span> v_proj(inputs[<span style="color:#ae81ff">2</span>]);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// [batch, heads, sequence, dims]
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    query_states <span style="color:#f92672">=</span> query_states.view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, num_heads, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, head_dim);
</span></span><span style="display:flex;"><span>    key_states <span style="color:#f92672">=</span> key_states.view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, num_key_value_heads, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, head_dim);
</span></span><span style="display:flex;"><span>    value_states <span style="color:#f92672">=</span> value_states.view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, num_key_value_heads, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, head_dim);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// embedding
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    query_states <span style="color:#f92672">=</span> q_rope(query_states);
</span></span><span style="display:flex;"><span>    key_states <span style="color:#f92672">=</span> k_rope(key_states);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// kv cache
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    key_states <span style="color:#f92672">=</span> k_cache(key_states);
</span></span><span style="display:flex;"><span>    value_states <span style="color:#f92672">=</span> v_cache(value_states);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// attention weight
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">auto</span> atten_weight <span style="color:#f92672">=</span> Tensor<span style="color:#f92672">::</span>mm(query_states, key_states.transpose(Chl<span style="color:#f92672">::</span>SEQUENCE, Chl<span style="color:#f92672">::</span>DIMENSION)) <span style="color:#f92672">/</span> std<span style="color:#f92672">::</span>sqrt(head_dim);
</span></span><span style="display:flex;"><span>    atten_weight <span style="color:#f92672">=</span> mask(atten_weight);
</span></span><span style="display:flex;"><span>    atten_weight <span style="color:#f92672">=</span> softmax(atten_weight);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// attention output
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">auto</span> atten_output <span style="color:#f92672">=</span> Tensor<span style="color:#f92672">::</span>mm(atten_weight, value_states);
</span></span><span style="display:flex;"><span>    atten_output <span style="color:#f92672">=</span> atten_output.view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, head_dim <span style="color:#f92672">*</span> num_heads);
</span></span><span style="display:flex;"><span>    atten_output <span style="color:#f92672">=</span> o_proj(atten_output);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> {atten_output};
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h2 id="55-è¿è¡Œ">5.5 è¿è¡Œ</h2>
<p>å®Œæ•´çš„Qwenæ¨¡å‹å®šä¹‰ä»£ç å¯ä»¥åœ¨é™„å½•1ä¸­æ‰¾åˆ°ã€‚è¯»è€…å¯ä»¥åƒTorchä¸€æ ·è°ƒç”¨å®šä¹‰å¥½çš„æ¨¡å‹ï¼šé¦–å…ˆï¼Œåˆ›å»ºæ¨¡å‹ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span>QWenConfig <span style="color:#a6e22e">config</span>(tokens_limit, <span style="color:#e6db74">&#34;0.5B&#34;</span>, RoPEType<span style="color:#f92672">::</span>HFHUBROPE);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">auto</span> model <span style="color:#f92672">=</span> QWenForCausalLM(config);
</span></span><span style="display:flex;"><span>model.load(model_path);
</span></span></code></pre></div><p><code>module</code>classé‡è½½äº†()operatorï¼Œè¯»è€…å¯ä»¥ä½¿ç”¨<code>model({input_tensor})</code>æ¥è¿›è¡Œæ¨ç†ã€‚</p>
<h1 id="6-mllmæ¡†æ¶çš„ä¸è¶³">6. mllmæ¡†æ¶çš„ä¸è¶³</h1>
<p>è¿™é‡Œå†™çš„æœ‰ç‚¹meanï¼Œæœ¬äººä¸“ä¸šçŸ¥è¯†æµ…è–„ï¼Œåœ¨å­¦æœ¯ä¸Šæ˜¯ä¾æ‰˜ç­”è¾©ï¼Œå¯¹mllmçš„ç†è§£æ›´æ˜¯ä¸åˆ°ä½ï¼Œå¤§å®¶è½»å–·ã€‚</p>
<h2 id="61-benchmark">6.1 Benchmark</h2>
<ol>
<li>ç¼ºå°‘ç®—å­çš„Benchmark</li>
</ol>
<p>æœ¬æ–‡è®¤ä¸ºï¼Œmllmåœ¨å®ç°çš„æ—¶å€™æåŠ›çš„é¿å…ä½¿ç”¨ç¬¬ä¸‰æ–¹çš„åº“ï¼Œå› ä¸ºmllméœ€è¦è¿ç§»åˆ°ç§»åŠ¨è®¾å¤‡ä¸Šï¼Œä¸€äº›ä¸‰æ–¹åº“å¯èƒ½ä¸èƒ½æ­£å¸¸å·¥ä½œã€‚ä½†æ˜¯æ‰‹å·¥å®ç°çš„Kernelè¿˜æ˜¯éœ€è¦ä¸€ä¸ªBenchmarkæ¥å’Œç›®æ ‡å¹³å°ä¸Šæä¾›çš„ç®—å­åº“æ¥è¿›è¡Œæ€§èƒ½æ¯”è¾ƒçš„ã€‚å°±mllmç›®å‰æä¾›çš„MatMul Kernelæ¥çœ‹ï¼Œä¼¼ä¹ç¼ºå°‘Packä¼˜åŒ–å’Œ/micro Kernelçš„ä¼˜åŒ–ï¼Ÿ</p>
<ol start="2">
<li>ç¼ºå°‘prefill/decodeçš„Benchmark</li>
</ol>
<p>mllmçš„issuesä¸­ä¹Ÿæœ‰äººæåˆ°è¿‡è¿™ä¸ªé—®é¢˜ã€‚ä½œä¸ºå…·æœ‰LLMæ¨ç†èƒ½åŠ›çš„å¼•æ“ï¼Œåº”å½“æµ‹ä¸€ä¸‹è¿™ä¸¤ä¸ªåŸºæœ¬èƒ½åŠ›ã€‚</p>
<h2 id="62-å¯¹äºç§»åŠ¨ç«¯llmæ¨ç†çš„ç‰¹å®šä¼˜åŒ–">6.2 å¯¹äºç§»åŠ¨ç«¯LLMæ¨ç†çš„ç‰¹å®šä¼˜åŒ–</h2>
<ol>
<li>KV Cacheé‡åŒ–</li>
</ol>
<p>IIRCï¼Œåœ¨OPPOçš„Transformer-Lite[2]ä¸­ï¼Œç”¨åˆ°äº†KV Cacheé‡åŒ–çš„å°æŠ€å·§ã€‚è¿™å¯¹ç§»åŠ¨è®¾å¤‡æœ‰é™çš„å†…å­˜æ¥è¯´å¯èƒ½ä¼šæ›´åŠ å‹å¥½ï¼Œå½“ç„¶è¿˜éœ€è¦è€ƒé‡é‡åŒ–å¸¦æ¥çš„CPUè´Ÿè½½é—®é¢˜ã€‚</p>
<ol start="2">
<li>åŠ¨æ€å½¢çŠ¶æ¨ç†/å†…å­˜å¤ç”¨/KV Cacheæ¬ç§»ä¼˜åŒ–</li>
</ol>
<p>ç›®å‰mllmæ˜¯æ²¡æœ‰åšå†…å­˜å¤ç”¨çš„ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ç¬¦å·æ¨ç†æ–¹æ³•æ¥åšåŠ¨æ€å½¢çŠ¶çš„æ”¯æŒè¿›è€Œä¾¿äºæ±‚è§£ä¸‹ä¸€è½®çš„å†…å­˜ä½¿ç”¨æƒ…å†µã€‚æˆ–è®¸å¯ä»¥è€ƒè™‘ä¸€ä¸‹PageAttention[3]çš„Tensorç®¡ç†æ–¹æ³•æˆ–è€…[2]ä¸­çš„KV Cacheè§„åˆ’æ–¹æ³•æ¥è¿›ä¸€æ­¥å‡å°‘å†…å­˜çš„æ¬ç§»ã€‚</p>
<ol start="3">
<li>å¼‚æ„ç®—åŠ›</li>
</ol>
<p>å¯ä»¥è€ƒè™‘æŠŠå½¢çŠ¶æ¨ç†ï¼ˆCPUï¼‰å’Œè®¡ç®—ï¼ˆGPU/NPUï¼‰å¹¶è¡Œæ‰§è¡Œèµ·æ¥ã€‚æˆ–è€…æ˜¯6.2.4ä¸­æåˆ°çš„å†…å®¹ä¸è®¡ç®—å¹¶è¡Œèµ·æ¥ã€‚</p>
<ol start="4">
<li>å¯¹æ¨¡å‹å‚æ•°çš„Lazy Fetchå’ŒPre Fetch</li>
</ol>
<p>ç›®å‰ï¼Œmllmä¼šæŠŠå‚æ•°ä¸€æ¬¡æ€§çš„è¯»å…¥å†…å­˜ï¼Ÿè€ƒè™‘åˆ°ç§»åŠ¨è®¾å¤‡çš„å†…å­˜æœ‰é™ï¼Œå¯ä»¥åœ¨åˆé€‚çš„æ—¶æœºæå‰ä»å¤–å­˜ä¸Šé¢„å–è€Œä¸æ˜¯å…¨æ•°è½½å…¥ã€‚</p>
<h2 id="63-æ˜“ç”¨æ€§">6.3 æ˜“ç”¨æ€§</h2>
<ol>
<li>æ¨¡å‹ç»“æ„éœ€è¦æ‰‹åŠ¨ç¼–å†™ä¸”æ— æ³•ä¿å­˜</li>
</ol>
<p>ç›®å‰ï¼Œmllmçš„æ¨¡å‹ç»“æ„è¿˜æ˜¯éœ€è¦åœ¨C++æ–‡ä»¶ä¸­è¿›è¡Œæ˜¾ç¤ºçš„æ‰‹åŠ¨å®šä¹‰ã€‚æˆ–è®¸å¯ä»¥è€ƒè™‘åˆ›å»ºè‡ªå·±çš„è®¡ç®—å›¾å’Œç®—å­æè¿°æ–¹å¼ï¼Œä½¿ç”¨flatbuffersæ¥å­˜å‚¨è®¡ç®—å›¾ã€‚</p>
<ol start="2">
<li>å¦‚æœè¦å¾ˆå¥½çš„ä½¿ç”¨æ‰€æœ‰çš„ç®—åŠ›ï¼Œå¯èƒ½è¿˜æ˜¯éœ€è¦å®Œå–„çš„è®¡ç®—å›¾æœºåˆ¶ï¼Œè¿™æ ·ä¾¿äºä¼˜åŒ–åˆ†æã€‚</li>
<li>å°è¯•å¼•å…¥ä¸‰æ–¹æ˜“ç”¨çš„åº“å¦‚icuç­‰æ¥å¼¥è¡¥C++ utf-8å¤„ç†èƒ½åŠ›çš„ä¸è¶³ã€‚</li>
</ol>
<hr>
<p><strong>Refï¼š</strong>
[1] mllm, <a href="https://github.com/UbiquitousLearning/mllm">https://github.com/UbiquitousLearning/mllm</a>
[2] transformer-lite, <a href="https://arxiv.org/abs/2403.20041">https://arxiv.org/abs/2403.20041</a>
[3] PageAttention, <a href="https://arxiv.org/abs/2309.06180">https://arxiv.org/abs/2309.06180</a></p>
<h1 id="a1-qwenæ¨¡å‹å®šä¹‰">A1. Qwenæ¨¡å‹å®šä¹‰</h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#75715e">#ifndef MODELING_QWEN_HPP
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">#define MODELING_QWEN_HPP
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#include</span> <span style="color:#75715e">&#34;Backend.hpp&#34;</span><span style="color:#75715e">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">#include</span> <span style="color:#75715e">&#34;Layer.hpp&#34;</span><span style="color:#75715e">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">#include</span> <span style="color:#75715e">&#34;Module.hpp&#34;</span><span style="color:#75715e">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">#include</span> <span style="color:#75715e">&#34;Tensor.hpp&#34;</span><span style="color:#75715e">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">#include</span> <span style="color:#75715e">&#34;configuration_qwen.hpp&#34;</span><span style="color:#75715e">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">#include</span> <span style="color:#75715e">&lt;cmath&gt;</span><span style="color:#75715e">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">using</span> <span style="color:#66d9ef">namespace</span> mllm;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">// Copied from GemmaMLP with Gemma-&gt;Qwen and using silu
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">QWenMLP</span> <span style="color:#66d9ef">final</span> <span style="color:#f92672">:</span> <span style="color:#66d9ef">public</span> Module {
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">public</span><span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>    QWenMLP() <span style="color:#f92672">=</span> <span style="color:#66d9ef">default</span>;
</span></span><span style="display:flex;"><span>    QWenMLP(<span style="color:#66d9ef">int</span> hidden_size, <span style="color:#66d9ef">int</span> intermediate_size, <span style="color:#66d9ef">const</span> QWenNameConfig <span style="color:#f92672">&amp;</span>names, <span style="color:#66d9ef">const</span> std<span style="color:#f92672">::</span>string <span style="color:#f92672">&amp;</span>base_name) {
</span></span><span style="display:flex;"><span>        gate_proj <span style="color:#f92672">=</span> Linear(hidden_size, intermediate_size, false, base_name <span style="color:#f92672">+</span> names._gate_proj_name);
</span></span><span style="display:flex;"><span>        silu <span style="color:#f92672">=</span> SiLU(base_name <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;act&#34;</span>);
</span></span><span style="display:flex;"><span>        up_proj <span style="color:#f92672">=</span> Linear(hidden_size, intermediate_size, false, base_name <span style="color:#f92672">+</span> names._up_proj_name);
</span></span><span style="display:flex;"><span>        down_proj <span style="color:#f92672">=</span> Linear(intermediate_size, hidden_size, false, base_name <span style="color:#f92672">+</span> names._down_proj_name);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;</span> Forward(std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;</span> inputs, std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>std<span style="color:#f92672">::</span>any<span style="color:#f92672">&gt;</span> args) <span style="color:#66d9ef">override</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">auto</span> x <span style="color:#f92672">=</span> gate_proj(inputs[<span style="color:#ae81ff">0</span>]);
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> silu(x);
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">auto</span> y <span style="color:#f92672">=</span> up_proj(inputs[<span style="color:#ae81ff">0</span>]);
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">*</span> y;
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> down_proj(x);
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> {x};
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">private</span><span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>    Layer gate_proj;
</span></span><span style="display:flex;"><span>    Layer up_proj;
</span></span><span style="display:flex;"><span>    Layer down_proj;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    Layer silu;
</span></span><span style="display:flex;"><span>};
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">// Copied from GemmaAttention with Gemma-&gt;Qwen and using SWA
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">QWenAttention</span> <span style="color:#66d9ef">final</span> <span style="color:#f92672">:</span> <span style="color:#66d9ef">public</span> Module {
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">public</span><span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>    QWenAttention() <span style="color:#f92672">=</span> <span style="color:#66d9ef">default</span>;
</span></span><span style="display:flex;"><span>    QWenAttention(<span style="color:#66d9ef">const</span> QWenConfig <span style="color:#f92672">&amp;</span>config, <span style="color:#66d9ef">const</span> QWenNameConfig <span style="color:#f92672">&amp;</span>names, <span style="color:#66d9ef">const</span> string <span style="color:#f92672">&amp;</span>base_name) {
</span></span><span style="display:flex;"><span>        hidden_size <span style="color:#f92672">=</span> config.hidden_size;
</span></span><span style="display:flex;"><span>        num_heads <span style="color:#f92672">=</span> config.num_attention_heads;
</span></span><span style="display:flex;"><span>        head_dim <span style="color:#f92672">=</span> config.hidden_size <span style="color:#f92672">/</span> num_heads;
</span></span><span style="display:flex;"><span>        num_key_value_heads <span style="color:#f92672">=</span> config.num_key_value_heads;
</span></span><span style="display:flex;"><span>        num_key_value_groups <span style="color:#f92672">=</span> num_heads <span style="color:#f92672">/</span> num_key_value_heads;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">// init layers
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        q_proj <span style="color:#f92672">=</span> Linear(hidden_size, num_heads <span style="color:#f92672">*</span> head_dim, true, base_name <span style="color:#f92672">+</span> names._q_proj_name);
</span></span><span style="display:flex;"><span>        k_proj <span style="color:#f92672">=</span> Linear(hidden_size, num_key_value_heads <span style="color:#f92672">*</span> head_dim, true, base_name <span style="color:#f92672">+</span> names._k_proj_name);
</span></span><span style="display:flex;"><span>        v_proj <span style="color:#f92672">=</span> Linear(hidden_size, num_key_value_heads <span style="color:#f92672">*</span> head_dim, true, base_name <span style="color:#f92672">+</span> names._v_proj_name);
</span></span><span style="display:flex;"><span>        o_proj <span style="color:#f92672">=</span> Linear(num_heads <span style="color:#f92672">*</span> head_dim, hidden_size, false, base_name <span style="color:#f92672">+</span> names._o_proj_name);
</span></span><span style="display:flex;"><span>        q_rope <span style="color:#f92672">=</span> RoPE(config.RoPE_type, config.rope_theta, config.max_position_embeddings, base_name <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;q_rope&#34;</span>);
</span></span><span style="display:flex;"><span>        k_rope <span style="color:#f92672">=</span> RoPE(config.RoPE_type, config.rope_theta, config.max_position_embeddings, base_name <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;k_rope&#34;</span>);
</span></span><span style="display:flex;"><span>        k_cache <span style="color:#f92672">=</span> KVCache(num_key_value_groups, config.cache_limit, base_name <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;k_cache&#34;</span>);
</span></span><span style="display:flex;"><span>        v_cache <span style="color:#f92672">=</span> KVCache(num_key_value_groups, config.cache_limit, base_name <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;v_cache&#34;</span>);
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">// mask = SlidingWindowMask(config.sliding_window, base_name + &#34;mask&#34;);
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        mask <span style="color:#f92672">=</span> Causalmask(base_name <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;mask&#34;</span>);
</span></span><span style="display:flex;"><span>        softmax <span style="color:#f92672">=</span> Softmax(DIMENSION, base_name <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;softmax&#34;</span>);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;</span> Forward(std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;</span> inputs, std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>std<span style="color:#f92672">::</span>any<span style="color:#f92672">&gt;</span> args) <span style="color:#66d9ef">override</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">auto</span> query_states <span style="color:#f92672">=</span> q_proj(inputs[<span style="color:#ae81ff">0</span>]);
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">auto</span> key_states <span style="color:#f92672">=</span> k_proj(inputs[<span style="color:#ae81ff">1</span>]);
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">auto</span> value_states <span style="color:#f92672">=</span> v_proj(inputs[<span style="color:#ae81ff">2</span>]);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">// [batch, heads, sequence, dims]
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        query_states <span style="color:#f92672">=</span> query_states.view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, num_heads, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, head_dim);
</span></span><span style="display:flex;"><span>        key_states <span style="color:#f92672">=</span> key_states.view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, num_key_value_heads, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, head_dim);
</span></span><span style="display:flex;"><span>        value_states <span style="color:#f92672">=</span> value_states.view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, num_key_value_heads, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, head_dim);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">// embedding
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        query_states <span style="color:#f92672">=</span> q_rope(query_states);
</span></span><span style="display:flex;"><span>        key_states <span style="color:#f92672">=</span> k_rope(key_states);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">// kv cache
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        key_states <span style="color:#f92672">=</span> k_cache(key_states);
</span></span><span style="display:flex;"><span>        value_states <span style="color:#f92672">=</span> v_cache(value_states);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">// attention weight
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        <span style="color:#66d9ef">auto</span> atten_weight <span style="color:#f92672">=</span> Tensor<span style="color:#f92672">::</span>mm(query_states, key_states.transpose(Chl<span style="color:#f92672">::</span>SEQUENCE, Chl<span style="color:#f92672">::</span>DIMENSION)) <span style="color:#f92672">/</span> std<span style="color:#f92672">::</span>sqrt(head_dim);
</span></span><span style="display:flex;"><span>        atten_weight <span style="color:#f92672">=</span> mask(atten_weight);
</span></span><span style="display:flex;"><span>        atten_weight <span style="color:#f92672">=</span> softmax(atten_weight);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">// attention output
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        <span style="color:#66d9ef">auto</span> atten_output <span style="color:#f92672">=</span> Tensor<span style="color:#f92672">::</span>mm(atten_weight, value_states);
</span></span><span style="display:flex;"><span>        atten_output <span style="color:#f92672">=</span> atten_output.view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, head_dim <span style="color:#f92672">*</span> num_heads);
</span></span><span style="display:flex;"><span>        atten_output <span style="color:#f92672">=</span> o_proj(atten_output);
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> {atten_output};
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">private</span><span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> hidden_size;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> num_heads;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> head_dim;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> num_key_value_heads;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> num_key_value_groups;
</span></span><span style="display:flex;"><span>    Layer q_proj;
</span></span><span style="display:flex;"><span>    Layer k_proj;
</span></span><span style="display:flex;"><span>    Layer v_proj;
</span></span><span style="display:flex;"><span>    Layer o_proj;
</span></span><span style="display:flex;"><span>    Layer q_rope;
</span></span><span style="display:flex;"><span>    Layer k_rope;
</span></span><span style="display:flex;"><span>    Layer k_cache;
</span></span><span style="display:flex;"><span>    Layer v_cache;
</span></span><span style="display:flex;"><span>    Layer mask;
</span></span><span style="display:flex;"><span>    Layer softmax;
</span></span><span style="display:flex;"><span>};
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">// Copied from GemmaDecoder with Gemma-&gt;Qwen and set RmsNorm(without add_unit_offset)
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">QWenDecoder</span> <span style="color:#66d9ef">final</span> <span style="color:#f92672">:</span> <span style="color:#66d9ef">public</span> Module {
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">public</span><span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>    QWenDecoder() <span style="color:#f92672">=</span> <span style="color:#66d9ef">default</span>;
</span></span><span style="display:flex;"><span>    QWenDecoder(<span style="color:#66d9ef">const</span> QWenConfig <span style="color:#f92672">&amp;</span>config, <span style="color:#66d9ef">const</span> QWenNameConfig <span style="color:#f92672">&amp;</span>names, <span style="color:#66d9ef">const</span> string <span style="color:#f92672">&amp;</span>base_name) {
</span></span><span style="display:flex;"><span>        self_atten <span style="color:#f92672">=</span> QWenAttention(config, names, base_name <span style="color:#f92672">+</span> names._attn_base_name);
</span></span><span style="display:flex;"><span>        mlp <span style="color:#f92672">=</span> QWenMLP(config.hidden_size, config.intermediate_size, names, base_name <span style="color:#f92672">+</span> names._ffn_base_name);
</span></span><span style="display:flex;"><span>        input_layernorm <span style="color:#f92672">=</span> RMSNorm(config.hidden_size, config.rms_norm_eps, base_name <span style="color:#f92672">+</span> names._attn_norm_name);
</span></span><span style="display:flex;"><span>        post_attention_layernorm <span style="color:#f92672">=</span> RMSNorm(config.hidden_size, config.rms_norm_eps, base_name <span style="color:#f92672">+</span> names._ffn_norm_name);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;</span> Forward(std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;</span> inputs, std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>std<span style="color:#f92672">::</span>any<span style="color:#f92672">&gt;</span> args) <span style="color:#66d9ef">override</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">auto</span> x <span style="color:#f92672">=</span> input_layernorm(inputs[<span style="color:#ae81ff">0</span>]);
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self_atten({x, x, x})[<span style="color:#ae81ff">0</span>];
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">auto</span> tmp <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> inputs[<span style="color:#ae81ff">0</span>];
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> post_attention_layernorm(tmp);
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> mlp({x})[<span style="color:#ae81ff">0</span>];
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> tmp;
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> {x};
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">private</span><span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>    QWenAttention self_atten;
</span></span><span style="display:flex;"><span>    QWenMLP mlp;
</span></span><span style="display:flex;"><span>    Layer input_layernorm;
</span></span><span style="display:flex;"><span>    Layer post_attention_layernorm;
</span></span><span style="display:flex;"><span>};
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">// Copied from GemmaModel with Gemma-&gt;Qwen and set RmsNorm(without add_unit_offset)
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">QWenModel</span> <span style="color:#66d9ef">final</span> <span style="color:#f92672">:</span> <span style="color:#66d9ef">public</span> Module {
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">public</span><span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>    QWenModel() <span style="color:#f92672">=</span> <span style="color:#66d9ef">default</span>;
</span></span><span style="display:flex;"><span>    QWenModel(<span style="color:#66d9ef">const</span> QWenConfig <span style="color:#f92672">&amp;</span>config, <span style="color:#66d9ef">const</span> QWenNameConfig <span style="color:#f92672">&amp;</span>names, <span style="color:#66d9ef">const</span> string <span style="color:#f92672">&amp;</span>base_name) {
</span></span><span style="display:flex;"><span>        blocks <span style="color:#f92672">=</span> List<span style="color:#f92672">&lt;</span>QWenDecoder<span style="color:#f92672">&gt;</span>(config.num_hidden_layers, config, names, base_name);
</span></span><span style="display:flex;"><span>        norm <span style="color:#f92672">=</span> RMSNorm(config.hidden_size, config.rms_norm_eps, names.post_norm_name);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;</span> Forward(std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;</span> inputs, std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>std<span style="color:#f92672">::</span>any<span style="color:#f92672">&gt;</span> args) <span style="color:#66d9ef">override</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">auto</span> x <span style="color:#f92672">=</span> inputs[<span style="color:#ae81ff">0</span>];
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">auto</span> <span style="color:#f92672">&amp;</span>block : blocks) {
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> block({x})[<span style="color:#ae81ff">0</span>];
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> norm(x);
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> {x};
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">private</span><span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>    std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>QWenDecoder<span style="color:#f92672">&gt;</span> blocks;
</span></span><span style="display:flex;"><span>    Layer norm;
</span></span><span style="display:flex;"><span>};
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">QWenForCausalLM</span> <span style="color:#66d9ef">final</span> <span style="color:#f92672">:</span> <span style="color:#66d9ef">public</span> Module {
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">public</span><span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>    QWenForCausalLM(QWenConfig <span style="color:#f92672">&amp;</span>config) {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">auto</span> names <span style="color:#f92672">=</span> config.names_config;
</span></span><span style="display:flex;"><span>        hidden_size <span style="color:#f92672">=</span> config.hidden_size;
</span></span><span style="display:flex;"><span>        tie_embedding_words <span style="color:#f92672">=</span> config.tie_embedding_words;
</span></span><span style="display:flex;"><span>        embedding <span style="color:#f92672">=</span> Embedding(config.vocab_size, config.hidden_size, names.token_embd_name);
</span></span><span style="display:flex;"><span>        model <span style="color:#f92672">=</span> QWenModel(config, names, names.blk_name);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">// FIXME Qwen-0.5 use tied embedding
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        <span style="color:#75715e">// Others use nn.Linear()
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        <span style="color:#66d9ef">if</span> (tie_embedding_words) {
</span></span><span style="display:flex;"><span>            lm_head <span style="color:#f92672">=</span> Parameter(<span style="color:#ae81ff">1</span>, config.vocab_size, <span style="color:#ae81ff">1</span>, config.hidden_size, names.token_embd_name <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;.weight&#34;</span>);
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;</span> Forward(std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>Tensor<span style="color:#f92672">&gt;</span> inputs, std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>std<span style="color:#f92672">::</span>any<span style="color:#f92672">&gt;</span> args) <span style="color:#66d9ef">override</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">auto</span> x <span style="color:#f92672">=</span> embedding(inputs[<span style="color:#ae81ff">0</span>]);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">// go through model
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        <span style="color:#66d9ef">auto</span> outputs <span style="color:#f92672">=</span> model({x})[<span style="color:#ae81ff">0</span>];
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (tie_embedding_words) {
</span></span><span style="display:flex;"><span>            outputs <span style="color:#f92672">=</span> Tensor<span style="color:#f92672">::</span>mm(outputs, lm_head().transpose(Chl<span style="color:#f92672">::</span>SEQUENCE, Chl<span style="color:#f92672">::</span>DIMENSION));
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> {outputs};
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">private</span><span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> hidden_size;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">bool</span> tie_embedding_words;
</span></span><span style="display:flex;"><span>    Layer embedding;
</span></span><span style="display:flex;"><span>    Parameter lm_head;
</span></span><span style="display:flex;"><span>    QWenModel model;
</span></span><span style="display:flex;"><span>};
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#endif </span><span style="color:#75715e">//! MODELING_QWEN_HPP
</span></span></span></code></pre></div>
		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/keep-moving-forward/tags/llm-server/" rel="tag">LLM Server</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/keep-moving-forward/tags/llm/" rel="tag">LLM</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>

<div class="authorbox clearfix">
	<figure class="authorbox__avatar">
		<img alt="chenghua.wang avatar" src="/keep-moving-forward/img/Cornell_box.png" class="avatar" height="90" width="90">
	</figure>
	<div class="authorbox__header">
		<span class="authorbox__name">About chenghua.wang</span>
	</div>
	<div class="authorbox__description">
		Currently working on AI&amp;Sys, CV (low-level) and LLM topics.
	</div>
</div>

<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/keep-moving-forward/tech/x86_avx_sgemm_6x16/" rel="prev">
			<span class="pager__subtitle">Â«&thinsp;Previous</span>
			<p class="pager__title">ã€æ–½å·¥ä¸­ã€‘6xKx16 SGEMM Kernel on X86-AVX</p>
		</a>
	</div>
</nav>


			</div>
			<aside class="sidebar"><div class="widget-search widget">
	<form class="widget-search__form" role="search" method="get" action="https://google.com/search">
		<label>
			<input class="widget-search__field" type="search" placeholder="SEARCHâ€¦" value="" name="q" aria-label="SEARCHâ€¦">
		</label>
		<input class="widget-search__submit" type="submit" value="Search">
		<input type="hidden" name="don&#39;t use this search" value="don&#39;t use this searchhttps://chenghuawang.github.io/keep-moving-forward/">
	</form>
</div>
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/tech/mllm-qwen/">mllmæ¡†æ¶æµ…æ-ä»¥QWen0.5Bä¸ºä¾‹</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/papers/mlsys2024-qmoe/">âœ…[Oct 2023] QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/papers/prompt_cache/">âœ…[April 2024] Prompt Cache: Modular Attention Reuse for Low-Latency Inference</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/papers/transformer-lite/">âœ…[Mar 2024] Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/papers/awq/">âœ…[April 2024] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/tech/x86_avx_sgemm_6x16/">ã€æ–½å·¥ä¸­ã€‘6xKx16 SGEMM Kernel on X86-AVX</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/tech/introduction_mldistri/">æµ…ææœºå™¨å­¦ä¹ ä¸­çš„å¹¶è¡Œæ¨¡å‹å’Œè‡ªåŠ¨å¹¶è¡Œæ–¹æ³•</a></li>
			<li class="widget__item"><a class="widget__link" href="/keep-moving-forward/tech/cuda_nsight_system/">CUDA: NSight System</a></li>
		</ul>
	</div>
</div>
<div class="widget-taglist widget">
	<h4 class="widget__title">Tags</h4>
	<div class="widget__content">
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/cuda/" title="CUDA">CUDA (1)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/distributed-system/" title="Distributed System">Distributed System (1)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/edge/" title="Edge">Edge (1)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/kernel/" title="Kernel">Kernel (1)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/lecture/" title="Lecture">Lecture (5)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/llm/" title="LLM">LLM (5)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/llm-cache-optimize/" title="LLM Cache Optimize">LLM Cache Optimize (1)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/llm-server/" title="LLM Server">LLM Server (5)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/mlsys-2024/" title="MLSys 2024">MLSys 2024 (3)</a>
		<a class="widget-taglist__link widget__link btn" href="/keep-moving-forward/tags/quantization/" title="Quantization">Quantization (2)</a>
	</div>
</div>
<div class="toc__block_div">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
		<nav id="TableOfContents">
  <ul>
    <li><a href="#1-ç®€ä»‹">1. ç®€ä»‹</a></li>
    <li><a href="#2-æ¡†æ¶æ‰§è¡Œæµç¨‹">2. æ¡†æ¶æ‰§è¡Œæµç¨‹</a>
      <ul>
        <li><a href="#21-ä»¥ä¸¤å±‚linearå±‚è¿è¡Œä¸ºä¾‹">2.1 ä»¥ä¸¤å±‚Linearå±‚è¿è¡Œä¸ºä¾‹</a>
          <ul>
            <li><a href="#211-åŠ è½½å‚æ•°">2.1.1 åŠ è½½å‚æ•°</a></li>
            <li><a href="#212moduleçš„operatoræ˜¯å¦‚ä½•è°ƒç”¨forwardå‡½æ•°çš„">2.1.2Moduleçš„Operator()æ˜¯å¦‚ä½•è°ƒç”¨Forwardå‡½æ•°çš„ï¼Ÿ</a></li>
            <li><a href="#213-linearå±‚çš„æ‰§è¡Œ">2.1.3 Linearå±‚çš„æ‰§è¡Œ</a></li>
          </ul>
        </li>
        <li><a href="#22-æ€»ç»“">2.2 æ€»ç»“</a></li>
      </ul>
    </li>
    <li><a href="#3-å¦‚ä½•ç¼–å†™opä¸è‡ªå®šä¹‰layer">3. å¦‚ä½•ç¼–å†™Opä¸è‡ªå®šä¹‰Layer</a>
      <ul>
        <li><a href="#31-æ–°å¢å¯¹åº”backendçš„opæ–‡ä»¶">3.1 æ–°å¢å¯¹åº”Backendçš„Opæ–‡ä»¶</a></li>
        <li><a href="#32-opå‚æ•°è‡ªå®šä¹‰">3.2 Opå‚æ•°è‡ªå®šä¹‰</a></li>
        <li><a href="#33-é‡è½½å‡½æ•°">3.3 é‡è½½å‡½æ•°</a></li>
        <li><a href="#34-opæ˜¯å¦‚ä½•è¢«æ³¨å†Œå’Œåˆ›å»ºçš„">3.4 Opæ˜¯å¦‚ä½•è¢«æ³¨å†Œå’Œåˆ›å»ºçš„ï¼Ÿ</a>
          <ul>
            <li><a href="#341-åœ¨backendä¸­æ³¨å†Œop">3.4.1 åœ¨Backendä¸­æ³¨å†ŒOp</a></li>
            <li><a href="#342-åœ¨layerhppä¸­åŠ å…¥å¯¹åº”çš„op-layer">3.4.2 åœ¨Layer.hppä¸­åŠ å…¥å¯¹åº”çš„Op Layer</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#4-tokenizer">4. Tokenizer</a></li>
    <li><a href="#5-å¦‚ä½•å¯¹æ–°æ¨¡å‹è¿›è¡Œæ”¯æŒ">5. å¦‚ä½•å¯¹æ–°æ¨¡å‹è¿›è¡Œæ”¯æŒ</a>
      <ul>
        <li><a href="#51-ç”Ÿæˆmllmæ”¯æŒçš„vocabå’Œæ¨¡å‹å‚æ•°">5.1 ç”Ÿæˆmllmæ”¯æŒçš„vocabå’Œæ¨¡å‹å‚æ•°</a>
          <ul>
            <li><a href="#511-æ¨¡å‹è½¬æ¢">5.1.1 æ¨¡å‹è½¬æ¢</a></li>
            <li><a href="#512-vocabè½¬æ¢">5.1.2 Vocabè½¬æ¢</a></li>
            <li><a href="#513-é‡åŒ–">5.1.3 é‡åŒ–</a></li>
          </ul>
        </li>
        <li><a href="#52-configuration">5.2 Configuration</a></li>
        <li><a href="#53-tokenization">5.3 Tokenization</a></li>
        <li><a href="#54-modeling">5.4 Modeling</a>
          <ul>
            <li><a href="#541-åˆ›å»ºè¯¥moduleéœ€è¦ä½¿ç”¨çš„layers">5.4.1 åˆ›å»ºè¯¥Moduleéœ€è¦ä½¿ç”¨çš„Layers</a></li>
            <li><a href="#542-é‡è½½forwardå‰å‘æ¨ç†å‡½æ•°">5.4.2 é‡è½½Forwardå‰å‘æ¨ç†å‡½æ•°</a></li>
          </ul>
        </li>
        <li><a href="#55-è¿è¡Œ">5.5 è¿è¡Œ</a></li>
      </ul>
    </li>
    <li><a href="#6-mllmæ¡†æ¶çš„ä¸è¶³">6. mllmæ¡†æ¶çš„ä¸è¶³</a>
      <ul>
        <li><a href="#61-benchmark">6.1 Benchmark</a></li>
        <li><a href="#62-å¯¹äºç§»åŠ¨ç«¯llmæ¨ç†çš„ç‰¹å®šä¼˜åŒ–">6.2 å¯¹äºç§»åŠ¨ç«¯LLMæ¨ç†çš„ç‰¹å®šä¼˜åŒ–</a></li>
        <li><a href="#63-æ˜“ç”¨æ€§">6.3 æ˜“ç”¨æ€§</a></li>
      </ul>
    </li>
    <li><a href="#a1-qwenæ¨¡å‹å®šä¹‰">A1. Qwenæ¨¡å‹å®šä¹‰</a></li>
  </ul>
</nav>
	</div>
</div>

</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 chenghua.wang.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/keep-moving-forward/js/menu.js"></script>




<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { fonts: ["TeX"] }
  });
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async>
</script>
</body>
</html>