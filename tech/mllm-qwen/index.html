<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>mllmæ¡†æ¶æµ…æ(ä¸€)-ä»¥QWen0.5Bä¸ºä¾‹ | Ubios Home</title>
<meta name="keywords" content="LLM Server, LLM">
<meta name="description" content="ä»¥Qwen0.5Bä¸ºä¾‹è§£æmllmçš„åŸºæœ¬å®ç°ï¼ŒCPU Backend">
<meta name="author" content="chenghua.Wang">
<link rel="canonical" href="https://chenghuawang.github.io/keep-moving-forward/tech/mllm-qwen/">
<link crossorigin="anonymous" href="/keep-moving-forward/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://chenghuawang.github.io/keep-moving-forward/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://chenghuawang.github.io/keep-moving-forward/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://chenghuawang.github.io/keep-moving-forward/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://chenghuawang.github.io/keep-moving-forward/apple-touch-icon.png">
<link rel="mask-icon" href="https://chenghuawang.github.io/keep-moving-forward/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://chenghuawang.github.io/keep-moving-forward/tech/mllm-qwen/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>



  

<meta property="og:title" content="mllmæ¡†æ¶æµ…æ(ä¸€)-ä»¥QWen0.5Bä¸ºä¾‹" />
<meta property="og:description" content="ä»¥Qwen0.5Bä¸ºä¾‹è§£æmllmçš„åŸºæœ¬å®ç°ï¼ŒCPU Backend" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chenghuawang.github.io/keep-moving-forward/tech/mllm-qwen/" /><meta property="article:section" content="tech" />
<meta property="article:published_time" content="2024-06-28T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-06-28T00:00:00+00:00" />



<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="mllmæ¡†æ¶æµ…æ(ä¸€)-ä»¥QWen0.5Bä¸ºä¾‹"/>
<meta name="twitter:description" content="ä»¥Qwen0.5Bä¸ºä¾‹è§£æmllmçš„åŸºæœ¬å®ç°ï¼ŒCPU Backend"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Technique",
      "item": "https://chenghuawang.github.io/keep-moving-forward/tech/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "mllmæ¡†æ¶æµ…æ(ä¸€)-ä»¥QWen0.5Bä¸ºä¾‹",
      "item": "https://chenghuawang.github.io/keep-moving-forward/tech/mllm-qwen/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "mllmæ¡†æ¶æµ…æ(ä¸€)-ä»¥QWen0.5Bä¸ºä¾‹",
  "name": "mllmæ¡†æ¶æµ…æ(ä¸€)-ä»¥QWen0.5Bä¸ºä¾‹",
  "description": "ä»¥Qwen0.5Bä¸ºä¾‹è§£æmllmçš„åŸºæœ¬å®ç°ï¼ŒCPU Backend",
  "keywords": [
    "LLM Server", "LLM"
  ],
  "articleBody": " ç¬”è€…æœ€è¿‘åœ¨åšä¸€äº›mllmç›¸å…³çš„å·¥ä½œï¼Œä¹¦å†™æ­¤æ–‡å¯¹mllmæ¡†æ¶è¿›è¡Œæ¢³ç†æ€»ç»“ï¼Œå®šæœ‰ä¸å°‘çº°æ¼ï¼Œè¯·è¯»è€…ç«‹å³æŒ‡å‡ºï¼Œè°¢è°¢ã€‚mllmç›®å‰åœ¨åšä¸€äº›å…¶ä»–å·¥ä½œï¼Œè¿™ç¯‡æ–‡ç« çš„ä¹¦å†™æ—¶é—´ä¸ºå‘å¸ƒæ—¶é—´ã€‚åœ¨mllmçš„å…¶ä»–å·¥ä½œåˆå¹¶è¿›ä¸»ä»“åº“åï¼Œæœ¬æ–‡è¿˜ä¼šè¿›ä¸€æ­¥çš„è·Ÿè¿›ã€‚è¯»è€…è¯·æ³¨æ„æœ¬æ–‡çš„æ—¶æ•ˆæ€§ã€‚\n1. ç®€ä»‹ mllmæ˜¯ä¸€æ¬¾é€‚ç”¨äºç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¾å¤‡çš„å¿«é€Ÿã€è½»é‡çš„å¤šæ¨¡æ€LLMæ¨ç†å¼•æ“ã€‚\nå®Œå…¨çš„C/C++å®ç°ï¼Œæ— ç¬¬ä¸‰æ–¹ä¾èµ– é’ˆå¯¹fuyu-8Bç­‰å¤šæ¨¡æ€LLMè¿›è¡Œäº†ä¼˜åŒ– æ”¯æŒARM NEONå’ŒX86 AVX2å‘é‡æŒ‡ä»¤ æ”¯æŒ4 bitså’Œ6 bitsæ•´æ•°é‡åŒ– æœ¬æ–‡å°†æ›´å¤šçš„ä»¥å·¥ç¨‹çš„è§†è§’æ¥è§£æmllmæ¡†æ¶ï¼Œåœ¨è¡Œæ–‡è¿‡ç¨‹ä¸­ï¼Œæœ¬æ–‡ä¼šå°†mllmä¸å…¶ä»–æ¡†æ¶çš„è®¾è®¡æ–¹æ³•åšå¯¹æ¯”ã€‚æ¥ä¸‹æ¥ï¼Œæœ¬æ–‡å°†ä¼šç”¨é¡¹ç›®ç»„ç»‡ç»“æ„ã€æ¡†æ¶æ‰§è¡Œæµç¨‹ã€è‡ªå®šä¹‰Op/Layerã€Tokenizerå’Œå¦‚ä½•æ”¯æŒæ–°æ¨¡å‹äº”ä¸ªç« èŠ‚æ¥è¯¦ç»†æè¿°mllmæ¡†æ¶çš„å„é¡¹ç‰¹æ€§å’Œæ€»ä½“ç»“æ„ã€‚è¯»è€…å¯ä»¥æŠŠè¯¥æ–‡ç« åšmllmçš„ä½¿ç”¨æ–‡æ¡£ã€‚åœ¨æœ€åï¼Œæœ¬æ–‡å°†ä¼šæŒ‡å‡ºmllmçš„ä¸è¶³ä¹‹å¤„å’Œå¯ä»¥å°è¯•è·Ÿè¿›çš„å·¥ä½œã€‚\nåœ¨å¼€å§‹æ­£å¼è§£æmllmä¹‹å‰ï¼Œè¯»è€…å¯ä»¥å…ˆcloneä¸‹mllmçš„ä»£ç åº“ï¼Œä»¥ä¾¿äºè·Ÿè¿›åˆ†ææµç¨‹ã€‚mllmä¸ä¾èµ–äºgit submoduleï¼Œé¡¹ç›®é…ç½®èµ·æ¥å¾ˆæ–¹ä¾¿ï¼Œç›®å‰mllmå¯ä»¥åœ¨linuxä¸Šä½¿ç”¨Clang/GCCç¼–è¯‘å™¨è¿›è¡Œç¼–è¯‘ã€‚ç›®å‰mllmæ”¯æŒçš„ç›®æ ‡è®¾å¤‡ä½“ç³»ç»“æ„æ˜¯X86å’ŒArmã€‚\ngit clone https://github.com/UbiquitousLearning/mllm mllmå›¢é˜Ÿå°†æ‰€æœ‰LLMç›¸å…³çš„vocabæ–‡ä»¶éƒ½æ”¾åœ¨äº†gitä»“åº“ä¸­ï¼ˆè¿™ä¸ªå…¶å®å¯ä»¥ç§»åŠ¨åˆ°HuggingFaceçš„ä»“åº“ä¸Šï¼‰ï¼ŒLLMé‡åŒ–åçš„æ¨¡å‹æ–‡ä»¶éƒ½å­˜å‚¨åœ¨HuggingFaceä¸Šï¼Œè¯»è€…å¯ä»¥åœ¨https://huggingface.co/mllmTeamä¸Šæ‰¾åˆ°mllmæä¾›çš„æ¨¡å‹æ–‡ä»¶ã€‚\n2. æ¡†æ¶æ‰§è¡Œæµç¨‹ 2.1 ä»¥ä¸¤å±‚Linearå±‚è¿è¡Œä¸ºä¾‹ é¦–å…ˆï¼Œè€ƒè™‘ä¸‹é¢çš„ä»£ç ï¼Œå®šä¹‰äº†ä¸¤ä¸ªLinear Layersï¼Œå¹¶ä¸”è¾“å…¥$X$é€šè¿‡ä¸¤ä¸ªLinear Layersæ¥å¾—åˆ°è¾“å‡ºï¼š\nclass TwoLinear final : public Module { public: TwoLinear() = default; TwoLinear() { linear1 = Linear(in_f, out_f, /*bias*/true, \"linear1\"); linear2 = Linear(out_f, out_f, /*bias*/true, \"linear2\"); } std::vector\u003cTensor\u003e Forward(std::vector\u003cTensor\u003e inputs, std::vector\u003cstd::any\u003e args) override { x = inputs[0]; x = linear1(x); x = linear2(x); return x; } private: Layer linear1; Layer linear2; } TwoLinear tl; 2.1.1 åŠ è½½å‚æ•° è¯»è€…å¯ä»¥ä½¿ç”¨ tl.load(path)æ¥åŠ è½½å‚æ•°ã€‚é‚£ä¹ˆmllmæ˜¯å¦‚ä½•å®ç°å‚æ•°åŠ è½½çš„å‘¢ï¼Ÿåœ¨loadå‡½æ•°ä¸­ï¼Œmllmä¼šåˆ›å»ºä¸€ä¸ªParamLoaderï¼Œè¿™ä¸ªParamLoaderæ˜¯Staticçš„ï¼Œåœ¨å…¨å±€å¯ä»¥è®¿é—®ã€‚ç„¶åmllmä¼šè®¾ç½®å¦ä¸€ä¸ªå…¨å±€å‚æ•°doLoadä¸ºTrueï¼Œè¿›è€Œè¿›å…¥æ¨ç†æµç¨‹operator()(tmps, tmpt);ã€‚åœ¨æ¨ç†æµç¨‹ä¸­ï¼Œè¦æ˜¯æ‰§è¡Œå±‚å‘ç°doLoadä¸ºTrueï¼Œé‚£ä¹ˆå°±æ‰§è¡Œæ¯ä¸ªç®—å­å†…å®šä¹‰å¥½çš„loadæŒ‡ä»¤ï¼Œè€Œä¸æ˜¯æ‰§è¡Œæ¯ä¸ªç®—å­çš„åŸæœ¬é€»è¾‘ã€‚ loadçš„æ‰§è¡Œåœ¨Layer.hppæ–‡ä»¶çš„INIT_OP()ä¸­ã€‚\n2.1.2Moduleçš„Operator()æ˜¯å¦‚ä½•è°ƒç”¨Forwardå‡½æ•°çš„ï¼Ÿ å¯¹äºå¸¸è§çš„INPUT_TENSORç±»å‹çš„Tensorï¼Œmllmé¦–å…ˆä¼šè®¾ç½®è¿™ä¸ªTensorçš„ç±»å‹ä¸ºTENSOR_STATIC_INITï¼Œè¿›è¡Œä¸€éForwardæ¨ç†ï¼›ç¬¬ä¸€éForwardæ¨ç†å®Œæ¯•ä»¥åå†æŠŠTensorçš„ç±»å‹è®¾ä¸ºTENSOR_STATIC_READYï¼Œç„¶åè¿›è¡Œç¬¬äºŒéForwardæ¨ç†ã€‚\nif (inputs[0].ttype() == TensorType::INPUT_TENSOR) { for (auto \u0026input : inputs) { input.setTtype(TensorType::NORMAL_TENSOR); input.status() = TENSOR_STATIC_INIT; if(input.batch() == 0){ Tensor::gph_[input.name()] = input; } } tensor_status = TENSOR_STATIC_INIT; Forward(inputs, anyArgs); for (auto \u0026input : inputs) { input.status() = TENSOR_STATIC_READY; } tensor_status = TENSOR_STATIC_READY; return Forward(inputs, anyArgs); } ç¬¬ä¸€æ¬¡Forwardæ¨ç†çš„ç›®çš„æ˜¯è°ƒç”¨Opå®šä¹‰çš„Reshapeå’ŒSetUpå‡½æ•°ï¼ŒReshapeå‡½æ•°ä¼šæ¨ç†å‡ºè¿™ä¸€æ¬¡æ¨¡å‹æ¨ç†çš„è¿‡ç¨‹ä¸­æ¯ä¸ªTensorçš„å½¢çŠ¶å¤§å°ã€‚SetUpå‡½æ•°ä¼šå¯¹Opéœ€è¦è¾“å‡ºçš„Tensoråšå†…å­˜çš„ç”³è¯·ã€‚ ç¬¬äºŒæ¬¡Forwardæ¨ç†æ‰æ˜¯çœŸæ­£çš„è®¡ç®—ã€‚\n2.1.3 Linearå±‚çš„æ‰§è¡Œ æ¯ä¸ªLayeråœ¨å®ç°çš„æ—¶å€™éƒ½ä¼šé‡è½½operator()ï¼Œæ¯”å¦‚linear layerçš„operator()å‡½æ•°å¦‚ä¸‹ï¼š\nTensor \u0026operator()(Tensor \u0026input) { return _1I1O_OP(input); } å…¶ä¸­ï¼Œ_1I1O_OPè¡¨ç¤ºçš„æ„æ€æ˜¯ï¼Œè¿™æ˜¯éœ€è¦ä½¿ç”¨1ä¸ªè¾“å…¥å’Œ1ä¸ªè¾“å‡ºçš„å‡½æ•°æ¥å¤„ç†è¿™ä¸ªç®—å­ã€‚mllmè¿˜æä¾›äº†è®¸å¤šç±»ä¼¼äº_1I1O_OPçš„å‡½æ•°æ¥å¤„ç†ä¸åŒçš„ç®—å­ã€‚\n2.2 æ€»ç»“ å¤§ä½“æ¥è¯´ï¼Œmllmä½¿ç”¨äº†ç±»ä¼¼äºçŠ¶æ€æœºçš„å‚æ•°æ¥è®¾ç½®äº†å½“å‰æ¨ç†è¿‡ç¨‹çš„è¿è¡ŒçŠ¶æ€ã€‚æ¯ä¸€æ¬¡éƒ½æ˜¯é€šè¿‡Forwardå‡½æ•°æ¥è¿›è¡Œå…¨æ¨¡å‹çš„éå†ï¼Œåœ¨Opçš„æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œç”¨è¿™äº›è®¾å®šçš„å‚æ•°æ¥åŒºåˆ†æ¯æ¬¡Opéœ€è¦è¡¨ç°çš„è¡Œä¸ºã€‚\n3. å¦‚ä½•ç¼–å†™Opä¸è‡ªå®šä¹‰Layer 3.1 æ–°å¢å¯¹åº”Backendçš„Opæ–‡ä»¶ mllmæä¾›äº†src/backends/new_op.pyå®ç”¨å·¥å…·æ¥å¸®åŠ©åˆ›å»ºOp Classã€‚è¯¥æ–‡ä»¶ä¼šå¸®åŠ©è¯»è€…åˆ›å»ºä¸‹è¿°åŸºæœ¬å‡½æ•°ï¼š\nErrorCode reshape(vector\u003cshared_ptr\u003cTensor\u003e\u003e inputs, vector\u003cshared_ptr\u003cTensor\u003e\u003e outputs) override; ErrorCode execute(vector\u003cshared_ptr\u003cTensor\u003e\u003e inputs, vector\u003cshared_ptr\u003cTensor\u003e\u003e outputs) override; ErrorCode load(AbstructLoader \u0026loader) override; ErrorCode free(vector\u003cshared_ptr\u003cTensor\u003e\u003e inputs, vector\u003cshared_ptr\u003cTensor\u003e\u003e outputs) override; ErrorCode setUp(vector\u003cshared_ptr\u003cTensor\u003e\u003e inputs, vector\u003cshared_ptr\u003cTensor\u003e\u003e outputs) override; 3.2 Opå‚æ•°è‡ªå®šä¹‰ æ¯”å¦‚å¯¹äºCPUä¸Šçš„LinearOpï¼Œéœ€è¦in_featuresã€out_featureså’Œhas_biasä¸‰ä¸ªå‚æ•°ã€‚é‚£ä¹ˆå¯ä»¥åœ¨3.1è‡ªåŠ¨ç”Ÿæˆçš„classä¸­åŠ å…¥ï¼š\nclass CPULinear final : public Op { ... private: int in_features_; int out_features_; bool support_bias_; int thread_count = 4; Tensor weight_; Tensor bias_; }; åœ¨CPULinearCreatorä¸­åŠ å…¥ï¼š\nclass CPULinearCreator : public CPUBackend::Creator { public: virtual Op *create(OpParam op_param, Backend *bn, string name, int threadCount) const { int in_features = op_param[\"in_features\"]; int out_features = op_param[\"out_features\"]; int bias = op_param[\"bias\"]; return new CPULinear(bn, name, in_features, out_features, (bool)bias, threadCount); } }; è¯·æ³¨æ„ï¼ŒOpParamæ˜¯ä¸€ä¸ªstring-float mapã€‚\n3.3 é‡è½½å‡½æ•° è¯»è€…éœ€è¦è‡ªè¡Œå®ç°reshapeï¼Œexecuteï¼Œloadï¼Œfreeå‡½æ•°ï¼Œè§†æƒ…å†µé‡è½½setUpå‡½æ•°ã€‚ ä»¥Linear Opä¸ºä¾‹ï¼Œreshapeå‡½æ•°å°±ä¼šé€šè¿‡in_features_å˜é‡æ¥æ£€æŸ¥è¾“å…¥çš„Tensorçš„ç»´åº¦æ˜¯å¦æ­£ç¡®ï¼Œç„¶åå¯¹output Tensoråšoutputs[0]-\u003ereshape(inputs[0]-\u003ebatch(), inputs[0]-\u003ehead(), inputs[0]-\u003esequence(), out_features_)\nåœ¨loadå‡½æ•°ä¸­ï¼Œå®ç°Weightå’ŒBiasçš„åŠ è½½ã€‚\nåœ¨executeå‡½æ•°ä¸­ï¼Œå…·ä½“å®ç°çŸ©é˜µä¹˜æ³•ç­‰è®¡ç®—æ“ä½œã€‚\nåœ¨freeå‡½æ•°ä¸­é‡Šæ”¾Weightå’ŒBiasã€‚\n3.4 Opæ˜¯å¦‚ä½•è¢«æ³¨å†Œå’Œåˆ›å»ºçš„ï¼Ÿ åœ¨å®šä¹‰å®ŒæˆOpåï¼Œè¯»è€…è¿˜éœ€è¦æŠŠè¯¥Opæ³¨å†Œåˆ°ç›¸åº”çš„Backendä¸­ï¼Œä»¥åŠå°†OpæŠ½è±¡æˆLayerã€‚\n3.4.1 åœ¨Backendä¸­æ³¨å†ŒOp ä»¥CPU Backendä¸ºä¾‹ï¼Œè¯»è€…éœ€è¦å†CPUBackendæ–‡ä»¶ä¸­åŠ å…¥addCreator(LINEAR, (CPUBackend::Creator *)(new CPULinearCreator()));\nå¦‚æœè¿™æ˜¯ä¸€ä¸ªæ–°çš„ç®—å­ï¼Œè¯»è€…è¿˜éœ€è¦åœ¨OpDefinedæ–‡ä»¶ä¸­åŠ å…¥æ–°Opçš„Enumé¡¹ã€‚\n3.4.2 åœ¨Layer.hppä¸­åŠ å…¥å¯¹åº”çš„Op Layer å¦‚Linear Layer:\nclass Linear final : public Layer { public: explicit Linear(int in_features, int out_features, bool bias, std::string name) { param_[\"in_features\"] = in_features; param_[\"out_features\"] = out_features; param_[\"bias\"] = (float)bias; init(std::move(name), OpType::LINEAR); } Tensor \u0026operator()(Tensor \u0026input) { return _1I1O_OP(input); } }; å…¶ä¸­ï¼Œåœ¨æ„é€ å‡½æ•°ä¸­çš„**init()**å‡½æ•°å¹¶æ²¡æœ‰åˆ›å»ºè¿™ä¸ªLinearç®—å­ã€‚å®ƒåªæ˜¯è´Ÿè´£ç»™è¿™ä¸ªLinearæŒ‡æ´¾äº†Backendã€‚ çœŸæ­£çš„ç®—å­åˆ›å»ºè¿˜æ˜¯åœ¨INIT_OP()å‡½æ•°ä¸­ã€‚åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œå®ƒä¼šé€šè¿‡backend_-\u003eopCreate(param_, name_);æ¥åˆ›å»ºç®—å­ã€‚\n4. Tokenizer mllmæä¾›äº†åŸºç¡€çš„Tokenizeræ”¯æŒï¼Œç›®å‰æ”¯æŒBPEå’ŒUnigramä¸¤ç§åˆ†è¯ç®—æ³•ã€‚\n5. å¦‚ä½•å¯¹æ–°æ¨¡å‹è¿›è¡Œæ”¯æŒ åœ¨mllmä¸­ï¼Œå¯¹æ¨¡å‹ç»„ä»¶ï¼ˆmodelã€Tokenizerã€Configurationï¼‰çš„å®šä¹‰å’ŒHuggingFace Transformeråº“ä¸­çš„å®šä¹‰æ–¹æ³•åŸºæœ¬ä¸€è‡´ã€‚ä»¥æ”¯æŒQWen0.5Bæ¨¡å‹ä¸ºä¾‹ï¼Œéœ€è¦ç¼–å†™ä¸‰ä¸ªæ–‡ä»¶ï¼š\nconfiguration_qwen.cpp modeling_qwen.cpp tokenization_qwen.cpp å…¶ä¸­configuration_qwen.cppå®šä¹‰äº†Qwen LLMçš„å„ç±»å‚æ•°ï¼Œå¦‚Headæ•°é‡ï¼Œhidden dimç­‰ã€‚modeling_qwen.cppå®šä¹‰äº†Qwen LLMç½‘ç»œã€‚tokenization_qwen.cppåŒ…å«äº†å°†å¥å­è½¬åŒ–ä¸ºTokençš„é¢„å¤„ç†è¡Œä¸ºã€‚\n5.1 ç”Ÿæˆmllmæ”¯æŒçš„vocabå’Œæ¨¡å‹å‚æ•° 5.1.1 æ¨¡å‹è½¬æ¢ ä½¿ç”¨mllmæä¾›çš„Converterå®ç”¨å·¥å…·æ¥è¿›è¡Œè½¬æ¢ï¼š\ncd tools/convertor pip install -r ./requirements.txt # for one file pytorch model python convert.py --input_model=model.pth --output_model=model.mllm --type=torch # for multi-file pytorch model python convert.py --input_model=pytorch_model.bin.index.json --output_model=model.mllm --type=torch # for one file safetensor model python convert.py --input_model=model.bin --output_model=model.mllm --type=safetensor # for multi-file safetensor model python convert.py --input_model=model.safetensors.index.json --output_model=model.mllm --type=safetensor 5.1.2 Vocabè½¬æ¢ ä½¿ç”¨mllmæä¾›çš„Converterå®ç”¨å·¥å…·æ¥è¿›è¡Œè½¬æ¢ï¼š\ncd tools/convertor python vocab.py --input_file=tokenizer.json --output_file=vocab.mllm --type=Unigram 5.1.3 é‡åŒ– mllmæä¾›äº†é‡åŒ–å·¥å…·ï¼Œè¯¥å·¥å…·æ”¯æŒ4 bitså’Œ6 bitsæ•´æ•°é‡åŒ–ï¼Œä½ å¯ä»¥ä½¿ç”¨ä¸‹è¿°æŒ‡ä»¤æ¥å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œé‡åŒ–\ncd bin ./quantize model.mllm model_q4_0.mllm Q4_K 5.2 Configuration è®¾ç½®æ–‡ä»¶é‡Œé¢ä¸»è¦å®ç°ä¸¤ä¸ªç±»ï¼Œä¸€ä¸ªæ˜¯QWenNameConfigï¼Œä¸€ä¸ªæ˜¯QWenConfigï¼Œå…¶ä¸­QWenNameConfigåŒ…å«QWenConfigã€‚åœ¨ä¸€ä¸ªmllmæ¨¡å‹å‚æ•°æ–‡ä»¶ä¸­ï¼Œæ¨¡å‹å‚æ•°æ˜¯ä»¥key-valueå¯¹çš„å½¢å¼ç»Ÿä¸€èµ·æ¥çš„ã€‚QWenNameConfigçš„ç›®çš„å°±æ˜¯ç»™å‡ºæ¯ä¸ªå‚æ•°çš„åç§°ï¼Œä»¥ä¾¿äºmllmæ¡†æ¶ç´¢å¼•åˆ°æ­£ç¡®çš„æ¨¡å‹å‚æ•°ã€‚\nclass QWenNameConfig : public TransformerNameConfig { public: /** * @brief QWen2 following the hugging face naming method * * @param type RoPEType */ void init(RoPEType type = RoPEType::HFHUBROPE) { switch (type) { case RoPEType::HFHUBROPE: { blk_name = \"model.layers.\"; _attn_base_name = \"self_attn.\"; _ffn_base_name = \"mlp.\"; _q_proj_name = \"q_proj\"; _k_proj_name = \"k_proj\"; _v_proj_name = \"v_proj\"; _o_proj_name = \"o_proj\"; _gate_proj_name = \"gate_proj\"; _up_proj_name = \"up_proj\"; _down_proj_name = \"down_proj\"; _attn_norm_name = \"input_layernorm\"; _ffn_norm_name = \"post_attention_layernorm\"; token_embd_name = \"model.embed_tokens\"; post_norm_name = \"model.norm\"; lm_head_name = \"lm_head\"; break; } ... } } std::string blk_name; std::string token_embd_name; std::string post_norm_name; std::string lm_head_name; std::string _gate_proj_name; }; åœ¨QWenConfigä¸­åˆ™ä¸»è¦å®šä¹‰å„å±‚çš„è¶…å‚æ•°ï¼Œå¦‚ropeçš„thetaå€¼ã€ä¸­é—´å±‚ç»´åº¦å¤§å°ç­‰ï¼Œå¦‚ä¸‹é¢çš„ä»£ç æ‰€ç¤ºï¼š\nstruct QWenConfig { explicit QWenConfig(int token_limit, string billions = \"0.5B\", RoPEType type = RoPEType::HFHUBROPE) : cache_limit(token_limit) { ... }; float attention_dropout = 0.0; int bos_token_id = 151643; int eos_token_id = 151643; std::string hidden_act = \"silu\"; int hidden_size = 1024; float initializer_range = 0.02; int intermediate_size = 2816; int max_position_embeddings = 32768; int max_window_layers = 21; std::string model_type = \"qwen2\"; int num_attention_heads = 16; int num_hidden_layers = 24; int num_key_value_heads = 16; double rms_norm_eps = 1e-6; float rope_theta = 1000000.0; int sliding_window = 32768; int vocab_size = 151936; bool tie_embedding_words = false; int cache_limit; RoPEType RoPE_type = RoPEType::HFHUBROPE; QWenNameConfig names_config; }; 5.3 Tokenization Tokenizationæ˜¯ä¸€ä¸ªéå¸¸å®¢åˆ¶åŒ–çš„æ­¥éª¤ï¼Œæ¯ä¸ªLLMçš„Tokenizationæ–¹æ³•éƒ½ä¸å°½ç›¸åŒã€‚ä»¥QWenä¸ºä¾‹å­ï¼ŒQWenä½¿ç”¨äº†BBPEæ–¹æ³•ï¼Œé‚£ä¹ˆè¯»è€…åœ¨æ”¯æŒQWenæ¨¡å‹çš„æ—¶å€™ï¼Œå°±è¦ç»™å‡ºå®ç°äº†BBPEçš„Tokenizerã€‚mllmå†…éƒ¨å·²ç»å®ç°ä¸€ä¸ªBPEç®—æ³•ï¼Œè¯»è€…å¯ä»¥å¤ç”¨è¯¥å®ç°æ¥å®ç°è‡ªå·±çš„Tokenizerã€‚\n5.4 Modeling ä½¿ç”¨mllmæ¡†æ¶æä¾›çš„ç®—å­æ¥å®ç°æ¨¡å‹æ˜¯éå¸¸ç®€å•å’Œä¾¿åˆ©çš„ï¼Œç†Ÿæ‚‰Pytorchçš„è¯»è€…å¯ä»¥å¿«é€Ÿçš„ä¸Šæ‰‹mllmã€‚æœ¬æ–‡åœ¨è¿™é‡Œé»˜è®¤è¯»è€…å¯¹llama/qwen/mistralç­‰å¸¸è§LLMçš„æ¨¡å‹æœ‰ç€åŸºæœ¬çš„äº†è§£ã€‚åœ¨ä¸‹æ–‡ä¸­ï¼Œæœ¬æ–‡ä»¥Attentionæ¨¡å—ä¸ºä¾‹æ¥æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨mllmæ¥æ­å»ºæ¨¡å‹ã€‚ é¦–å…ˆï¼Œæ‰€æœ‰çš„classéœ€è¦ç»§æ‰¿Moduleçˆ¶ç±»ã€‚Moduleçˆ¶ç±»æä¾›äº†Forwardå‡½æ•°ï¼Œè¯»è€…éœ€è¦é‡è½½è¯¥å‡½æ•°æ¥å®ç°ç›¸åº”çš„è®¡ç®—æµç¨‹ã€‚\nclass QWenAttention final : public Module ... 5.4.1 åˆ›å»ºè¯¥Moduleéœ€è¦ä½¿ç”¨çš„Layers class QWenAttention final : public Module { publicï¼š QWenAttention() = default; QWenAttention(const QWenConfig \u0026config, const QWenNameConfig \u0026names, const string \u0026base_name) { hidden_size = config.hidden_size; num_heads = config.num_attention_heads; head_dim = config.hidden_size / num_heads; num_key_value_heads = config.num_key_value_heads; num_key_value_groups = num_heads / num_key_value_heads; // init layers q_proj = Linear(hidden_size, num_heads * head_dim, true, base_name + names._q_proj_name); k_proj = Linear(hidden_size, num_key_value_heads * head_dim, true, base_name + names._k_proj_name); v_proj = Linear(hidden_size, num_key_value_heads * head_dim, true, base_name + names._v_proj_name); o_proj = Linear(num_heads * head_dim, hidden_size, false, base_name + names._o_proj_name); q_rope = RoPE(config.RoPE_type, config.rope_theta, config.max_position_embeddings, base_name + \"q_rope\"); k_rope = RoPE(config.RoPE_type, config.rope_theta, config.max_position_embeddings, base_name + \"k_rope\"); k_cache = KVCache(num_key_value_groups, config.cache_limit, base_name + \"k_cache\"); v_cache = KVCache(num_key_value_groups, config.cache_limit, base_name + \"v_cache\"); mask = Causalmask(base_name + \"mask\"); softmax = Softmax(DIMENSION, base_name + \"softmax\"); } private: int hidden_size; int num_heads; int head_dim; int num_key_value_heads; int num_key_value_groups; Layer q_proj; Layer k_proj; Layer v_proj; Layer o_proj; Layer q_rope; Layer k_rope; Layer k_cache; Layer v_cache; Layer mask; Layer softmax; } ç»†å¿ƒçš„è¯»è€…å¯èƒ½å·²ç»å‘ç°äº†ï¼Œåœ¨QWenAttentionçš„æ„é€ å‡½æ•°ä¸­ï¼Œåˆ›å»ºæ¯ä¸ªLayerçš„æ—¶å€™éƒ½åœ¨æœ€åä¸€ä¸ªå‚æ•°ä¸Šä¼ é€’äº†Layeråç§°ï¼ˆstd::string typeï¼‰ï¼Œè¿™æ˜¯å› ä¸ºmllmä¾èµ–äºLayerçš„åç§°æ¥å¯»æ‰¾è¯¥Layeræ‰€éœ€è¦çš„å‚æ•°ã€‚\n5.4.2 é‡è½½Forwardå‰å‘æ¨ç†å‡½æ•° åˆ›å»ºå®Œäº†æ‰€æœ‰æˆ‘ä»¬éœ€è¦çš„Layersä»¥åï¼Œå°±å¯ä»¥ç¼–å†™Forwardå‡½æ•°æ¥å®šä¹‰Attentionæ¨¡å—çš„è®¡ç®—æµç¨‹ï¼ŒForwardå‡½æ•°æ¥æ”¶ä¸€ä¸ªTensor Arrayå’Œä¸€ä¸ªstd::any Arrayï¼Œè¿”å›Tensor Arrayï¼š\nstd::vector\u003cTensor\u003e Forward(std::vector\u003cTensor\u003e inputs, std::vector\u003cstd::any\u003e args) override { auto query_states = q_proj(inputs[0]); auto key_states = k_proj(inputs[1]); auto value_states = v_proj(inputs[2]); // [batch, heads, sequence, dims] query_states = query_states.view(-1, num_heads, -1, head_dim); key_states = key_states.view(-1, num_key_value_heads, -1, head_dim); value_states = value_states.view(-1, num_key_value_heads, -1, head_dim); // embedding query_states = q_rope(query_states); key_states = k_rope(key_states); // kv cache key_states = k_cache(key_states); value_states = v_cache(value_states); // attention weight auto atten_weight = Tensor::mm(query_states, key_states.transpose(Chl::SEQUENCE, Chl::DIMENSION)) / std::sqrt(head_dim); atten_weight = mask(atten_weight); atten_weight = softmax(atten_weight); // attention output auto atten_output = Tensor::mm(atten_weight, value_states); atten_output = atten_output.view(-1, 1, -1, head_dim * num_heads); atten_output = o_proj(atten_output); return {atten_output}; } 5.5 è¿è¡Œ å®Œæ•´çš„Qwenæ¨¡å‹å®šä¹‰ä»£ç å¯ä»¥åœ¨é™„å½•1ä¸­æ‰¾åˆ°ã€‚è¯»è€…å¯ä»¥åƒTorchä¸€æ ·è°ƒç”¨å®šä¹‰å¥½çš„æ¨¡å‹ï¼šé¦–å…ˆï¼Œåˆ›å»ºæ¨¡å‹ï¼š\nQWenConfig config(tokens_limit, \"0.5B\", RoPEType::HFHUBROPE); auto model = QWenForCausalLM(config); model.load(model_path); moduleclassé‡è½½äº†()operatorï¼Œè¯»è€…å¯ä»¥ä½¿ç”¨model({input_tensor})æ¥è¿›è¡Œæ¨ç†ã€‚\n6. mllmæ¡†æ¶çš„ä¸è¶³ è¿™é‡Œå†™çš„æœ‰ç‚¹meanï¼Œæœ¬äººä¸“ä¸šçŸ¥è¯†æµ…è–„ï¼Œåœ¨å­¦æœ¯ä¸Šæ˜¯ä¾æ‰˜ç­”è¾©ï¼Œå¯¹mllmçš„ç†è§£æ›´æ˜¯ä¸åˆ°ä½ï¼Œå¤§å®¶è½»å–·ã€‚\n6.1 Benchmark ç¼ºå°‘ç®—å­çš„Benchmark æœ¬æ–‡è®¤ä¸ºï¼Œmllmåœ¨å®ç°çš„æ—¶å€™æåŠ›çš„é¿å…ä½¿ç”¨ç¬¬ä¸‰æ–¹çš„åº“ï¼Œå› ä¸ºmllméœ€è¦è¿ç§»åˆ°ç§»åŠ¨è®¾å¤‡ä¸Šï¼Œä¸€äº›ä¸‰æ–¹åº“å¯èƒ½ä¸èƒ½æ­£å¸¸å·¥ä½œã€‚ä½†æ˜¯æ‰‹å·¥å®ç°çš„Kernelè¿˜æ˜¯éœ€è¦ä¸€ä¸ªBenchmarkæ¥å’Œç›®æ ‡å¹³å°ä¸Šæä¾›çš„ç®—å­åº“æ¥è¿›è¡Œæ€§èƒ½æ¯”è¾ƒçš„ã€‚å°±mllmç›®å‰æä¾›çš„MatMul Kernelæ¥çœ‹ï¼Œä¼¼ä¹ç¼ºå°‘Packä¼˜åŒ–å’Œ/micro Kernelçš„ä¼˜åŒ–ï¼Ÿ\nç¼ºå°‘prefill/decodeçš„Benchmark mllmçš„issuesä¸­ä¹Ÿæœ‰äººæåˆ°è¿‡è¿™ä¸ªé—®é¢˜ã€‚ä½œä¸ºå…·æœ‰LLMæ¨ç†èƒ½åŠ›çš„å¼•æ“ï¼Œåº”å½“æµ‹ä¸€ä¸‹è¿™ä¸¤ä¸ªåŸºæœ¬èƒ½åŠ›ã€‚\n6.2 å¯¹äºç§»åŠ¨ç«¯LLMæ¨ç†çš„ç‰¹å®šä¼˜åŒ– KV Cacheé‡åŒ– IIRCï¼Œåœ¨OPPOçš„Transformer-Lite[2]ä¸­ï¼Œç”¨åˆ°äº†KV Cacheé‡åŒ–çš„å°æŠ€å·§ã€‚è¿™å¯¹ç§»åŠ¨è®¾å¤‡æœ‰é™çš„å†…å­˜æ¥è¯´å¯èƒ½ä¼šæ›´åŠ å‹å¥½ï¼Œå½“ç„¶è¿˜éœ€è¦è€ƒé‡é‡åŒ–å¸¦æ¥çš„CPUè´Ÿè½½é—®é¢˜ã€‚\nåŠ¨æ€å½¢çŠ¶æ¨ç†/å†…å­˜å¤ç”¨/KV Cacheæ¬ç§»ä¼˜åŒ– ç›®å‰mllmæ˜¯æ²¡æœ‰åšå†…å­˜å¤ç”¨çš„ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ç¬¦å·æ¨ç†æ–¹æ³•æ¥åšåŠ¨æ€å½¢çŠ¶çš„æ”¯æŒè¿›è€Œä¾¿äºæ±‚è§£ä¸‹ä¸€è½®çš„å†…å­˜ä½¿ç”¨æƒ…å†µã€‚æˆ–è®¸å¯ä»¥è€ƒè™‘ä¸€ä¸‹PageAttention[3]çš„Tensorç®¡ç†æ–¹æ³•æˆ–è€…[2]ä¸­çš„KV Cacheè§„åˆ’æ–¹æ³•æ¥è¿›ä¸€æ­¥å‡å°‘å†…å­˜çš„æ¬ç§»ã€‚\nå¼‚æ„ç®—åŠ› å¯ä»¥è€ƒè™‘æŠŠå½¢çŠ¶æ¨ç†ï¼ˆCPUï¼‰å’Œè®¡ç®—ï¼ˆGPU/NPUï¼‰å¹¶è¡Œæ‰§è¡Œèµ·æ¥ã€‚æˆ–è€…æ˜¯6.2.4ä¸­æåˆ°çš„å†…å®¹ä¸è®¡ç®—å¹¶è¡Œèµ·æ¥ã€‚\nå¯¹æ¨¡å‹å‚æ•°çš„Lazy Fetchå’ŒPre Fetch ç›®å‰ï¼Œmllmä¼šæŠŠå‚æ•°ä¸€æ¬¡æ€§çš„è¯»å…¥å†…å­˜ï¼Ÿè€ƒè™‘åˆ°ç§»åŠ¨è®¾å¤‡çš„å†…å­˜æœ‰é™ï¼Œå¯ä»¥åœ¨åˆé€‚çš„æ—¶æœºæå‰ä»å¤–å­˜ä¸Šé¢„å–è€Œä¸æ˜¯å…¨æ•°è½½å…¥ã€‚\n6.3 æ˜“ç”¨æ€§ æ¨¡å‹ç»“æ„éœ€è¦æ‰‹åŠ¨ç¼–å†™ä¸”æ— æ³•ä¿å­˜ ç›®å‰ï¼Œmllmçš„æ¨¡å‹ç»“æ„è¿˜æ˜¯éœ€è¦åœ¨C++æ–‡ä»¶ä¸­è¿›è¡Œæ˜¾ç¤ºçš„æ‰‹åŠ¨å®šä¹‰ã€‚æˆ–è®¸å¯ä»¥è€ƒè™‘åˆ›å»ºè‡ªå·±çš„è®¡ç®—å›¾å’Œç®—å­æè¿°æ–¹å¼ï¼Œä½¿ç”¨flatbuffersæ¥å­˜å‚¨è®¡ç®—å›¾ã€‚\nå¦‚æœè¦å¾ˆå¥½çš„ä½¿ç”¨æ‰€æœ‰çš„ç®—åŠ›ï¼Œå¯èƒ½è¿˜æ˜¯éœ€è¦å®Œå–„çš„è®¡ç®—å›¾æœºåˆ¶ï¼Œè¿™æ ·ä¾¿äºä¼˜åŒ–åˆ†æã€‚ å°è¯•å¼•å…¥ä¸‰æ–¹æ˜“ç”¨çš„åº“å¦‚icuç­‰æ¥å¼¥è¡¥C++ utf-8å¤„ç†èƒ½åŠ›çš„ä¸è¶³ã€‚ Refï¼š\n[1] mllm, https://github.com/UbiquitousLearning/mllm\n[2] transformer-lite, https://arxiv.org/abs/2403.20041\n[3] PageAttention, https://arxiv.org/abs/2309.06180\nA1. Qwenæ¨¡å‹å®šä¹‰ #ifndef MODELING_QWEN_HPP #define MODELING_QWEN_HPP #include \"Backend.hpp\" #include \"Layer.hpp\" #include \"Module.hpp\" #include \"Tensor.hpp\" #include \"configuration_qwen.hpp\" #include using namespace mllm; // Copied from GemmaMLP with Gemma-\u003eQwen and using silu class QWenMLP final : public Module { public: QWenMLP() = default; QWenMLP(int hidden_size, int intermediate_size, const QWenNameConfig \u0026names, const std::string \u0026base_name) { gate_proj = Linear(hidden_size, intermediate_size, false, base_name + names._gate_proj_name); silu = SiLU(base_name + \"act\"); up_proj = Linear(hidden_size, intermediate_size, false, base_name + names._up_proj_name); down_proj = Linear(intermediate_size, hidden_size, false, base_name + names._down_proj_name); } std::vector\u003cTensor\u003e Forward(std::vector\u003cTensor\u003e inputs, std::vector\u003cstd::any\u003e args) override { auto x = gate_proj(inputs[0]); x = silu(x); auto y = up_proj(inputs[0]); x = x * y; x = down_proj(x); return {x}; } private: Layer gate_proj; Layer up_proj; Layer down_proj; Layer silu; }; // Copied from GemmaAttention with Gemma-\u003eQwen and using SWA class QWenAttention final : public Module { public: QWenAttention() = default; QWenAttention(const QWenConfig \u0026config, const QWenNameConfig \u0026names, const string \u0026base_name) { hidden_size = config.hidden_size; num_heads = config.num_attention_heads; head_dim = config.hidden_size / num_heads; num_key_value_heads = config.num_key_value_heads; num_key_value_groups = num_heads / num_key_value_heads; // init layers q_proj = Linear(hidden_size, num_heads * head_dim, true, base_name + names._q_proj_name); k_proj = Linear(hidden_size, num_key_value_heads * head_dim, true, base_name + names._k_proj_name); v_proj = Linear(hidden_size, num_key_value_heads * head_dim, true, base_name + names._v_proj_name); o_proj = Linear(num_heads * head_dim, hidden_size, false, base_name + names._o_proj_name); q_rope = RoPE(config.RoPE_type, config.rope_theta, config.max_position_embeddings, base_name + \"q_rope\"); k_rope = RoPE(config.RoPE_type, config.rope_theta, config.max_position_embeddings, base_name + \"k_rope\"); k_cache = KVCache(num_key_value_groups, config.cache_limit, base_name + \"k_cache\"); v_cache = KVCache(num_key_value_groups, config.cache_limit, base_name + \"v_cache\"); // mask = SlidingWindowMask(config.sliding_window, base_name + \"mask\"); mask = Causalmask(base_name + \"mask\"); softmax = Softmax(DIMENSION, base_name + \"softmax\"); } std::vector\u003cTensor\u003e Forward(std::vector\u003cTensor\u003e inputs, std::vector\u003cstd::any\u003e args) override { auto query_states = q_proj(inputs[0]); auto key_states = k_proj(inputs[1]); auto value_states = v_proj(inputs[2]); // [batch, heads, sequence, dims] query_states = query_states.view(-1, num_heads, -1, head_dim); key_states = key_states.view(-1, num_key_value_heads, -1, head_dim); value_states = value_states.view(-1, num_key_value_heads, -1, head_dim); // embedding query_states = q_rope(query_states); key_states = k_rope(key_states); // kv cache key_states = k_cache(key_states); value_states = v_cache(value_states); // attention weight auto atten_weight = Tensor::mm(query_states, key_states.transpose(Chl::SEQUENCE, Chl::DIMENSION)) / std::sqrt(head_dim); atten_weight = mask(atten_weight); atten_weight = softmax(atten_weight); // attention output auto atten_output = Tensor::mm(atten_weight, value_states); atten_output = atten_output.view(-1, 1, -1, head_dim * num_heads); atten_output = o_proj(atten_output); return {atten_output}; } private: int hidden_size; int num_heads; int head_dim; int num_key_value_heads; int num_key_value_groups; Layer q_proj; Layer k_proj; Layer v_proj; Layer o_proj; Layer q_rope; Layer k_rope; Layer k_cache; Layer v_cache; Layer mask; Layer softmax; }; // Copied from GemmaDecoder with Gemma-\u003eQwen and set RmsNorm(without add_unit_offset) class QWenDecoder final : public Module { public: QWenDecoder() = default; QWenDecoder(const QWenConfig \u0026config, const QWenNameConfig \u0026names, const string \u0026base_name) { self_atten = QWenAttention(config, names, base_name + names._attn_base_name); mlp = QWenMLP(config.hidden_size, config.intermediate_size, names, base_name + names._ffn_base_name); input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps, base_name + names._attn_norm_name); post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps, base_name + names._ffn_norm_name); } std::vector\u003cTensor\u003e Forward(std::vector\u003cTensor\u003e inputs, std::vector\u003cstd::any\u003e args) override { auto x = input_layernorm(inputs[0]); x = self_atten({x, x, x})[0]; auto tmp = x + inputs[0]; x = post_attention_layernorm(tmp); x = mlp({x})[0]; x = x + tmp; return {x}; } private: QWenAttention self_atten; QWenMLP mlp; Layer input_layernorm; Layer post_attention_layernorm; }; // Copied from GemmaModel with Gemma-\u003eQwen and set RmsNorm(without add_unit_offset) class QWenModel final : public Module { public: QWenModel() = default; QWenModel(const QWenConfig \u0026config, const QWenNameConfig \u0026names, const string \u0026base_name) { blocks = List\u003cQWenDecoder\u003e(config.num_hidden_layers, config, names, base_name); norm = RMSNorm(config.hidden_size, config.rms_norm_eps, names.post_norm_name); } std::vector\u003cTensor\u003e Forward(std::vector\u003cTensor\u003e inputs, std::vector\u003cstd::any\u003e args) override { auto x = inputs[0]; for (auto \u0026block : blocks) { x = block({x})[0]; } x = norm(x); return {x}; } private: std::vector\u003cQWenDecoder\u003e blocks; Layer norm; }; class QWenForCausalLM final : public Module { public: QWenForCausalLM(QWenConfig \u0026config) { auto names = config.names_config; hidden_size = config.hidden_size; tie_embedding_words = config.tie_embedding_words; embedding = Embedding(config.vocab_size, config.hidden_size, names.token_embd_name); model = QWenModel(config, names, names.blk_name); // FIXME Qwen-0.5 use tied embedding // Others use nn.Linear() if (tie_embedding_words) { lm_head = Parameter(1, config.vocab_size, 1, config.hidden_size, names.token_embd_name + \".weight\"); } } std::vector\u003cTensor\u003e Forward(std::vector\u003cTensor\u003e inputs, std::vector\u003cstd::any\u003e args) override { auto x = embedding(inputs[0]); // go through model auto outputs = model({x})[0]; if (tie_embedding_words) { outputs = Tensor::mm(outputs, lm_head().transpose(Chl::SEQUENCE, Chl::DIMENSION)); } return {outputs}; } private: int hidden_size; bool tie_embedding_words; Layer embedding; Parameter lm_head; QWenModel model; }; #endif //! MODELING_QWEN_HPP ",
  "wordCount" : "1623",
  "inLanguage": "en",
  "datePublished": "2024-06-28T00:00:00Z",
  "dateModified": "2024-06-28T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "chenghua.Wang"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://chenghuawang.github.io/keep-moving-forward/tech/mllm-qwen/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Ubios Home",
    "logo": {
      "@type": "ImageObject",
      "url": "https://chenghuawang.github.io/keep-moving-forward/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://chenghuawang.github.io/keep-moving-forward/" accesskey="h" title="Ubios Home (Alt + H)">Ubios Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://chenghuawang.github.io/keep-moving-forward/about/about/" title="å…³äºæˆ‘">
                    <span>å…³äºæˆ‘</span>
                </a>
            </li>
            <li>
                <a href="https://chenghuawang.github.io/keep-moving-forward/about/tech_posts/" title="æŠ€æœ¯ç›¸å…³">
                    <span>æŠ€æœ¯ç›¸å…³</span>
                </a>
            </li>
            <li>
                <a href="https://chenghuawang.github.io/keep-moving-forward/about/paper_posts/" title="è®ºæ–‡è§£æ">
                    <span>è®ºæ–‡è§£æ</span>
                </a>
            </li>
            <li>
                <a href="https://chenghuawang.github.io/keep-moving-forward/about/news/" title="ğŸ‰NewsğŸ‰">
                    <span>ğŸ‰NewsğŸ‰</span>
                </a>
            </li>
            <li>
                <a href="https://chenghuawang.github.io/keep-moving-forward/about/thingking/" title="æ€è€ƒ">
                    <span>æ€è€ƒ</span>
                </a>
            </li>
            <li>
                <a href="https://chenghuawang.github.io/keep-moving-forward/about/hpc_ai/" title="AI&amp;Sys å…¥é—¨">
                    <span>AI&amp;Sys å…¥é—¨</span>
                </a>
            </li>
            <li>
                <a href="https://chenghuawang.github.io/keep-moving-forward/tags/" title="æ ‡ç­¾">
                    <span>æ ‡ç­¾</span>
                </a>
            </li>
            <li>
                <a href="https://chenghuawang.github.io/keep-moving-forward/series" title="ç³»åˆ—æ–‡ç« ">
                    <span>ç³»åˆ—æ–‡ç« </span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://chenghuawang.github.io/keep-moving-forward/">Home</a>&nbsp;Â»&nbsp;<a href="https://chenghuawang.github.io/keep-moving-forward/tech/">Technique</a></div>
    <h1 class="post-title entry-hint-parent">
      mllmæ¡†æ¶æµ…æ(ä¸€)-ä»¥QWen0.5Bä¸ºä¾‹
    </h1>
    <div class="post-description">
      ä»¥Qwen0.5Bä¸ºä¾‹è§£æmllmçš„åŸºæœ¬å®ç°ï¼ŒCPU Backend
    </div>
    <div class="post-meta"><span title='2024-06-28 00:00:00 +0000 UTC'>June 28, 2024</span>&nbsp;Â·&nbsp;8 min&nbsp;Â·&nbsp;chenghua.Wang

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#1-%e7%ae%80%e4%bb%8b" aria-label="1. ç®€ä»‹">1. ç®€ä»‹</a></li>
                <li>
                    <a href="#2-%e6%a1%86%e6%9e%b6%e6%89%a7%e8%a1%8c%e6%b5%81%e7%a8%8b" aria-label="2. æ¡†æ¶æ‰§è¡Œæµç¨‹">2. æ¡†æ¶æ‰§è¡Œæµç¨‹</a><ul>
                        
                <li>
                    <a href="#21-%e4%bb%a5%e4%b8%a4%e5%b1%82linear%e5%b1%82%e8%bf%90%e8%a1%8c%e4%b8%ba%e4%be%8b" aria-label="2.1 ä»¥ä¸¤å±‚Linearå±‚è¿è¡Œä¸ºä¾‹">2.1 ä»¥ä¸¤å±‚Linearå±‚è¿è¡Œä¸ºä¾‹</a><ul>
                        
                <li>
                    <a href="#211-%e5%8a%a0%e8%bd%bd%e5%8f%82%e6%95%b0" aria-label="2.1.1 åŠ è½½å‚æ•°">2.1.1 åŠ è½½å‚æ•°</a></li>
                <li>
                    <a href="#212module%e7%9a%84operator%e6%98%af%e5%a6%82%e4%bd%95%e8%b0%83%e7%94%a8forward%e5%87%bd%e6%95%b0%e7%9a%84" aria-label="2.1.2Moduleçš„Operator()æ˜¯å¦‚ä½•è°ƒç”¨Forwardå‡½æ•°çš„ï¼Ÿ">2.1.2Moduleçš„Operator()æ˜¯å¦‚ä½•è°ƒç”¨Forwardå‡½æ•°çš„ï¼Ÿ</a></li>
                <li>
                    <a href="#213-linear%e5%b1%82%e7%9a%84%e6%89%a7%e8%a1%8c" aria-label="2.1.3 Linearå±‚çš„æ‰§è¡Œ">2.1.3 Linearå±‚çš„æ‰§è¡Œ</a></li></ul>
                </li>
                <li>
                    <a href="#22-%e6%80%bb%e7%bb%93" aria-label="2.2 æ€»ç»“">2.2 æ€»ç»“</a></li></ul>
                </li>
                <li>
                    <a href="#3-%e5%a6%82%e4%bd%95%e7%bc%96%e5%86%99op%e4%b8%8e%e8%87%aa%e5%ae%9a%e4%b9%89layer" aria-label="3. å¦‚ä½•ç¼–å†™Opä¸è‡ªå®šä¹‰Layer">3. å¦‚ä½•ç¼–å†™Opä¸è‡ªå®šä¹‰Layer</a><ul>
                        
                <li>
                    <a href="#31-%e6%96%b0%e5%a2%9e%e5%af%b9%e5%ba%94backend%e7%9a%84op%e6%96%87%e4%bb%b6" aria-label="3.1 æ–°å¢å¯¹åº”Backendçš„Opæ–‡ä»¶">3.1 æ–°å¢å¯¹åº”Backendçš„Opæ–‡ä»¶</a></li>
                <li>
                    <a href="#32-op%e5%8f%82%e6%95%b0%e8%87%aa%e5%ae%9a%e4%b9%89" aria-label="3.2 Opå‚æ•°è‡ªå®šä¹‰">3.2 Opå‚æ•°è‡ªå®šä¹‰</a></li>
                <li>
                    <a href="#33-%e9%87%8d%e8%bd%bd%e5%87%bd%e6%95%b0" aria-label="3.3 é‡è½½å‡½æ•°">3.3 é‡è½½å‡½æ•°</a></li>
                <li>
                    <a href="#34-op%e6%98%af%e5%a6%82%e4%bd%95%e8%a2%ab%e6%b3%a8%e5%86%8c%e5%92%8c%e5%88%9b%e5%bb%ba%e7%9a%84" aria-label="3.4 Opæ˜¯å¦‚ä½•è¢«æ³¨å†Œå’Œåˆ›å»ºçš„ï¼Ÿ">3.4 Opæ˜¯å¦‚ä½•è¢«æ³¨å†Œå’Œåˆ›å»ºçš„ï¼Ÿ</a><ul>
                        
                <li>
                    <a href="#341-%e5%9c%a8backend%e4%b8%ad%e6%b3%a8%e5%86%8cop" aria-label="3.4.1 åœ¨Backendä¸­æ³¨å†ŒOp">3.4.1 åœ¨Backendä¸­æ³¨å†ŒOp</a></li>
                <li>
                    <a href="#342-%e5%9c%a8layerhpp%e4%b8%ad%e5%8a%a0%e5%85%a5%e5%af%b9%e5%ba%94%e7%9a%84op-layer" aria-label="3.4.2 åœ¨Layer.hppä¸­åŠ å…¥å¯¹åº”çš„Op Layer">3.4.2 åœ¨Layer.hppä¸­åŠ å…¥å¯¹åº”çš„Op Layer</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#4-tokenizer" aria-label="4. Tokenizer">4. Tokenizer</a></li>
                <li>
                    <a href="#5-%e5%a6%82%e4%bd%95%e5%af%b9%e6%96%b0%e6%a8%a1%e5%9e%8b%e8%bf%9b%e8%a1%8c%e6%94%af%e6%8c%81" aria-label="5. å¦‚ä½•å¯¹æ–°æ¨¡å‹è¿›è¡Œæ”¯æŒ">5. å¦‚ä½•å¯¹æ–°æ¨¡å‹è¿›è¡Œæ”¯æŒ</a><ul>
                        
                <li>
                    <a href="#51-%e7%94%9f%e6%88%90mllm%e6%94%af%e6%8c%81%e7%9a%84vocab%e5%92%8c%e6%a8%a1%e5%9e%8b%e5%8f%82%e6%95%b0" aria-label="5.1 ç”Ÿæˆmllmæ”¯æŒçš„vocabå’Œæ¨¡å‹å‚æ•°">5.1 ç”Ÿæˆmllmæ”¯æŒçš„vocabå’Œæ¨¡å‹å‚æ•°</a><ul>
                        
                <li>
                    <a href="#511-%e6%a8%a1%e5%9e%8b%e8%bd%ac%e6%8d%a2" aria-label="5.1.1 æ¨¡å‹è½¬æ¢">5.1.1 æ¨¡å‹è½¬æ¢</a></li>
                <li>
                    <a href="#512-vocab%e8%bd%ac%e6%8d%a2" aria-label="5.1.2 Vocabè½¬æ¢">5.1.2 Vocabè½¬æ¢</a></li>
                <li>
                    <a href="#513-%e9%87%8f%e5%8c%96" aria-label="5.1.3 é‡åŒ–">5.1.3 é‡åŒ–</a></li></ul>
                </li>
                <li>
                    <a href="#52-configuration" aria-label="5.2 Configuration">5.2 Configuration</a></li>
                <li>
                    <a href="#53-tokenization" aria-label="5.3 Tokenization">5.3 Tokenization</a></li>
                <li>
                    <a href="#54-modeling" aria-label="5.4 Modeling">5.4 Modeling</a><ul>
                        
                <li>
                    <a href="#541-%e5%88%9b%e5%bb%ba%e8%af%a5module%e9%9c%80%e8%a6%81%e4%bd%bf%e7%94%a8%e7%9a%84layers" aria-label="5.4.1 åˆ›å»ºè¯¥Moduleéœ€è¦ä½¿ç”¨çš„Layers">5.4.1 åˆ›å»ºè¯¥Moduleéœ€è¦ä½¿ç”¨çš„Layers</a></li>
                <li>
                    <a href="#542-%e9%87%8d%e8%bd%bdforward%e5%89%8d%e5%90%91%e6%8e%a8%e7%90%86%e5%87%bd%e6%95%b0" aria-label="5.4.2 é‡è½½Forwardå‰å‘æ¨ç†å‡½æ•°">5.4.2 é‡è½½Forwardå‰å‘æ¨ç†å‡½æ•°</a></li></ul>
                </li>
                <li>
                    <a href="#55-%e8%bf%90%e8%a1%8c" aria-label="5.5 è¿è¡Œ">5.5 è¿è¡Œ</a></li></ul>
                </li>
                <li>
                    <a href="#6-mllm%e6%a1%86%e6%9e%b6%e7%9a%84%e4%b8%8d%e8%b6%b3" aria-label="6. mllmæ¡†æ¶çš„ä¸è¶³">6. mllmæ¡†æ¶çš„ä¸è¶³</a><ul>
                        
                <li>
                    <a href="#61-benchmark" aria-label="6.1 Benchmark">6.1 Benchmark</a></li>
                <li>
                    <a href="#62-%e5%af%b9%e4%ba%8e%e7%a7%bb%e5%8a%a8%e7%ab%afllm%e6%8e%a8%e7%90%86%e7%9a%84%e7%89%b9%e5%ae%9a%e4%bc%98%e5%8c%96" aria-label="6.2 å¯¹äºç§»åŠ¨ç«¯LLMæ¨ç†çš„ç‰¹å®šä¼˜åŒ–">6.2 å¯¹äºç§»åŠ¨ç«¯LLMæ¨ç†çš„ç‰¹å®šä¼˜åŒ–</a></li>
                <li>
                    <a href="#63-%e6%98%93%e7%94%a8%e6%80%a7" aria-label="6.3 æ˜“ç”¨æ€§">6.3 æ˜“ç”¨æ€§</a></li></ul>
                </li>
                <li>
                    <a href="#a1-qwen%e6%a8%a1%e5%9e%8b%e5%ae%9a%e4%b9%89" aria-label="A1. Qwenæ¨¡å‹å®šä¹‰">A1. Qwenæ¨¡å‹å®šä¹‰</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><hr>
<p>ç¬”è€…æœ€è¿‘åœ¨åšä¸€äº›mllmç›¸å…³çš„å·¥ä½œï¼Œä¹¦å†™æ­¤æ–‡å¯¹mllmæ¡†æ¶è¿›è¡Œæ¢³ç†æ€»ç»“ï¼Œå®šæœ‰ä¸å°‘çº°æ¼ï¼Œè¯·è¯»è€…ç«‹å³æŒ‡å‡ºï¼Œè°¢è°¢ã€‚mllmç›®å‰åœ¨åšä¸€äº›å…¶ä»–å·¥ä½œï¼Œè¿™ç¯‡æ–‡ç« çš„ä¹¦å†™æ—¶é—´ä¸ºå‘å¸ƒæ—¶é—´ã€‚åœ¨mllmçš„å…¶ä»–å·¥ä½œåˆå¹¶è¿›ä¸»ä»“åº“åï¼Œæœ¬æ–‡è¿˜ä¼šè¿›ä¸€æ­¥çš„è·Ÿè¿›ã€‚è¯»è€…è¯·æ³¨æ„æœ¬æ–‡çš„æ—¶æ•ˆæ€§ã€‚</p>
<hr>
<h1 id="1-ç®€ä»‹">1. ç®€ä»‹<a hidden class="anchor" aria-hidden="true" href="#1-ç®€ä»‹">#</a></h1>
<p><strong>mllm</strong>æ˜¯ä¸€æ¬¾é€‚ç”¨äº<strong>ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¾å¤‡</strong>çš„å¿«é€Ÿã€è½»é‡çš„å¤šæ¨¡æ€LLMæ¨ç†å¼•æ“ã€‚</p>
<ul>
<li>å®Œå…¨çš„C/C++å®ç°ï¼Œæ— ç¬¬ä¸‰æ–¹ä¾èµ–</li>
<li>é’ˆå¯¹fuyu-8Bç­‰å¤šæ¨¡æ€LLMè¿›è¡Œäº†ä¼˜åŒ–</li>
<li>æ”¯æŒARM NEONå’ŒX86 AVX2å‘é‡æŒ‡ä»¤</li>
<li>æ”¯æŒ4 bitså’Œ6 bitsæ•´æ•°é‡åŒ–</li>
</ul>
<p>æœ¬æ–‡å°†æ›´å¤šçš„ä»¥å·¥ç¨‹çš„è§†è§’æ¥è§£æmllmæ¡†æ¶ï¼Œåœ¨è¡Œæ–‡è¿‡ç¨‹ä¸­ï¼Œæœ¬æ–‡ä¼šå°†mllmä¸å…¶ä»–æ¡†æ¶çš„è®¾è®¡æ–¹æ³•åšå¯¹æ¯”ã€‚æ¥ä¸‹æ¥ï¼Œæœ¬æ–‡å°†ä¼šç”¨<strong>é¡¹ç›®ç»„ç»‡ç»“æ„ã€æ¡†æ¶æ‰§è¡Œæµç¨‹ã€è‡ªå®šä¹‰Op/Layerã€Tokenizerå’Œå¦‚ä½•æ”¯æŒæ–°æ¨¡å‹</strong>äº”ä¸ªç« èŠ‚æ¥è¯¦ç»†æè¿°mllmæ¡†æ¶çš„å„é¡¹ç‰¹æ€§å’Œæ€»ä½“ç»“æ„ã€‚è¯»è€…å¯ä»¥æŠŠè¯¥æ–‡ç« åšmllmçš„ä½¿ç”¨æ–‡æ¡£ã€‚<strong>åœ¨æœ€åï¼Œæœ¬æ–‡å°†ä¼šæŒ‡å‡ºmllmçš„ä¸è¶³ä¹‹å¤„å’Œå¯ä»¥å°è¯•è·Ÿè¿›çš„å·¥ä½œ</strong>ã€‚</p>
<hr>
<p>åœ¨å¼€å§‹æ­£å¼è§£æmllmä¹‹å‰ï¼Œè¯»è€…å¯ä»¥å…ˆcloneä¸‹mllmçš„ä»£ç åº“ï¼Œä»¥ä¾¿äºè·Ÿè¿›åˆ†ææµç¨‹ã€‚mllmä¸ä¾èµ–äºgit submoduleï¼Œé¡¹ç›®é…ç½®èµ·æ¥å¾ˆæ–¹ä¾¿ï¼Œç›®å‰mllmå¯ä»¥åœ¨linuxä¸Šä½¿ç”¨Clang/GCCç¼–è¯‘å™¨è¿›è¡Œç¼–è¯‘ã€‚ç›®å‰mllmæ”¯æŒçš„ç›®æ ‡è®¾å¤‡ä½“ç³»ç»“æ„æ˜¯X86å’ŒArmã€‚</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">git clone https://github.com/UbiquitousLearning/mllm
</span></span></code></pre></div><p>mllmå›¢é˜Ÿå°†æ‰€æœ‰LLMç›¸å…³çš„vocabæ–‡ä»¶éƒ½æ”¾åœ¨äº†gitä»“åº“ä¸­ï¼ˆè¿™ä¸ªå…¶å®å¯ä»¥ç§»åŠ¨åˆ°HuggingFaceçš„ä»“åº“ä¸Šï¼‰ï¼ŒLLMé‡åŒ–åçš„æ¨¡å‹æ–‡ä»¶éƒ½å­˜å‚¨åœ¨HuggingFaceä¸Šï¼Œè¯»è€…å¯ä»¥åœ¨<a href="https://huggingface.co/mllmTeam">https://huggingface.co/mllmTeam</a>ä¸Šæ‰¾åˆ°mllmæä¾›çš„æ¨¡å‹æ–‡ä»¶ã€‚</p>
<h1 id="2-æ¡†æ¶æ‰§è¡Œæµç¨‹">2. æ¡†æ¶æ‰§è¡Œæµç¨‹<a hidden class="anchor" aria-hidden="true" href="#2-æ¡†æ¶æ‰§è¡Œæµç¨‹">#</a></h1>
<h2 id="21-ä»¥ä¸¤å±‚linearå±‚è¿è¡Œä¸ºä¾‹">2.1 ä»¥ä¸¤å±‚Linearå±‚è¿è¡Œä¸ºä¾‹<a hidden class="anchor" aria-hidden="true" href="#21-ä»¥ä¸¤å±‚linearå±‚è¿è¡Œä¸ºä¾‹">#</a></h2>
<p>é¦–å…ˆï¼Œè€ƒè™‘ä¸‹é¢çš„ä»£ç ï¼Œå®šä¹‰äº†ä¸¤ä¸ªLinear Layersï¼Œå¹¶ä¸”è¾“å…¥$X$é€šè¿‡ä¸¤ä¸ªLinear Layersæ¥å¾—åˆ°è¾“å‡ºï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">TwoLinear</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Module</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">TwoLinear</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">TwoLinear</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">linear1</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">out_f</span><span class="p">,</span> <span class="cm">/*bias*/</span><span class="nb">true</span><span class="p">,</span> <span class="s">&#34;linear1&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">linear2</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">out_f</span><span class="p">,</span> <span class="n">out_f</span><span class="p">,</span> <span class="cm">/*bias*/</span><span class="nb">true</span><span class="p">,</span> <span class="s">&#34;linear2&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">Forward</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">any</span><span class="o">&gt;</span> <span class="n">args</span><span class="p">)</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">linear1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">linear2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">TwoLinear</span> <span class="n">tl</span><span class="p">;</span>
</span></span></code></pre></div><h3 id="211-åŠ è½½å‚æ•°">2.1.1 åŠ è½½å‚æ•°<a hidden class="anchor" aria-hidden="true" href="#211-åŠ è½½å‚æ•°">#</a></h3>
<p>è¯»è€…å¯ä»¥ä½¿ç”¨ <code>tl.load(path)</code>æ¥åŠ è½½å‚æ•°ã€‚é‚£ä¹ˆmllmæ˜¯å¦‚ä½•å®ç°å‚æ•°åŠ è½½çš„å‘¢ï¼Ÿåœ¨loadå‡½æ•°ä¸­ï¼Œmllmä¼šåˆ›å»ºä¸€ä¸ªParamLoaderï¼Œè¿™ä¸ªParamLoaderæ˜¯Staticçš„ï¼Œåœ¨å…¨å±€å¯ä»¥è®¿é—®ã€‚ç„¶åmllmä¼šè®¾ç½®å¦ä¸€ä¸ªå…¨å±€å‚æ•°doLoadä¸ºTrueï¼Œè¿›è€Œè¿›å…¥æ¨ç†æµç¨‹<code>operator()(tmps, tmpt);</code>ã€‚åœ¨æ¨ç†æµç¨‹ä¸­ï¼Œè¦æ˜¯æ‰§è¡Œå±‚å‘ç°doLoadä¸ºTrueï¼Œé‚£ä¹ˆå°±æ‰§è¡Œæ¯ä¸ªç®—å­å†…å®šä¹‰å¥½çš„loadæŒ‡ä»¤ï¼Œè€Œä¸æ˜¯æ‰§è¡Œæ¯ä¸ªç®—å­çš„åŸæœ¬é€»è¾‘ã€‚
loadçš„æ‰§è¡Œåœ¨<code>Layer.hpp</code>æ–‡ä»¶çš„<code>INIT_OP()</code>ä¸­ã€‚</p>
<h3 id="212moduleçš„operatoræ˜¯å¦‚ä½•è°ƒç”¨forwardå‡½æ•°çš„">2.1.2Moduleçš„Operator()æ˜¯å¦‚ä½•è°ƒç”¨Forwardå‡½æ•°çš„ï¼Ÿ<a hidden class="anchor" aria-hidden="true" href="#212moduleçš„operatoræ˜¯å¦‚ä½•è°ƒç”¨forwardå‡½æ•°çš„">#</a></h3>
<p>å¯¹äºå¸¸è§çš„<code>INPUT_TENSOR</code>ç±»å‹çš„Tensorï¼Œmllmé¦–å…ˆä¼šè®¾ç½®è¿™ä¸ªTensorçš„ç±»å‹ä¸º<code>TENSOR_STATIC_INIT</code>ï¼Œè¿›è¡Œä¸€éForwardæ¨ç†ï¼›ç¬¬ä¸€éForwardæ¨ç†å®Œæ¯•ä»¥åå†æŠŠTensorçš„ç±»å‹è®¾ä¸º<code>TENSOR_STATIC_READY</code>ï¼Œç„¶åè¿›è¡Œç¬¬äºŒéForwardæ¨ç†ã€‚</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">ttype</span><span class="p">()</span> <span class="o">==</span> <span class="n">TensorType</span><span class="o">::</span><span class="n">INPUT_TENSOR</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="o">&amp;</span><span class="nl">input</span> <span class="p">:</span> <span class="n">inputs</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">input</span><span class="p">.</span><span class="n">setTtype</span><span class="p">(</span><span class="n">TensorType</span><span class="o">::</span><span class="n">NORMAL_TENSOR</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">input</span><span class="p">.</span><span class="n">status</span><span class="p">()</span> <span class="o">=</span> <span class="n">TENSOR_STATIC_INIT</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span><span class="p">(</span><span class="n">input</span><span class="p">.</span><span class="n">batch</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">            <span class="n">Tensor</span><span class="o">::</span><span class="n">gph_</span><span class="p">[</span><span class="n">input</span><span class="p">.</span><span class="n">name</span><span class="p">()]</span> <span class="o">=</span> <span class="n">input</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor_status</span> <span class="o">=</span> <span class="n">TENSOR_STATIC_INIT</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">Forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">anyArgs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="o">&amp;</span><span class="nl">input</span> <span class="p">:</span> <span class="n">inputs</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">input</span><span class="p">.</span><span class="n">status</span><span class="p">()</span> <span class="o">=</span> <span class="n">TENSOR_STATIC_READY</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor_status</span> <span class="o">=</span> <span class="n">TENSOR_STATIC_READY</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nf">Forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">anyArgs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>ç¬¬ä¸€æ¬¡Forwardæ¨ç†çš„ç›®çš„æ˜¯è°ƒç”¨Opå®šä¹‰çš„Reshapeå’ŒSetUpå‡½æ•°ï¼ŒReshapeå‡½æ•°ä¼šæ¨ç†å‡ºè¿™ä¸€æ¬¡æ¨¡å‹æ¨ç†çš„è¿‡ç¨‹ä¸­æ¯ä¸ªTensorçš„å½¢çŠ¶å¤§å°ã€‚SetUpå‡½æ•°ä¼šå¯¹Opéœ€è¦è¾“å‡ºçš„Tensoråšå†…å­˜çš„ç”³è¯·ã€‚
ç¬¬äºŒæ¬¡Forwardæ¨ç†æ‰æ˜¯çœŸæ­£çš„è®¡ç®—ã€‚</p>
<h3 id="213-linearå±‚çš„æ‰§è¡Œ">2.1.3 Linearå±‚çš„æ‰§è¡Œ<a hidden class="anchor" aria-hidden="true" href="#213-linearå±‚çš„æ‰§è¡Œ">#</a></h3>
<p>æ¯ä¸ªLayeråœ¨å®ç°çš„æ—¶å€™éƒ½ä¼šé‡è½½<code>operator()</code>ï¼Œæ¯”å¦‚linear layerçš„<code>operator()</code>å‡½æ•°å¦‚ä¸‹ï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">Tensor</span> <span class="o">&amp;</span><span class="k">operator</span><span class="p">()(</span><span class="n">Tensor</span> <span class="o">&amp;</span><span class="n">input</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nf">_1I1O_OP</span><span class="p">(</span><span class="n">input</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>å…¶ä¸­ï¼Œ<code>_1I1O_OP</code>è¡¨ç¤ºçš„æ„æ€æ˜¯ï¼Œè¿™æ˜¯éœ€è¦ä½¿ç”¨1ä¸ªè¾“å…¥å’Œ1ä¸ªè¾“å‡ºçš„å‡½æ•°æ¥å¤„ç†è¿™ä¸ªç®—å­ã€‚mllmè¿˜æä¾›äº†è®¸å¤šç±»ä¼¼äº<code>_1I1O_OP</code>çš„å‡½æ•°æ¥å¤„ç†ä¸åŒçš„ç®—å­ã€‚</p>
<h2 id="22-æ€»ç»“">2.2 æ€»ç»“<a hidden class="anchor" aria-hidden="true" href="#22-æ€»ç»“">#</a></h2>
<p>å¤§ä½“æ¥è¯´ï¼Œmllmä½¿ç”¨äº†ç±»ä¼¼äºçŠ¶æ€æœºçš„å‚æ•°æ¥è®¾ç½®äº†å½“å‰æ¨ç†è¿‡ç¨‹çš„è¿è¡ŒçŠ¶æ€ã€‚æ¯ä¸€æ¬¡éƒ½æ˜¯é€šè¿‡Forwardå‡½æ•°æ¥è¿›è¡Œå…¨æ¨¡å‹çš„éå†ï¼Œåœ¨Opçš„æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œç”¨è¿™äº›è®¾å®šçš„å‚æ•°æ¥åŒºåˆ†æ¯æ¬¡Opéœ€è¦è¡¨ç°çš„è¡Œä¸ºã€‚</p>
<h1 id="3-å¦‚ä½•ç¼–å†™opä¸è‡ªå®šä¹‰layer">3. å¦‚ä½•ç¼–å†™Opä¸è‡ªå®šä¹‰Layer<a hidden class="anchor" aria-hidden="true" href="#3-å¦‚ä½•ç¼–å†™opä¸è‡ªå®šä¹‰layer">#</a></h1>
<h2 id="31-æ–°å¢å¯¹åº”backendçš„opæ–‡ä»¶">3.1 æ–°å¢å¯¹åº”Backendçš„Opæ–‡ä»¶<a hidden class="anchor" aria-hidden="true" href="#31-æ–°å¢å¯¹åº”backendçš„opæ–‡ä»¶">#</a></h2>
<p>mllmæä¾›äº†<code>src/backends/new_op.py</code>å®ç”¨å·¥å…·æ¥å¸®åŠ©åˆ›å»ºOp Classã€‚è¯¥æ–‡ä»¶ä¼šå¸®åŠ©è¯»è€…åˆ›å»ºä¸‹è¿°åŸºæœ¬å‡½æ•°ï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">ErrorCode</span> <span class="nf">reshape</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span> <span class="n">outputs</span><span class="p">)</span> <span class="k">override</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">ErrorCode</span> <span class="nf">execute</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span> <span class="n">outputs</span><span class="p">)</span> <span class="k">override</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">ErrorCode</span> <span class="nf">load</span><span class="p">(</span><span class="n">AbstructLoader</span> <span class="o">&amp;</span><span class="n">loader</span><span class="p">)</span> <span class="k">override</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">ErrorCode</span> <span class="nf">free</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span> <span class="n">outputs</span><span class="p">)</span> <span class="k">override</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">ErrorCode</span> <span class="nf">setUp</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span> <span class="n">outputs</span><span class="p">)</span> <span class="k">override</span><span class="p">;</span>
</span></span></code></pre></div><h2 id="32-opå‚æ•°è‡ªå®šä¹‰">3.2 Opå‚æ•°è‡ªå®šä¹‰<a hidden class="anchor" aria-hidden="true" href="#32-opå‚æ•°è‡ªå®šä¹‰">#</a></h2>
<p>æ¯”å¦‚å¯¹äºCPUä¸Šçš„LinearOpï¼Œéœ€è¦<code>in_features</code>ã€<code>out_features</code>å’Œ<code>has_bias</code>ä¸‰ä¸ªå‚æ•°ã€‚é‚£ä¹ˆå¯ä»¥åœ¨3.1è‡ªåŠ¨ç”Ÿæˆçš„classä¸­åŠ å…¥ï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">CPULinear</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Op</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="p">...</span>
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">in_features_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">out_features_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">bool</span> <span class="n">support_bias_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">thread_count</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Tensor</span> <span class="n">weight_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Tensor</span> <span class="n">bias_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div><p>åœ¨CPULinearCreatorä¸­åŠ å…¥ï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">CPULinearCreator</span> <span class="o">:</span> <span class="k">public</span> <span class="n">CPUBackend</span><span class="o">::</span><span class="n">Creator</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">virtual</span> <span class="n">Op</span> <span class="o">*</span><span class="n">create</span><span class="p">(</span><span class="n">OpParam</span> <span class="n">op_param</span><span class="p">,</span> <span class="n">Backend</span> <span class="o">*</span><span class="n">bn</span><span class="p">,</span> <span class="n">string</span> <span class="n">name</span><span class="p">,</span> <span class="kt">int</span> <span class="n">threadCount</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="kt">int</span> <span class="n">in_features</span> <span class="o">=</span> <span class="n">op_param</span><span class="p">[</span><span class="s">&#34;in_features&#34;</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="kt">int</span> <span class="n">out_features</span> <span class="o">=</span> <span class="n">op_param</span><span class="p">[</span><span class="s">&#34;out_features&#34;</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="kt">int</span> <span class="n">bias</span> <span class="o">=</span> <span class="n">op_param</span><span class="p">[</span><span class="s">&#34;bias&#34;</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="k">new</span> <span class="nf">CPULinear</span><span class="p">(</span><span class="n">bn</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="p">(</span><span class="kt">bool</span><span class="p">)</span><span class="n">bias</span><span class="p">,</span> <span class="n">threadCount</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div><p>è¯·æ³¨æ„ï¼ŒOpParamæ˜¯ä¸€ä¸ªstring-float mapã€‚</p>
<h2 id="33-é‡è½½å‡½æ•°">3.3 é‡è½½å‡½æ•°<a hidden class="anchor" aria-hidden="true" href="#33-é‡è½½å‡½æ•°">#</a></h2>
<p>è¯»è€…éœ€è¦è‡ªè¡Œå®ç°reshapeï¼Œexecuteï¼Œloadï¼Œfreeå‡½æ•°ï¼Œè§†æƒ…å†µé‡è½½setUpå‡½æ•°ã€‚
ä»¥Linear Opä¸ºä¾‹ï¼Œreshapeå‡½æ•°å°±ä¼šé€šè¿‡<code>in_features_</code>å˜é‡æ¥æ£€æŸ¥è¾“å…¥çš„Tensorçš„ç»´åº¦æ˜¯å¦æ­£ç¡®ï¼Œç„¶åå¯¹output Tensoråš<code>outputs[0]-&gt;reshape(inputs[0]-&gt;batch(), inputs[0]-&gt;head(), inputs[0]-&gt;sequence(), out_features_)</code></p>
<p>åœ¨loadå‡½æ•°ä¸­ï¼Œå®ç°Weightå’ŒBiasçš„åŠ è½½ã€‚</p>
<p>åœ¨executeå‡½æ•°ä¸­ï¼Œå…·ä½“å®ç°çŸ©é˜µä¹˜æ³•ç­‰è®¡ç®—æ“ä½œã€‚</p>
<p>åœ¨freeå‡½æ•°ä¸­é‡Šæ”¾Weightå’ŒBiasã€‚</p>
<h2 id="34-opæ˜¯å¦‚ä½•è¢«æ³¨å†Œå’Œåˆ›å»ºçš„">3.4 Opæ˜¯å¦‚ä½•è¢«æ³¨å†Œå’Œåˆ›å»ºçš„ï¼Ÿ<a hidden class="anchor" aria-hidden="true" href="#34-opæ˜¯å¦‚ä½•è¢«æ³¨å†Œå’Œåˆ›å»ºçš„">#</a></h2>
<p>åœ¨å®šä¹‰å®ŒæˆOpåï¼Œè¯»è€…è¿˜éœ€è¦æŠŠè¯¥Opæ³¨å†Œåˆ°ç›¸åº”çš„Backendä¸­ï¼Œä»¥åŠå°†OpæŠ½è±¡æˆLayerã€‚</p>
<h3 id="341-åœ¨backendä¸­æ³¨å†Œop">3.4.1 åœ¨Backendä¸­æ³¨å†ŒOp<a hidden class="anchor" aria-hidden="true" href="#341-åœ¨backendä¸­æ³¨å†Œop">#</a></h3>
<p>ä»¥CPU Backendä¸ºä¾‹ï¼Œè¯»è€…éœ€è¦å†CPUBackendæ–‡ä»¶ä¸­åŠ å…¥<code>addCreator(LINEAR, (CPUBackend::Creator *)(new CPULinearCreator()));</code></p>
<p>å¦‚æœè¿™æ˜¯ä¸€ä¸ªæ–°çš„ç®—å­ï¼Œè¯»è€…è¿˜éœ€è¦åœ¨<code>OpDefined</code>æ–‡ä»¶ä¸­åŠ å…¥æ–°Opçš„Enumé¡¹ã€‚</p>
<h3 id="342-åœ¨layerhppä¸­åŠ å…¥å¯¹åº”çš„op-layer">3.4.2 åœ¨Layer.hppä¸­åŠ å…¥å¯¹åº”çš„Op Layer<a hidden class="anchor" aria-hidden="true" href="#342-åœ¨layerhppä¸­åŠ å…¥å¯¹åº”çš„op-layer">#</a></h3>
<p>å¦‚Linear Layer:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Linear</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Layer</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">explicit</span> <span class="n">Linear</span><span class="p">(</span><span class="kt">int</span> <span class="n">in_features</span><span class="p">,</span> <span class="kt">int</span> <span class="n">out_features</span><span class="p">,</span> <span class="kt">bool</span> <span class="n">bias</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">name</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">param_</span><span class="p">[</span><span class="s">&#34;in_features&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">in_features</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">param_</span><span class="p">[</span><span class="s">&#34;out_features&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">out_features</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">param_</span><span class="p">[</span><span class="s">&#34;bias&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="n">bias</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">init</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">OpType</span><span class="o">::</span><span class="n">LINEAR</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">Tensor</span> <span class="o">&amp;</span><span class="k">operator</span><span class="p">()(</span><span class="n">Tensor</span> <span class="o">&amp;</span><span class="n">input</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="nf">_1I1O_OP</span><span class="p">(</span><span class="n">input</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div><p><strong>å…¶ä¸­ï¼Œåœ¨æ„é€ å‡½æ•°ä¸­çš„</strong><code>**init()**</code><strong>å‡½æ•°å¹¶æ²¡æœ‰åˆ›å»ºè¿™ä¸ªLinearç®—å­ã€‚å®ƒåªæ˜¯è´Ÿè´£ç»™è¿™ä¸ªLinearæŒ‡æ´¾äº†Backendã€‚</strong>
çœŸæ­£çš„ç®—å­åˆ›å»ºè¿˜æ˜¯åœ¨<code>INIT_OP()</code>å‡½æ•°ä¸­ã€‚åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œå®ƒä¼šé€šè¿‡<code>backend_-&gt;opCreate(param_, name_);</code>æ¥åˆ›å»ºç®—å­ã€‚</p>
<h1 id="4-tokenizer">4. Tokenizer<a hidden class="anchor" aria-hidden="true" href="#4-tokenizer">#</a></h1>
<p>mllmæä¾›äº†åŸºç¡€çš„Tokenizeræ”¯æŒï¼Œç›®å‰æ”¯æŒBPEå’ŒUnigramä¸¤ç§åˆ†è¯ç®—æ³•ã€‚</p>
<h1 id="5-å¦‚ä½•å¯¹æ–°æ¨¡å‹è¿›è¡Œæ”¯æŒ">5. å¦‚ä½•å¯¹æ–°æ¨¡å‹è¿›è¡Œæ”¯æŒ<a hidden class="anchor" aria-hidden="true" href="#5-å¦‚ä½•å¯¹æ–°æ¨¡å‹è¿›è¡Œæ”¯æŒ">#</a></h1>
<p>åœ¨mllmä¸­ï¼Œå¯¹æ¨¡å‹ç»„ä»¶ï¼ˆmodelã€Tokenizerã€Configurationï¼‰çš„å®šä¹‰å’ŒHuggingFace Transformeråº“ä¸­çš„å®šä¹‰æ–¹æ³•åŸºæœ¬ä¸€è‡´ã€‚ä»¥æ”¯æŒQWen0.5Bæ¨¡å‹ä¸ºä¾‹ï¼Œéœ€è¦ç¼–å†™ä¸‰ä¸ªæ–‡ä»¶ï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">configuration_qwen.cpp
</span></span><span class="line"><span class="cl">modeling_qwen.cpp
</span></span><span class="line"><span class="cl">tokenization_qwen.cpp
</span></span></code></pre></div><p>å…¶ä¸­<code>configuration_qwen.cpp</code>å®šä¹‰äº†Qwen LLMçš„å„ç±»å‚æ•°ï¼Œå¦‚Headæ•°é‡ï¼Œhidden dimç­‰ã€‚<code>modeling_qwen.cpp</code>å®šä¹‰äº†Qwen LLMç½‘ç»œã€‚<code>tokenization_qwen.cpp</code>åŒ…å«äº†å°†å¥å­è½¬åŒ–ä¸ºTokençš„é¢„å¤„ç†è¡Œä¸ºã€‚</p>
<h2 id="51-ç”Ÿæˆmllmæ”¯æŒçš„vocabå’Œæ¨¡å‹å‚æ•°">5.1 ç”Ÿæˆmllmæ”¯æŒçš„vocabå’Œæ¨¡å‹å‚æ•°<a hidden class="anchor" aria-hidden="true" href="#51-ç”Ÿæˆmllmæ”¯æŒçš„vocabå’Œæ¨¡å‹å‚æ•°">#</a></h2>
<h3 id="511-æ¨¡å‹è½¬æ¢">5.1.1 æ¨¡å‹è½¬æ¢<a hidden class="anchor" aria-hidden="true" href="#511-æ¨¡å‹è½¬æ¢">#</a></h3>
<p>ä½¿ç”¨mllmæä¾›çš„Converterå®ç”¨å·¥å…·æ¥è¿›è¡Œè½¬æ¢ï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">cd</span> tools/convertor
</span></span><span class="line"><span class="cl">pip install -r ./requirements.txt
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># for one file pytorch model</span>
</span></span><span class="line"><span class="cl">python convert.py --input_model<span class="o">=</span>model.pth --output_model<span class="o">=</span>model.mllm --type<span class="o">=</span>torch
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># for multi-file pytorch model</span>
</span></span><span class="line"><span class="cl">python convert.py --input_model<span class="o">=</span>pytorch_model.bin.index.json --output_model<span class="o">=</span>model.mllm --type<span class="o">=</span>torch
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># for one file safetensor model</span>
</span></span><span class="line"><span class="cl">python convert.py --input_model<span class="o">=</span>model.bin --output_model<span class="o">=</span>model.mllm --type<span class="o">=</span>safetensor
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># for multi-file safetensor model</span>
</span></span><span class="line"><span class="cl">python convert.py --input_model<span class="o">=</span>model.safetensors.index.json --output_model<span class="o">=</span>model.mllm --type<span class="o">=</span>safetensor
</span></span></code></pre></div><h3 id="512-vocabè½¬æ¢">5.1.2 Vocabè½¬æ¢<a hidden class="anchor" aria-hidden="true" href="#512-vocabè½¬æ¢">#</a></h3>
<p>ä½¿ç”¨mllmæä¾›çš„Converterå®ç”¨å·¥å…·æ¥è¿›è¡Œè½¬æ¢ï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">cd</span> tools/convertor
</span></span><span class="line"><span class="cl">python vocab.py --input_file<span class="o">=</span>tokenizer.json --output_file<span class="o">=</span>vocab.mllm --type<span class="o">=</span>Unigram
</span></span></code></pre></div><h3 id="513-é‡åŒ–">5.1.3 é‡åŒ–<a hidden class="anchor" aria-hidden="true" href="#513-é‡åŒ–">#</a></h3>
<p>mllmæä¾›äº†é‡åŒ–å·¥å…·ï¼Œè¯¥å·¥å…·æ”¯æŒ4 bitså’Œ6 bitsæ•´æ•°é‡åŒ–ï¼Œä½ å¯ä»¥ä½¿ç”¨ä¸‹è¿°æŒ‡ä»¤æ¥å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œé‡åŒ–</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">cd</span> bin
</span></span><span class="line"><span class="cl">./quantize model.mllm model_q4_0.mllm Q4_K
</span></span></code></pre></div><h2 id="52-configuration">5.2 Configuration<a hidden class="anchor" aria-hidden="true" href="#52-configuration">#</a></h2>
<p>è®¾ç½®æ–‡ä»¶é‡Œé¢ä¸»è¦å®ç°ä¸¤ä¸ªç±»ï¼Œä¸€ä¸ªæ˜¯<code>QWenNameConfig</code>ï¼Œä¸€ä¸ªæ˜¯<code>QWenConfig</code>ï¼Œå…¶ä¸­<code>QWenNameConfig</code>åŒ…å«<code>QWenConfig</code>ã€‚åœ¨ä¸€ä¸ªmllmæ¨¡å‹å‚æ•°æ–‡ä»¶ä¸­ï¼Œæ¨¡å‹å‚æ•°æ˜¯ä»¥key-valueå¯¹çš„å½¢å¼ç»Ÿä¸€èµ·æ¥çš„ã€‚<code>QWenNameConfig</code>çš„ç›®çš„å°±æ˜¯ç»™å‡ºæ¯ä¸ªå‚æ•°çš„åç§°ï¼Œä»¥ä¾¿äºmllmæ¡†æ¶ç´¢å¼•åˆ°æ­£ç¡®çš„æ¨¡å‹å‚æ•°ã€‚</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">QWenNameConfig</span> <span class="o">:</span> <span class="k">public</span> <span class="n">TransformerNameConfig</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="cm">/**
</span></span></span><span class="line"><span class="cl"><span class="cm">     * @brief QWen2 following the hugging face naming method
</span></span></span><span class="line"><span class="cl"><span class="cm">     *
</span></span></span><span class="line"><span class="cl"><span class="cm">     * @param type RoPEType
</span></span></span><span class="line"><span class="cl"><span class="cm">     */</span>
</span></span><span class="line"><span class="cl">    <span class="kt">void</span> <span class="n">init</span><span class="p">(</span><span class="n">RoPEType</span> <span class="n">type</span> <span class="o">=</span> <span class="n">RoPEType</span><span class="o">::</span><span class="n">HFHUBROPE</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">switch</span> <span class="p">(</span><span class="n">type</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">case</span> <span class="n">RoPEType</span><span class="o">::</span><span class="nl">HFHUBROPE</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">blk_name</span> <span class="o">=</span> <span class="s">&#34;model.layers.&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_attn_base_name</span> <span class="o">=</span> <span class="s">&#34;self_attn.&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_ffn_base_name</span> <span class="o">=</span> <span class="s">&#34;mlp.&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_q_proj_name</span> <span class="o">=</span> <span class="s">&#34;q_proj&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_k_proj_name</span> <span class="o">=</span> <span class="s">&#34;k_proj&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_v_proj_name</span> <span class="o">=</span> <span class="s">&#34;v_proj&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_o_proj_name</span> <span class="o">=</span> <span class="s">&#34;o_proj&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_gate_proj_name</span> <span class="o">=</span> <span class="s">&#34;gate_proj&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_up_proj_name</span> <span class="o">=</span> <span class="s">&#34;up_proj&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_down_proj_name</span> <span class="o">=</span> <span class="s">&#34;down_proj&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_attn_norm_name</span> <span class="o">=</span> <span class="s">&#34;input_layernorm&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_ffn_norm_name</span> <span class="o">=</span> <span class="s">&#34;post_attention_layernorm&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">token_embd_name</span> <span class="o">=</span> <span class="s">&#34;model.embed_tokens&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">post_norm_name</span> <span class="o">=</span> <span class="s">&#34;model.norm&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">lm_head_name</span> <span class="o">=</span> <span class="s">&#34;lm_head&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="k">break</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">...</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">blk_name</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">token_embd_name</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">post_norm_name</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">lm_head_name</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">_gate_proj_name</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div><p>åœ¨<code>QWenConfig</code>ä¸­åˆ™ä¸»è¦å®šä¹‰å„å±‚çš„è¶…å‚æ•°ï¼Œå¦‚ropeçš„thetaå€¼ã€ä¸­é—´å±‚ç»´åº¦å¤§å°ç­‰ï¼Œå¦‚ä¸‹é¢çš„ä»£ç æ‰€ç¤ºï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">QWenConfig</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">explicit</span> <span class="nf">QWenConfig</span><span class="p">(</span><span class="kt">int</span> <span class="n">token_limit</span><span class="p">,</span> <span class="n">string</span> <span class="n">billions</span> <span class="o">=</span> <span class="s">&#34;0.5B&#34;</span><span class="p">,</span> <span class="n">RoPEType</span> <span class="n">type</span> <span class="o">=</span> <span class="n">RoPEType</span><span class="o">::</span><span class="n">HFHUBROPE</span><span class="p">)</span> <span class="o">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">cache_limit</span><span class="p">(</span><span class="n">token_limit</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="p">...</span>
</span></span><span class="line"><span class="cl">    <span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">attention_dropout</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">bos_token_id</span> <span class="o">=</span> <span class="mi">151643</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">eos_token_id</span> <span class="o">=</span> <span class="mi">151643</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">hidden_act</span> <span class="o">=</span> <span class="s">&#34;silu&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">initializer_range</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">intermediate_size</span> <span class="o">=</span> <span class="mi">2816</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="mi">32768</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">max_window_layers</span> <span class="o">=</span> <span class="mi">21</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">model_type</span> <span class="o">=</span> <span class="s">&#34;qwen2&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">num_attention_heads</span> <span class="o">=</span> <span class="mi">16</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="mi">24</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">num_key_value_heads</span> <span class="o">=</span> <span class="mi">16</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">double</span> <span class="n">rms_norm_eps</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">rope_theta</span> <span class="o">=</span> <span class="mf">1000000.0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">sliding_window</span> <span class="o">=</span> <span class="mi">32768</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">151936</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">bool</span> <span class="n">tie_embedding_words</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">cache_limit</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">RoPEType</span> <span class="n">RoPE_type</span> <span class="o">=</span> <span class="n">RoPEType</span><span class="o">::</span><span class="n">HFHUBROPE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenNameConfig</span> <span class="n">names_config</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div><h2 id="53-tokenization">5.3 Tokenization<a hidden class="anchor" aria-hidden="true" href="#53-tokenization">#</a></h2>
<p>Tokenizationæ˜¯ä¸€ä¸ªéå¸¸å®¢åˆ¶åŒ–çš„æ­¥éª¤ï¼Œæ¯ä¸ªLLMçš„Tokenizationæ–¹æ³•éƒ½ä¸å°½ç›¸åŒã€‚ä»¥QWenä¸ºä¾‹å­ï¼ŒQWenä½¿ç”¨äº†BBPEæ–¹æ³•ï¼Œé‚£ä¹ˆè¯»è€…åœ¨æ”¯æŒQWenæ¨¡å‹çš„æ—¶å€™ï¼Œå°±è¦ç»™å‡ºå®ç°äº†BBPEçš„Tokenizerã€‚mllmå†…éƒ¨å·²ç»å®ç°ä¸€ä¸ªBPEç®—æ³•ï¼Œè¯»è€…å¯ä»¥å¤ç”¨è¯¥å®ç°æ¥å®ç°è‡ªå·±çš„Tokenizerã€‚</p>
<h2 id="54-modeling">5.4 Modeling<a hidden class="anchor" aria-hidden="true" href="#54-modeling">#</a></h2>
<p>ä½¿ç”¨mllmæ¡†æ¶æä¾›çš„ç®—å­æ¥å®ç°æ¨¡å‹æ˜¯éå¸¸ç®€å•å’Œä¾¿åˆ©çš„ï¼Œç†Ÿæ‚‰Pytorchçš„è¯»è€…å¯ä»¥å¿«é€Ÿçš„ä¸Šæ‰‹mllmã€‚æœ¬æ–‡åœ¨è¿™é‡Œé»˜è®¤è¯»è€…å¯¹llama/qwen/mistralç­‰å¸¸è§LLMçš„æ¨¡å‹æœ‰ç€åŸºæœ¬çš„äº†è§£ã€‚åœ¨ä¸‹æ–‡ä¸­ï¼Œæœ¬æ–‡ä»¥Attentionæ¨¡å—ä¸ºä¾‹æ¥æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨mllmæ¥æ­å»ºæ¨¡å‹ã€‚
é¦–å…ˆï¼Œæ‰€æœ‰çš„classéœ€è¦ç»§æ‰¿<code>Module</code>çˆ¶ç±»ã€‚<code>Module</code>çˆ¶ç±»æä¾›äº†<code>Forward</code>å‡½æ•°ï¼Œè¯»è€…éœ€è¦é‡è½½è¯¥å‡½æ•°æ¥å®ç°ç›¸åº”çš„è®¡ç®—æµç¨‹ã€‚</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">QWenAttention</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Module</span> <span class="p">...</span>
</span></span></code></pre></div><h3 id="541-åˆ›å»ºè¯¥moduleéœ€è¦ä½¿ç”¨çš„layers">5.4.1 åˆ›å»ºè¯¥Moduleéœ€è¦ä½¿ç”¨çš„Layers<a hidden class="anchor" aria-hidden="true" href="#541-åˆ›å»ºè¯¥moduleéœ€è¦ä½¿ç”¨çš„layers">#</a></h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">QWenAttention</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Module</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="err">ï¼š</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenAttention</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenAttention</span><span class="p">(</span><span class="k">const</span> <span class="n">QWenConfig</span> <span class="o">&amp;</span><span class="n">config</span><span class="p">,</span> <span class="k">const</span> <span class="n">QWenNameConfig</span> <span class="o">&amp;</span><span class="n">names</span><span class="p">,</span> <span class="k">const</span> <span class="n">string</span> <span class="o">&amp;</span><span class="n">base_name</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_key_value_heads</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">num_key_value_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_key_value_groups</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">/</span> <span class="n">num_key_value_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// init layers
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">q_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">,</span> <span class="nb">true</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_q_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">,</span> <span class="nb">true</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_k_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">,</span> <span class="nb">true</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_v_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">o_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="nb">false</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_o_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">q_rope</span> <span class="o">=</span> <span class="n">RoPE</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">RoPE_type</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">rope_theta</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">max_position_embeddings</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;q_rope&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_rope</span> <span class="o">=</span> <span class="n">RoPE</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">RoPE_type</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">rope_theta</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">max_position_embeddings</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;k_rope&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_cache</span> <span class="o">=</span> <span class="n">KVCache</span><span class="p">(</span><span class="n">num_key_value_groups</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">cache_limit</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;k_cache&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_cache</span> <span class="o">=</span> <span class="n">KVCache</span><span class="p">(</span><span class="n">num_key_value_groups</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">cache_limit</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;v_cache&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask</span> <span class="o">=</span> <span class="n">Causalmask</span><span class="p">(</span><span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;mask&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">softmax</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">DIMENSION</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;softmax&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">hidden_size</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">num_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">head_dim</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">num_key_value_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">num_key_value_groups</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">q_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">k_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">v_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">o_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">q_rope</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">k_rope</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">k_cache</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">v_cache</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">mask</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">softmax</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>ç»†å¿ƒçš„è¯»è€…å¯èƒ½å·²ç»å‘ç°äº†ï¼Œåœ¨<code>QWenAttention</code>çš„æ„é€ å‡½æ•°ä¸­ï¼Œåˆ›å»ºæ¯ä¸ªLayerçš„æ—¶å€™éƒ½åœ¨æœ€åä¸€ä¸ªå‚æ•°ä¸Šä¼ é€’äº†Layeråç§°ï¼ˆstd::string typeï¼‰ï¼Œè¿™æ˜¯å› ä¸ºmllmä¾èµ–äºLayerçš„åç§°æ¥å¯»æ‰¾è¯¥Layeræ‰€éœ€è¦çš„å‚æ•°ã€‚</p>
<h3 id="542-é‡è½½forwardå‰å‘æ¨ç†å‡½æ•°">5.4.2 é‡è½½Forwardå‰å‘æ¨ç†å‡½æ•°<a hidden class="anchor" aria-hidden="true" href="#542-é‡è½½forwardå‰å‘æ¨ç†å‡½æ•°">#</a></h3>
<p>åˆ›å»ºå®Œäº†æ‰€æœ‰æˆ‘ä»¬éœ€è¦çš„Layersä»¥åï¼Œå°±å¯ä»¥ç¼–å†™Forwardå‡½æ•°æ¥å®šä¹‰Attentionæ¨¡å—çš„è®¡ç®—æµç¨‹ï¼ŒForwardå‡½æ•°æ¥æ”¶ä¸€ä¸ªTensor Arrayå’Œä¸€ä¸ªstd::any Arrayï¼Œè¿”å›Tensor Arrayï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">Forward</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">any</span><span class="o">&gt;</span> <span class="n">args</span><span class="p">)</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">query_states</span> <span class="o">=</span> <span class="n">q_proj</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">key_states</span> <span class="o">=</span> <span class="n">k_proj</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">value_states</span> <span class="o">=</span> <span class="n">v_proj</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// [batch, heads, sequence, dims]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">query_states</span> <span class="o">=</span> <span class="n">query_states</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// embedding
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">query_states</span> <span class="o">=</span> <span class="n">q_rope</span><span class="p">(</span><span class="n">query_states</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">key_states</span> <span class="o">=</span> <span class="n">k_rope</span><span class="p">(</span><span class="n">key_states</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// kv cache
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">key_states</span> <span class="o">=</span> <span class="n">k_cache</span><span class="p">(</span><span class="n">key_states</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">value_states</span> <span class="o">=</span> <span class="n">v_cache</span><span class="p">(</span><span class="n">value_states</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// attention weight
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">atten_weight</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">::</span><span class="n">mm</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Chl</span><span class="o">::</span><span class="n">SEQUENCE</span><span class="p">,</span> <span class="n">Chl</span><span class="o">::</span><span class="n">DIMENSION</span><span class="p">))</span> <span class="o">/</span> <span class="n">std</span><span class="o">::</span><span class="n">sqrt</span><span class="p">(</span><span class="n">head_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">atten_weight</span> <span class="o">=</span> <span class="n">mask</span><span class="p">(</span><span class="n">atten_weight</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">atten_weight</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">atten_weight</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// attention output
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">atten_output</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">::</span><span class="n">mm</span><span class="p">(</span><span class="n">atten_weight</span><span class="p">,</span> <span class="n">value_states</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">atten_output</span> <span class="o">=</span> <span class="n">atten_output</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">atten_output</span> <span class="o">=</span> <span class="n">o_proj</span><span class="p">(</span><span class="n">atten_output</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">{</span><span class="n">atten_output</span><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h2 id="55-è¿è¡Œ">5.5 è¿è¡Œ<a hidden class="anchor" aria-hidden="true" href="#55-è¿è¡Œ">#</a></h2>
<p>å®Œæ•´çš„Qwenæ¨¡å‹å®šä¹‰ä»£ç å¯ä»¥åœ¨é™„å½•1ä¸­æ‰¾åˆ°ã€‚è¯»è€…å¯ä»¥åƒTorchä¸€æ ·è°ƒç”¨å®šä¹‰å¥½çš„æ¨¡å‹ï¼šé¦–å…ˆï¼Œåˆ›å»ºæ¨¡å‹ï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">QWenConfig</span> <span class="nf">config</span><span class="p">(</span><span class="n">tokens_limit</span><span class="p">,</span> <span class="s">&#34;0.5B&#34;</span><span class="p">,</span> <span class="n">RoPEType</span><span class="o">::</span><span class="n">HFHUBROPE</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="k">auto</span> <span class="n">model</span> <span class="o">=</span> <span class="n">QWenForCausalLM</span><span class="p">(</span><span class="n">config</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">);</span>
</span></span></code></pre></div><p><code>module</code>classé‡è½½äº†()operatorï¼Œè¯»è€…å¯ä»¥ä½¿ç”¨<code>model({input_tensor})</code>æ¥è¿›è¡Œæ¨ç†ã€‚</p>
<h1 id="6-mllmæ¡†æ¶çš„ä¸è¶³">6. mllmæ¡†æ¶çš„ä¸è¶³<a hidden class="anchor" aria-hidden="true" href="#6-mllmæ¡†æ¶çš„ä¸è¶³">#</a></h1>
<p>è¿™é‡Œå†™çš„æœ‰ç‚¹meanï¼Œæœ¬äººä¸“ä¸šçŸ¥è¯†æµ…è–„ï¼Œåœ¨å­¦æœ¯ä¸Šæ˜¯ä¾æ‰˜ç­”è¾©ï¼Œå¯¹mllmçš„ç†è§£æ›´æ˜¯ä¸åˆ°ä½ï¼Œå¤§å®¶è½»å–·ã€‚</p>
<h2 id="61-benchmark">6.1 Benchmark<a hidden class="anchor" aria-hidden="true" href="#61-benchmark">#</a></h2>
<ol>
<li>ç¼ºå°‘ç®—å­çš„Benchmark</li>
</ol>
<p>æœ¬æ–‡è®¤ä¸ºï¼Œmllmåœ¨å®ç°çš„æ—¶å€™æåŠ›çš„é¿å…ä½¿ç”¨ç¬¬ä¸‰æ–¹çš„åº“ï¼Œå› ä¸ºmllméœ€è¦è¿ç§»åˆ°ç§»åŠ¨è®¾å¤‡ä¸Šï¼Œä¸€äº›ä¸‰æ–¹åº“å¯èƒ½ä¸èƒ½æ­£å¸¸å·¥ä½œã€‚ä½†æ˜¯æ‰‹å·¥å®ç°çš„Kernelè¿˜æ˜¯éœ€è¦ä¸€ä¸ªBenchmarkæ¥å’Œç›®æ ‡å¹³å°ä¸Šæä¾›çš„ç®—å­åº“æ¥è¿›è¡Œæ€§èƒ½æ¯”è¾ƒçš„ã€‚å°±mllmç›®å‰æä¾›çš„MatMul Kernelæ¥çœ‹ï¼Œä¼¼ä¹ç¼ºå°‘Packä¼˜åŒ–å’Œ/micro Kernelçš„ä¼˜åŒ–ï¼Ÿ</p>
<ol start="2">
<li>ç¼ºå°‘prefill/decodeçš„Benchmark</li>
</ol>
<p>mllmçš„issuesä¸­ä¹Ÿæœ‰äººæåˆ°è¿‡è¿™ä¸ªé—®é¢˜ã€‚ä½œä¸ºå…·æœ‰LLMæ¨ç†èƒ½åŠ›çš„å¼•æ“ï¼Œåº”å½“æµ‹ä¸€ä¸‹è¿™ä¸¤ä¸ªåŸºæœ¬èƒ½åŠ›ã€‚</p>
<h2 id="62-å¯¹äºç§»åŠ¨ç«¯llmæ¨ç†çš„ç‰¹å®šä¼˜åŒ–">6.2 å¯¹äºç§»åŠ¨ç«¯LLMæ¨ç†çš„ç‰¹å®šä¼˜åŒ–<a hidden class="anchor" aria-hidden="true" href="#62-å¯¹äºç§»åŠ¨ç«¯llmæ¨ç†çš„ç‰¹å®šä¼˜åŒ–">#</a></h2>
<ol>
<li>KV Cacheé‡åŒ–</li>
</ol>
<p>IIRCï¼Œåœ¨OPPOçš„Transformer-Lite[2]ä¸­ï¼Œç”¨åˆ°äº†KV Cacheé‡åŒ–çš„å°æŠ€å·§ã€‚è¿™å¯¹ç§»åŠ¨è®¾å¤‡æœ‰é™çš„å†…å­˜æ¥è¯´å¯èƒ½ä¼šæ›´åŠ å‹å¥½ï¼Œå½“ç„¶è¿˜éœ€è¦è€ƒé‡é‡åŒ–å¸¦æ¥çš„CPUè´Ÿè½½é—®é¢˜ã€‚</p>
<ol start="2">
<li>åŠ¨æ€å½¢çŠ¶æ¨ç†/å†…å­˜å¤ç”¨/KV Cacheæ¬ç§»ä¼˜åŒ–</li>
</ol>
<p>ç›®å‰mllmæ˜¯æ²¡æœ‰åšå†…å­˜å¤ç”¨çš„ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ç¬¦å·æ¨ç†æ–¹æ³•æ¥åšåŠ¨æ€å½¢çŠ¶çš„æ”¯æŒè¿›è€Œä¾¿äºæ±‚è§£ä¸‹ä¸€è½®çš„å†…å­˜ä½¿ç”¨æƒ…å†µã€‚æˆ–è®¸å¯ä»¥è€ƒè™‘ä¸€ä¸‹PageAttention[3]çš„Tensorç®¡ç†æ–¹æ³•æˆ–è€…[2]ä¸­çš„KV Cacheè§„åˆ’æ–¹æ³•æ¥è¿›ä¸€æ­¥å‡å°‘å†…å­˜çš„æ¬ç§»ã€‚</p>
<ol start="3">
<li>å¼‚æ„ç®—åŠ›</li>
</ol>
<p>å¯ä»¥è€ƒè™‘æŠŠå½¢çŠ¶æ¨ç†ï¼ˆCPUï¼‰å’Œè®¡ç®—ï¼ˆGPU/NPUï¼‰å¹¶è¡Œæ‰§è¡Œèµ·æ¥ã€‚æˆ–è€…æ˜¯6.2.4ä¸­æåˆ°çš„å†…å®¹ä¸è®¡ç®—å¹¶è¡Œèµ·æ¥ã€‚</p>
<ol start="4">
<li>å¯¹æ¨¡å‹å‚æ•°çš„Lazy Fetchå’ŒPre Fetch</li>
</ol>
<p>ç›®å‰ï¼Œmllmä¼šæŠŠå‚æ•°ä¸€æ¬¡æ€§çš„è¯»å…¥å†…å­˜ï¼Ÿè€ƒè™‘åˆ°ç§»åŠ¨è®¾å¤‡çš„å†…å­˜æœ‰é™ï¼Œå¯ä»¥åœ¨åˆé€‚çš„æ—¶æœºæå‰ä»å¤–å­˜ä¸Šé¢„å–è€Œä¸æ˜¯å…¨æ•°è½½å…¥ã€‚</p>
<h2 id="63-æ˜“ç”¨æ€§">6.3 æ˜“ç”¨æ€§<a hidden class="anchor" aria-hidden="true" href="#63-æ˜“ç”¨æ€§">#</a></h2>
<ol>
<li>æ¨¡å‹ç»“æ„éœ€è¦æ‰‹åŠ¨ç¼–å†™ä¸”æ— æ³•ä¿å­˜</li>
</ol>
<p>ç›®å‰ï¼Œmllmçš„æ¨¡å‹ç»“æ„è¿˜æ˜¯éœ€è¦åœ¨C++æ–‡ä»¶ä¸­è¿›è¡Œæ˜¾ç¤ºçš„æ‰‹åŠ¨å®šä¹‰ã€‚æˆ–è®¸å¯ä»¥è€ƒè™‘åˆ›å»ºè‡ªå·±çš„è®¡ç®—å›¾å’Œç®—å­æè¿°æ–¹å¼ï¼Œä½¿ç”¨flatbuffersæ¥å­˜å‚¨è®¡ç®—å›¾ã€‚</p>
<ol start="2">
<li>å¦‚æœè¦å¾ˆå¥½çš„ä½¿ç”¨æ‰€æœ‰çš„ç®—åŠ›ï¼Œå¯èƒ½è¿˜æ˜¯éœ€è¦å®Œå–„çš„è®¡ç®—å›¾æœºåˆ¶ï¼Œè¿™æ ·ä¾¿äºä¼˜åŒ–åˆ†æã€‚</li>
<li>å°è¯•å¼•å…¥ä¸‰æ–¹æ˜“ç”¨çš„åº“å¦‚icuç­‰æ¥å¼¥è¡¥C++ utf-8å¤„ç†èƒ½åŠ›çš„ä¸è¶³ã€‚</li>
</ol>
<hr>
<p><strong>Refï¼š</strong></p>
<p>[1] mllm, <a href="https://github.com/UbiquitousLearning/mllm">https://github.com/UbiquitousLearning/mllm</a></p>
<p>[2] transformer-lite, <a href="https://arxiv.org/abs/2403.20041">https://arxiv.org/abs/2403.20041</a></p>
<p>[3] PageAttention, <a href="https://arxiv.org/abs/2309.06180">https://arxiv.org/abs/2309.06180</a></p>
<h1 id="a1-qwenæ¨¡å‹å®šä¹‰">A1. Qwenæ¨¡å‹å®šä¹‰<a hidden class="anchor" aria-hidden="true" href="#a1-qwenæ¨¡å‹å®šä¹‰">#</a></h1>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#ifndef MODELING_QWEN_HPP
</span></span></span><span class="line"><span class="cl"><span class="cp">#define MODELING_QWEN_HPP
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;Backend.hpp&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;Layer.hpp&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;Module.hpp&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;Tensor.hpp&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;configuration_qwen.hpp&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;cmath&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="k">using</span> <span class="k">namespace</span> <span class="n">mllm</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Copied from GemmaMLP with Gemma-&gt;Qwen and using silu
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">class</span> <span class="nc">QWenMLP</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Module</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenMLP</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenMLP</span><span class="p">(</span><span class="kt">int</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="kt">int</span> <span class="n">intermediate_size</span><span class="p">,</span> <span class="k">const</span> <span class="n">QWenNameConfig</span> <span class="o">&amp;</span><span class="n">names</span><span class="p">,</span> <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="o">&amp;</span><span class="n">base_name</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">gate_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">intermediate_size</span><span class="p">,</span> <span class="nb">false</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_gate_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">silu</span> <span class="o">=</span> <span class="n">SiLU</span><span class="p">(</span><span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;act&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">up_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">intermediate_size</span><span class="p">,</span> <span class="nb">false</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_up_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">down_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="nb">false</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_down_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">Forward</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">any</span><span class="o">&gt;</span> <span class="n">args</span><span class="p">)</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">x</span> <span class="o">=</span> <span class="n">gate_proj</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">silu</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">y</span> <span class="o">=</span> <span class="n">up_proj</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">down_proj</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">{</span><span class="n">x</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">gate_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">up_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">down_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">silu</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Copied from GemmaAttention with Gemma-&gt;Qwen and using SWA
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">class</span> <span class="nc">QWenAttention</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Module</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenAttention</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenAttention</span><span class="p">(</span><span class="k">const</span> <span class="n">QWenConfig</span> <span class="o">&amp;</span><span class="n">config</span><span class="p">,</span> <span class="k">const</span> <span class="n">QWenNameConfig</span> <span class="o">&amp;</span><span class="n">names</span><span class="p">,</span> <span class="k">const</span> <span class="n">string</span> <span class="o">&amp;</span><span class="n">base_name</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_key_value_heads</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">num_key_value_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_key_value_groups</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">/</span> <span class="n">num_key_value_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// init layers
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">q_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">,</span> <span class="nb">true</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_q_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">,</span> <span class="nb">true</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_k_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">,</span> <span class="nb">true</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_v_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">o_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="nb">false</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_o_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">q_rope</span> <span class="o">=</span> <span class="n">RoPE</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">RoPE_type</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">rope_theta</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">max_position_embeddings</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;q_rope&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_rope</span> <span class="o">=</span> <span class="n">RoPE</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">RoPE_type</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">rope_theta</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">max_position_embeddings</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;k_rope&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_cache</span> <span class="o">=</span> <span class="n">KVCache</span><span class="p">(</span><span class="n">num_key_value_groups</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">cache_limit</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;k_cache&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_cache</span> <span class="o">=</span> <span class="n">KVCache</span><span class="p">(</span><span class="n">num_key_value_groups</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">cache_limit</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;v_cache&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// mask = SlidingWindowMask(config.sliding_window, base_name + &#34;mask&#34;);
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">mask</span> <span class="o">=</span> <span class="n">Causalmask</span><span class="p">(</span><span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;mask&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">softmax</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">DIMENSION</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;softmax&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">Forward</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">any</span><span class="o">&gt;</span> <span class="n">args</span><span class="p">)</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">query_states</span> <span class="o">=</span> <span class="n">q_proj</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">key_states</span> <span class="o">=</span> <span class="n">k_proj</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">value_states</span> <span class="o">=</span> <span class="n">v_proj</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// [batch, heads, sequence, dims]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">query_states</span> <span class="o">=</span> <span class="n">query_states</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// embedding
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">query_states</span> <span class="o">=</span> <span class="n">q_rope</span><span class="p">(</span><span class="n">query_states</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">key_states</span> <span class="o">=</span> <span class="n">k_rope</span><span class="p">(</span><span class="n">key_states</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// kv cache
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">key_states</span> <span class="o">=</span> <span class="n">k_cache</span><span class="p">(</span><span class="n">key_states</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">value_states</span> <span class="o">=</span> <span class="n">v_cache</span><span class="p">(</span><span class="n">value_states</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// attention weight
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">atten_weight</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">::</span><span class="n">mm</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Chl</span><span class="o">::</span><span class="n">SEQUENCE</span><span class="p">,</span> <span class="n">Chl</span><span class="o">::</span><span class="n">DIMENSION</span><span class="p">))</span> <span class="o">/</span> <span class="n">std</span><span class="o">::</span><span class="n">sqrt</span><span class="p">(</span><span class="n">head_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">atten_weight</span> <span class="o">=</span> <span class="n">mask</span><span class="p">(</span><span class="n">atten_weight</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">atten_weight</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">atten_weight</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// attention output
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">atten_output</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">::</span><span class="n">mm</span><span class="p">(</span><span class="n">atten_weight</span><span class="p">,</span> <span class="n">value_states</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">atten_output</span> <span class="o">=</span> <span class="n">atten_output</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">atten_output</span> <span class="o">=</span> <span class="n">o_proj</span><span class="p">(</span><span class="n">atten_output</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">{</span><span class="n">atten_output</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">hidden_size</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">num_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">head_dim</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">num_key_value_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">num_key_value_groups</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">q_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">k_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">v_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">o_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">q_rope</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">k_rope</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">k_cache</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">v_cache</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">mask</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">softmax</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Copied from GemmaDecoder with Gemma-&gt;Qwen and set RmsNorm(without add_unit_offset)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">class</span> <span class="nc">QWenDecoder</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Module</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenDecoder</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenDecoder</span><span class="p">(</span><span class="k">const</span> <span class="n">QWenConfig</span> <span class="o">&amp;</span><span class="n">config</span><span class="p">,</span> <span class="k">const</span> <span class="n">QWenNameConfig</span> <span class="o">&amp;</span><span class="n">names</span><span class="p">,</span> <span class="k">const</span> <span class="n">string</span> <span class="o">&amp;</span><span class="n">base_name</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">self_atten</span> <span class="o">=</span> <span class="n">QWenAttention</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_attn_base_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">mlp</span> <span class="o">=</span> <span class="n">QWenMLP</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_ffn_base_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">input_layernorm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">rms_norm_eps</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_attn_norm_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">post_attention_layernorm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">rms_norm_eps</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_ffn_norm_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">Forward</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">any</span><span class="o">&gt;</span> <span class="n">args</span><span class="p">)</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">x</span> <span class="o">=</span> <span class="n">input_layernorm</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">self_atten</span><span class="p">({</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">})[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">tmp</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">post_attention_layernorm</span><span class="p">(</span><span class="n">tmp</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">({</span><span class="n">x</span><span class="p">})[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">tmp</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">{</span><span class="n">x</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenAttention</span> <span class="n">self_atten</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenMLP</span> <span class="n">mlp</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">input_layernorm</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">post_attention_layernorm</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Copied from GemmaModel with Gemma-&gt;Qwen and set RmsNorm(without add_unit_offset)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">class</span> <span class="nc">QWenModel</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Module</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenModel</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenModel</span><span class="p">(</span><span class="k">const</span> <span class="n">QWenConfig</span> <span class="o">&amp;</span><span class="n">config</span><span class="p">,</span> <span class="k">const</span> <span class="n">QWenNameConfig</span> <span class="o">&amp;</span><span class="n">names</span><span class="p">,</span> <span class="k">const</span> <span class="n">string</span> <span class="o">&amp;</span><span class="n">base_name</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">blocks</span> <span class="o">=</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">QWenDecoder</span><span class="o">&gt;</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">num_hidden_layers</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">base_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">rms_norm_eps</span><span class="p">,</span> <span class="n">names</span><span class="p">.</span><span class="n">post_norm_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">Forward</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">any</span><span class="o">&gt;</span> <span class="n">args</span><span class="p">)</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="o">&amp;</span><span class="nl">block</span> <span class="p">:</span> <span class="n">blocks</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">({</span><span class="n">x</span><span class="p">})[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">{</span><span class="n">x</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">QWenDecoder</span><span class="o">&gt;</span> <span class="n">blocks</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">norm</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">QWenForCausalLM</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Module</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenForCausalLM</span><span class="p">(</span><span class="n">QWenConfig</span> <span class="o">&amp;</span><span class="n">config</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">names</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">names_config</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">tie_embedding_words</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">tie_embedding_words</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">embedding</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">names</span><span class="p">.</span><span class="n">token_embd_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span> <span class="o">=</span> <span class="n">QWenModel</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">names</span><span class="p">.</span><span class="n">blk_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// FIXME Qwen-0.5 use tied embedding
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// Others use nn.Linear()
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="n">tie_embedding_words</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">lm_head</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">names</span><span class="p">.</span><span class="n">token_embd_name</span> <span class="o">+</span> <span class="s">&#34;.weight&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">Forward</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">any</span><span class="o">&gt;</span> <span class="n">args</span><span class="p">)</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">x</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// go through model
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">({</span><span class="n">x</span><span class="p">})[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">tie_embedding_words</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">outputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">::</span><span class="n">mm</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">lm_head</span><span class="p">().</span><span class="n">transpose</span><span class="p">(</span><span class="n">Chl</span><span class="o">::</span><span class="n">SEQUENCE</span><span class="p">,</span> <span class="n">Chl</span><span class="o">::</span><span class="n">DIMENSION</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">{</span><span class="n">outputs</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">hidden_size</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">bool</span> <span class="n">tie_embedding_words</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">embedding</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Parameter</span> <span class="n">lm_head</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenModel</span> <span class="n">model</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#endif </span><span class="c1">//! MODELING_QWEN_HPP
</span></span></span></code></pre></div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://chenghuawang.github.io/keep-moving-forward/tags/llm-server/">LLM Server</a></li>
      <li><a href="https://chenghuawang.github.io/keep-moving-forward/tags/llm/">LLM</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://chenghuawang.github.io/keep-moving-forward/tech/fundamental_from_online_softmax_to_flash_attentionv3/">
    <span class="title">Â« Prev</span>
    <br>
    <span>[Fundamental] From Online Softmax to Flash Attention V3</span>
  </a>
  <a class="next" href="https://chenghuawang.github.io/keep-moving-forward/papers/mlsys2024-qmoe/">
    <span class="title">Next Â»</span>
    <br>
    <span>âœ…[Oct 2023] QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share mllmæ¡†æ¶æµ…æ(ä¸€)-ä»¥QWen0.5Bä¸ºä¾‹ on x"
            href="https://x.com/intent/tweet/?text=mllm%e6%a1%86%e6%9e%b6%e6%b5%85%e6%9e%90%28%e4%b8%80%29-%e4%bb%a5QWen0.5B%e4%b8%ba%e4%be%8b&amp;url=https%3a%2f%2fchenghuawang.github.io%2fkeep-moving-forward%2ftech%2fmllm-qwen%2f&amp;hashtags=LLMServer%2cLLM">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share mllmæ¡†æ¶æµ…æ(ä¸€)-ä»¥QWen0.5Bä¸ºä¾‹ on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fchenghuawang.github.io%2fkeep-moving-forward%2ftech%2fmllm-qwen%2f&amp;title=mllm%e6%a1%86%e6%9e%b6%e6%b5%85%e6%9e%90%28%e4%b8%80%29-%e4%bb%a5QWen0.5B%e4%b8%ba%e4%be%8b&amp;summary=mllm%e6%a1%86%e6%9e%b6%e6%b5%85%e6%9e%90%28%e4%b8%80%29-%e4%bb%a5QWen0.5B%e4%b8%ba%e4%be%8b&amp;source=https%3a%2f%2fchenghuawang.github.io%2fkeep-moving-forward%2ftech%2fmllm-qwen%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share mllmæ¡†æ¶æµ…æ(ä¸€)-ä»¥QWen0.5Bä¸ºä¾‹ on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fchenghuawang.github.io%2fkeep-moving-forward%2ftech%2fmllm-qwen%2f&title=mllm%e6%a1%86%e6%9e%b6%e6%b5%85%e6%9e%90%28%e4%b8%80%29-%e4%bb%a5QWen0.5B%e4%b8%ba%e4%be%8b">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share mllmæ¡†æ¶æµ…æ(ä¸€)-ä»¥QWen0.5Bä¸ºä¾‹ on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fchenghuawang.github.io%2fkeep-moving-forward%2ftech%2fmllm-qwen%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share mllmæ¡†æ¶æµ…æ(ä¸€)-ä»¥QWen0.5Bä¸ºä¾‹ on whatsapp"
            href="https://api.whatsapp.com/send?text=mllm%e6%a1%86%e6%9e%b6%e6%b5%85%e6%9e%90%28%e4%b8%80%29-%e4%bb%a5QWen0.5B%e4%b8%ba%e4%be%8b%20-%20https%3a%2f%2fchenghuawang.github.io%2fkeep-moving-forward%2ftech%2fmllm-qwen%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share mllmæ¡†æ¶æµ…æ(ä¸€)-ä»¥QWen0.5Bä¸ºä¾‹ on telegram"
            href="https://telegram.me/share/url?text=mllm%e6%a1%86%e6%9e%b6%e6%b5%85%e6%9e%90%28%e4%b8%80%29-%e4%bb%a5QWen0.5B%e4%b8%ba%e4%be%8b&amp;url=https%3a%2f%2fchenghuawang.github.io%2fkeep-moving-forward%2ftech%2fmllm-qwen%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share mllmæ¡†æ¶æµ…æ(ä¸€)-ä»¥QWen0.5Bä¸ºä¾‹ on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=mllm%e6%a1%86%e6%9e%b6%e6%b5%85%e6%9e%90%28%e4%b8%80%29-%e4%bb%a5QWen0.5B%e4%b8%ba%e4%be%8b&u=https%3a%2f%2fchenghuawang.github.io%2fkeep-moving-forward%2ftech%2fmllm-qwen%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>Â© <a href="https://github.com/chenghuaWang">chenghua.wang</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a><script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js" integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<script>
const images = Array.from(document.querySelectorAll(".post-content img"));
images.forEach(img => {
  mediumZoom(img, {
    margin: 0,  
    scrollOffset: 40,  
    container: null,  
    template: null,  
    background: 'rgba(0, 0, 0, 0.8)'
  });
});
</script>
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5j20jf9ml5x&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>


<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
