<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>mllm框架浅析(一)-以QWen0.5B为例 | Ubios Home</title>
<meta name="keywords" content="LLM Server, LLM">
<meta name="description" content="以Qwen0.5B为例解析mllm的基本实现，CPU Backend">
<meta name="author" content="chenghua.Wang">
<link rel="canonical" href="https://chenghuawang.github.io/keep-moving-forward/tech/mllm-qwen/">
<link crossorigin="anonymous" href="/keep-moving-forward/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://chenghuawang.github.io/keep-moving-forward/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://chenghuawang.github.io/keep-moving-forward/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://chenghuawang.github.io/keep-moving-forward/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://chenghuawang.github.io/keep-moving-forward/apple-touch-icon.png">
<link rel="mask-icon" href="https://chenghuawang.github.io/keep-moving-forward/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://chenghuawang.github.io/keep-moving-forward/tech/mllm-qwen/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>



  

<meta property="og:title" content="mllm框架浅析(一)-以QWen0.5B为例" />
<meta property="og:description" content="以Qwen0.5B为例解析mllm的基本实现，CPU Backend" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chenghuawang.github.io/keep-moving-forward/tech/mllm-qwen/" /><meta property="article:section" content="tech" />
<meta property="article:published_time" content="2024-06-28T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-06-28T00:00:00+00:00" />



<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="mllm框架浅析(一)-以QWen0.5B为例"/>
<meta name="twitter:description" content="以Qwen0.5B为例解析mllm的基本实现，CPU Backend"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Technique",
      "item": "https://chenghuawang.github.io/keep-moving-forward/tech/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "mllm框架浅析(一)-以QWen0.5B为例",
      "item": "https://chenghuawang.github.io/keep-moving-forward/tech/mllm-qwen/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "mllm框架浅析(一)-以QWen0.5B为例",
  "name": "mllm框架浅析(一)-以QWen0.5B为例",
  "description": "以Qwen0.5B为例解析mllm的基本实现，CPU Backend",
  "keywords": [
    "LLM Server", "LLM"
  ],
  "articleBody": " 笔者最近在做一些mllm相关的工作，书写此文对mllm框架进行梳理总结，定有不少纰漏，请读者立即指出，谢谢。mllm目前在做一些其他工作，这篇文章的书写时间为发布时间。在mllm的其他工作合并进主仓库后，本文还会进一步的跟进。读者请注意本文的时效性。\n1. 简介 mllm是一款适用于移动设备和边缘设备的快速、轻量的多模态LLM推理引擎。\n完全的C/C++实现，无第三方依赖 针对fuyu-8B等多模态LLM进行了优化 支持ARM NEON和X86 AVX2向量指令 支持4 bits和6 bits整数量化 本文将更多的以工程的视角来解析mllm框架，在行文过程中，本文会将mllm与其他框架的设计方法做对比。接下来，本文将会用项目组织结构、框架执行流程、自定义Op/Layer、Tokenizer和如何支持新模型五个章节来详细描述mllm框架的各项特性和总体结构。读者可以把该文章做mllm的使用文档。在最后，本文将会指出mllm的不足之处和可以尝试跟进的工作。\n在开始正式解析mllm之前，读者可以先clone下mllm的代码库，以便于跟进分析流程。mllm不依赖于git submodule，项目配置起来很方便，目前mllm可以在linux上使用Clang/GCC编译器进行编译。目前mllm支持的目标设备体系结构是X86和Arm。\ngit clone https://github.com/UbiquitousLearning/mllm mllm团队将所有LLM相关的vocab文件都放在了git仓库中（这个其实可以移动到HuggingFace的仓库上），LLM量化后的模型文件都存储在HuggingFace上，读者可以在https://huggingface.co/mllmTeam上找到mllm提供的模型文件。\n2. 框架执行流程 2.1 以两层Linear层运行为例 首先，考虑下面的代码，定义了两个Linear Layers，并且输入$X$通过两个Linear Layers来得到输出：\nclass TwoLinear final : public Module { public: TwoLinear() = default; TwoLinear() { linear1 = Linear(in_f, out_f, /*bias*/true, \"linear1\"); linear2 = Linear(out_f, out_f, /*bias*/true, \"linear2\"); } std::vector\u003cTensor\u003e Forward(std::vector\u003cTensor\u003e inputs, std::vector\u003cstd::any\u003e args) override { x = inputs[0]; x = linear1(x); x = linear2(x); return x; } private: Layer linear1; Layer linear2; } TwoLinear tl; 2.1.1 加载参数 读者可以使用 tl.load(path)来加载参数。那么mllm是如何实现参数加载的呢？在load函数中，mllm会创建一个ParamLoader，这个ParamLoader是Static的，在全局可以访问。然后mllm会设置另一个全局参数doLoad为True，进而进入推理流程operator()(tmps, tmpt);。在推理流程中，要是执行层发现doLoad为True，那么就执行每个算子内定义好的load指令，而不是执行每个算子的原本逻辑。 load的执行在Layer.hpp文件的INIT_OP()中。\n2.1.2Module的Operator()是如何调用Forward函数的？ 对于常见的INPUT_TENSOR类型的Tensor，mllm首先会设置这个Tensor的类型为TENSOR_STATIC_INIT，进行一遍Forward推理；第一遍Forward推理完毕以后再把Tensor的类型设为TENSOR_STATIC_READY，然后进行第二遍Forward推理。\nif (inputs[0].ttype() == TensorType::INPUT_TENSOR) { for (auto \u0026input : inputs) { input.setTtype(TensorType::NORMAL_TENSOR); input.status() = TENSOR_STATIC_INIT; if(input.batch() == 0){ Tensor::gph_[input.name()] = input; } } tensor_status = TENSOR_STATIC_INIT; Forward(inputs, anyArgs); for (auto \u0026input : inputs) { input.status() = TENSOR_STATIC_READY; } tensor_status = TENSOR_STATIC_READY; return Forward(inputs, anyArgs); } 第一次Forward推理的目的是调用Op定义的Reshape和SetUp函数，Reshape函数会推理出这一次模型推理的过程中每个Tensor的形状大小。SetUp函数会对Op需要输出的Tensor做内存的申请。 第二次Forward推理才是真正的计算。\n2.1.3 Linear层的执行 每个Layer在实现的时候都会重载operator()，比如linear layer的operator()函数如下：\nTensor \u0026operator()(Tensor \u0026input) { return _1I1O_OP(input); } 其中，_1I1O_OP表示的意思是，这是需要使用1个输入和1个输出的函数来处理这个算子。mllm还提供了许多类似于_1I1O_OP的函数来处理不同的算子。\n2.2 总结 大体来说，mllm使用了类似于状态机的参数来设置了当前推理过程的运行状态。每一次都是通过Forward函数来进行全模型的遍历，在Op的执行过程中，用这些设定的参数来区分每次Op需要表现的行为。\n3. 如何编写Op与自定义Layer 3.1 新增对应Backend的Op文件 mllm提供了src/backends/new_op.py实用工具来帮助创建Op Class。该文件会帮助读者创建下述基本函数：\nErrorCode reshape(vector\u003cshared_ptr\u003cTensor\u003e\u003e inputs, vector\u003cshared_ptr\u003cTensor\u003e\u003e outputs) override; ErrorCode execute(vector\u003cshared_ptr\u003cTensor\u003e\u003e inputs, vector\u003cshared_ptr\u003cTensor\u003e\u003e outputs) override; ErrorCode load(AbstructLoader \u0026loader) override; ErrorCode free(vector\u003cshared_ptr\u003cTensor\u003e\u003e inputs, vector\u003cshared_ptr\u003cTensor\u003e\u003e outputs) override; ErrorCode setUp(vector\u003cshared_ptr\u003cTensor\u003e\u003e inputs, vector\u003cshared_ptr\u003cTensor\u003e\u003e outputs) override; 3.2 Op参数自定义 比如对于CPU上的LinearOp，需要in_features、out_features和has_bias三个参数。那么可以在3.1自动生成的class中加入：\nclass CPULinear final : public Op { ... private: int in_features_; int out_features_; bool support_bias_; int thread_count = 4; Tensor weight_; Tensor bias_; }; 在CPULinearCreator中加入：\nclass CPULinearCreator : public CPUBackend::Creator { public: virtual Op *create(OpParam op_param, Backend *bn, string name, int threadCount) const { int in_features = op_param[\"in_features\"]; int out_features = op_param[\"out_features\"]; int bias = op_param[\"bias\"]; return new CPULinear(bn, name, in_features, out_features, (bool)bias, threadCount); } }; 请注意，OpParam是一个string-float map。\n3.3 重载函数 读者需要自行实现reshape，execute，load，free函数，视情况重载setUp函数。 以Linear Op为例，reshape函数就会通过in_features_变量来检查输入的Tensor的维度是否正确，然后对output Tensor做outputs[0]-\u003ereshape(inputs[0]-\u003ebatch(), inputs[0]-\u003ehead(), inputs[0]-\u003esequence(), out_features_)\n在load函数中，实现Weight和Bias的加载。\n在execute函数中，具体实现矩阵乘法等计算操作。\n在free函数中释放Weight和Bias。\n3.4 Op是如何被注册和创建的？ 在定义完成Op后，读者还需要把该Op注册到相应的Backend中，以及将Op抽象成Layer。\n3.4.1 在Backend中注册Op 以CPU Backend为例，读者需要再CPUBackend文件中加入addCreator(LINEAR, (CPUBackend::Creator *)(new CPULinearCreator()));\n如果这是一个新的算子，读者还需要在OpDefined文件中加入新Op的Enum项。\n3.4.2 在Layer.hpp中加入对应的Op Layer 如Linear Layer:\nclass Linear final : public Layer { public: explicit Linear(int in_features, int out_features, bool bias, std::string name) { param_[\"in_features\"] = in_features; param_[\"out_features\"] = out_features; param_[\"bias\"] = (float)bias; init(std::move(name), OpType::LINEAR); } Tensor \u0026operator()(Tensor \u0026input) { return _1I1O_OP(input); } }; 其中，在构造函数中的**init()**函数并没有创建这个Linear算子。它只是负责给这个Linear指派了Backend。 真正的算子创建还是在INIT_OP()函数中。在这个函数中，它会通过backend_-\u003eopCreate(param_, name_);来创建算子。\n4. Tokenizer mllm提供了基础的Tokenizer支持，目前支持BPE和Unigram两种分词算法。\n5. 如何对新模型进行支持 在mllm中，对模型组件（model、Tokenizer、Configuration）的定义和HuggingFace Transformer库中的定义方法基本一致。以支持QWen0.5B模型为例，需要编写三个文件：\nconfiguration_qwen.cpp modeling_qwen.cpp tokenization_qwen.cpp 其中configuration_qwen.cpp定义了Qwen LLM的各类参数，如Head数量，hidden dim等。modeling_qwen.cpp定义了Qwen LLM网络。tokenization_qwen.cpp包含了将句子转化为Token的预处理行为。\n5.1 生成mllm支持的vocab和模型参数 5.1.1 模型转换 使用mllm提供的Converter实用工具来进行转换：\ncd tools/convertor pip install -r ./requirements.txt # for one file pytorch model python convert.py --input_model=model.pth --output_model=model.mllm --type=torch # for multi-file pytorch model python convert.py --input_model=pytorch_model.bin.index.json --output_model=model.mllm --type=torch # for one file safetensor model python convert.py --input_model=model.bin --output_model=model.mllm --type=safetensor # for multi-file safetensor model python convert.py --input_model=model.safetensors.index.json --output_model=model.mllm --type=safetensor 5.1.2 Vocab转换 使用mllm提供的Converter实用工具来进行转换：\ncd tools/convertor python vocab.py --input_file=tokenizer.json --output_file=vocab.mllm --type=Unigram 5.1.3 量化 mllm提供了量化工具，该工具支持4 bits和6 bits整数量化，你可以使用下述指令来对模型参数进行量化\ncd bin ./quantize model.mllm model_q4_0.mllm Q4_K 5.2 Configuration 设置文件里面主要实现两个类，一个是QWenNameConfig，一个是QWenConfig，其中QWenNameConfig包含QWenConfig。在一个mllm模型参数文件中，模型参数是以key-value对的形式统一起来的。QWenNameConfig的目的就是给出每个参数的名称，以便于mllm框架索引到正确的模型参数。\nclass QWenNameConfig : public TransformerNameConfig { public: /** * @brief QWen2 following the hugging face naming method * * @param type RoPEType */ void init(RoPEType type = RoPEType::HFHUBROPE) { switch (type) { case RoPEType::HFHUBROPE: { blk_name = \"model.layers.\"; _attn_base_name = \"self_attn.\"; _ffn_base_name = \"mlp.\"; _q_proj_name = \"q_proj\"; _k_proj_name = \"k_proj\"; _v_proj_name = \"v_proj\"; _o_proj_name = \"o_proj\"; _gate_proj_name = \"gate_proj\"; _up_proj_name = \"up_proj\"; _down_proj_name = \"down_proj\"; _attn_norm_name = \"input_layernorm\"; _ffn_norm_name = \"post_attention_layernorm\"; token_embd_name = \"model.embed_tokens\"; post_norm_name = \"model.norm\"; lm_head_name = \"lm_head\"; break; } ... } } std::string blk_name; std::string token_embd_name; std::string post_norm_name; std::string lm_head_name; std::string _gate_proj_name; }; 在QWenConfig中则主要定义各层的超参数，如rope的theta值、中间层维度大小等，如下面的代码所示：\nstruct QWenConfig { explicit QWenConfig(int token_limit, string billions = \"0.5B\", RoPEType type = RoPEType::HFHUBROPE) : cache_limit(token_limit) { ... }; float attention_dropout = 0.0; int bos_token_id = 151643; int eos_token_id = 151643; std::string hidden_act = \"silu\"; int hidden_size = 1024; float initializer_range = 0.02; int intermediate_size = 2816; int max_position_embeddings = 32768; int max_window_layers = 21; std::string model_type = \"qwen2\"; int num_attention_heads = 16; int num_hidden_layers = 24; int num_key_value_heads = 16; double rms_norm_eps = 1e-6; float rope_theta = 1000000.0; int sliding_window = 32768; int vocab_size = 151936; bool tie_embedding_words = false; int cache_limit; RoPEType RoPE_type = RoPEType::HFHUBROPE; QWenNameConfig names_config; }; 5.3 Tokenization Tokenization是一个非常客制化的步骤，每个LLM的Tokenization方法都不尽相同。以QWen为例子，QWen使用了BBPE方法，那么读者在支持QWen模型的时候，就要给出实现了BBPE的Tokenizer。mllm内部已经实现一个BPE算法，读者可以复用该实现来实现自己的Tokenizer。\n5.4 Modeling 使用mllm框架提供的算子来实现模型是非常简单和便利的，熟悉Pytorch的读者可以快速的上手mllm。本文在这里默认读者对llama/qwen/mistral等常见LLM的模型有着基本的了解。在下文中，本文以Attention模块为例来演示如何使用mllm来搭建模型。 首先，所有的class需要继承Module父类。Module父类提供了Forward函数，读者需要重载该函数来实现相应的计算流程。\nclass QWenAttention final : public Module ... 5.4.1 创建该Module需要使用的Layers class QWenAttention final : public Module { public： QWenAttention() = default; QWenAttention(const QWenConfig \u0026config, const QWenNameConfig \u0026names, const string \u0026base_name) { hidden_size = config.hidden_size; num_heads = config.num_attention_heads; head_dim = config.hidden_size / num_heads; num_key_value_heads = config.num_key_value_heads; num_key_value_groups = num_heads / num_key_value_heads; // init layers q_proj = Linear(hidden_size, num_heads * head_dim, true, base_name + names._q_proj_name); k_proj = Linear(hidden_size, num_key_value_heads * head_dim, true, base_name + names._k_proj_name); v_proj = Linear(hidden_size, num_key_value_heads * head_dim, true, base_name + names._v_proj_name); o_proj = Linear(num_heads * head_dim, hidden_size, false, base_name + names._o_proj_name); q_rope = RoPE(config.RoPE_type, config.rope_theta, config.max_position_embeddings, base_name + \"q_rope\"); k_rope = RoPE(config.RoPE_type, config.rope_theta, config.max_position_embeddings, base_name + \"k_rope\"); k_cache = KVCache(num_key_value_groups, config.cache_limit, base_name + \"k_cache\"); v_cache = KVCache(num_key_value_groups, config.cache_limit, base_name + \"v_cache\"); mask = Causalmask(base_name + \"mask\"); softmax = Softmax(DIMENSION, base_name + \"softmax\"); } private: int hidden_size; int num_heads; int head_dim; int num_key_value_heads; int num_key_value_groups; Layer q_proj; Layer k_proj; Layer v_proj; Layer o_proj; Layer q_rope; Layer k_rope; Layer k_cache; Layer v_cache; Layer mask; Layer softmax; } 细心的读者可能已经发现了，在QWenAttention的构造函数中，创建每个Layer的时候都在最后一个参数上传递了Layer名称（std::string type），这是因为mllm依赖于Layer的名称来寻找该Layer所需要的参数。\n5.4.2 重载Forward前向推理函数 创建完了所有我们需要的Layers以后，就可以编写Forward函数来定义Attention模块的计算流程，Forward函数接收一个Tensor Array和一个std::any Array，返回Tensor Array：\nstd::vector\u003cTensor\u003e Forward(std::vector\u003cTensor\u003e inputs, std::vector\u003cstd::any\u003e args) override { auto query_states = q_proj(inputs[0]); auto key_states = k_proj(inputs[1]); auto value_states = v_proj(inputs[2]); // [batch, heads, sequence, dims] query_states = query_states.view(-1, num_heads, -1, head_dim); key_states = key_states.view(-1, num_key_value_heads, -1, head_dim); value_states = value_states.view(-1, num_key_value_heads, -1, head_dim); // embedding query_states = q_rope(query_states); key_states = k_rope(key_states); // kv cache key_states = k_cache(key_states); value_states = v_cache(value_states); // attention weight auto atten_weight = Tensor::mm(query_states, key_states.transpose(Chl::SEQUENCE, Chl::DIMENSION)) / std::sqrt(head_dim); atten_weight = mask(atten_weight); atten_weight = softmax(atten_weight); // attention output auto atten_output = Tensor::mm(atten_weight, value_states); atten_output = atten_output.view(-1, 1, -1, head_dim * num_heads); atten_output = o_proj(atten_output); return {atten_output}; } 5.5 运行 完整的Qwen模型定义代码可以在附录1中找到。读者可以像Torch一样调用定义好的模型：首先，创建模型：\nQWenConfig config(tokens_limit, \"0.5B\", RoPEType::HFHUBROPE); auto model = QWenForCausalLM(config); model.load(model_path); moduleclass重载了()operator，读者可以使用model({input_tensor})来进行推理。\n6. mllm框架的不足 这里写的有点mean，本人专业知识浅薄，在学术上是依托答辩，对mllm的理解更是不到位，大家轻喷。\n6.1 Benchmark 缺少算子的Benchmark 本文认为，mllm在实现的时候极力的避免使用第三方的库，因为mllm需要迁移到移动设备上，一些三方库可能不能正常工作。但是手工实现的Kernel还是需要一个Benchmark来和目标平台上提供的算子库来进行性能比较的。就mllm目前提供的MatMul Kernel来看，似乎缺少Pack优化和/micro Kernel的优化？\n缺少prefill/decode的Benchmark mllm的issues中也有人提到过这个问题。作为具有LLM推理能力的引擎，应当测一下这两个基本能力。\n6.2 对于移动端LLM推理的特定优化 KV Cache量化 IIRC，在OPPO的Transformer-Lite[2]中，用到了KV Cache量化的小技巧。这对移动设备有限的内存来说可能会更加友好，当然还需要考量量化带来的CPU负载问题。\n动态形状推理/内存复用/KV Cache搬移优化 目前mllm是没有做内存复用的，可以考虑使用符号推理方法来做动态形状的支持进而便于求解下一轮的内存使用情况。或许可以考虑一下PageAttention[3]的Tensor管理方法或者[2]中的KV Cache规划方法来进一步减少内存的搬移。\n异构算力 可以考虑把形状推理（CPU）和计算（GPU/NPU）并行执行起来。或者是6.2.4中提到的内容与计算并行起来。\n对模型参数的Lazy Fetch和Pre Fetch 目前，mllm会把参数一次性的读入内存？考虑到移动设备的内存有限，可以在合适的时机提前从外存上预取而不是全数载入。\n6.3 易用性 模型结构需要手动编写且无法保存 目前，mllm的模型结构还是需要在C++文件中进行显示的手动定义。或许可以考虑创建自己的计算图和算子描述方式，使用flatbuffers来存储计算图。\n如果要很好的使用所有的算力，可能还是需要完善的计算图机制，这样便于优化分析。 尝试引入三方易用的库如icu等来弥补C++ utf-8处理能力的不足。 Ref：\n[1] mllm, https://github.com/UbiquitousLearning/mllm\n[2] transformer-lite, https://arxiv.org/abs/2403.20041\n[3] PageAttention, https://arxiv.org/abs/2309.06180\nA1. Qwen模型定义 #ifndef MODELING_QWEN_HPP #define MODELING_QWEN_HPP #include \"Backend.hpp\" #include \"Layer.hpp\" #include \"Module.hpp\" #include \"Tensor.hpp\" #include \"configuration_qwen.hpp\" #include using namespace mllm; // Copied from GemmaMLP with Gemma-\u003eQwen and using silu class QWenMLP final : public Module { public: QWenMLP() = default; QWenMLP(int hidden_size, int intermediate_size, const QWenNameConfig \u0026names, const std::string \u0026base_name) { gate_proj = Linear(hidden_size, intermediate_size, false, base_name + names._gate_proj_name); silu = SiLU(base_name + \"act\"); up_proj = Linear(hidden_size, intermediate_size, false, base_name + names._up_proj_name); down_proj = Linear(intermediate_size, hidden_size, false, base_name + names._down_proj_name); } std::vector\u003cTensor\u003e Forward(std::vector\u003cTensor\u003e inputs, std::vector\u003cstd::any\u003e args) override { auto x = gate_proj(inputs[0]); x = silu(x); auto y = up_proj(inputs[0]); x = x * y; x = down_proj(x); return {x}; } private: Layer gate_proj; Layer up_proj; Layer down_proj; Layer silu; }; // Copied from GemmaAttention with Gemma-\u003eQwen and using SWA class QWenAttention final : public Module { public: QWenAttention() = default; QWenAttention(const QWenConfig \u0026config, const QWenNameConfig \u0026names, const string \u0026base_name) { hidden_size = config.hidden_size; num_heads = config.num_attention_heads; head_dim = config.hidden_size / num_heads; num_key_value_heads = config.num_key_value_heads; num_key_value_groups = num_heads / num_key_value_heads; // init layers q_proj = Linear(hidden_size, num_heads * head_dim, true, base_name + names._q_proj_name); k_proj = Linear(hidden_size, num_key_value_heads * head_dim, true, base_name + names._k_proj_name); v_proj = Linear(hidden_size, num_key_value_heads * head_dim, true, base_name + names._v_proj_name); o_proj = Linear(num_heads * head_dim, hidden_size, false, base_name + names._o_proj_name); q_rope = RoPE(config.RoPE_type, config.rope_theta, config.max_position_embeddings, base_name + \"q_rope\"); k_rope = RoPE(config.RoPE_type, config.rope_theta, config.max_position_embeddings, base_name + \"k_rope\"); k_cache = KVCache(num_key_value_groups, config.cache_limit, base_name + \"k_cache\"); v_cache = KVCache(num_key_value_groups, config.cache_limit, base_name + \"v_cache\"); // mask = SlidingWindowMask(config.sliding_window, base_name + \"mask\"); mask = Causalmask(base_name + \"mask\"); softmax = Softmax(DIMENSION, base_name + \"softmax\"); } std::vector\u003cTensor\u003e Forward(std::vector\u003cTensor\u003e inputs, std::vector\u003cstd::any\u003e args) override { auto query_states = q_proj(inputs[0]); auto key_states = k_proj(inputs[1]); auto value_states = v_proj(inputs[2]); // [batch, heads, sequence, dims] query_states = query_states.view(-1, num_heads, -1, head_dim); key_states = key_states.view(-1, num_key_value_heads, -1, head_dim); value_states = value_states.view(-1, num_key_value_heads, -1, head_dim); // embedding query_states = q_rope(query_states); key_states = k_rope(key_states); // kv cache key_states = k_cache(key_states); value_states = v_cache(value_states); // attention weight auto atten_weight = Tensor::mm(query_states, key_states.transpose(Chl::SEQUENCE, Chl::DIMENSION)) / std::sqrt(head_dim); atten_weight = mask(atten_weight); atten_weight = softmax(atten_weight); // attention output auto atten_output = Tensor::mm(atten_weight, value_states); atten_output = atten_output.view(-1, 1, -1, head_dim * num_heads); atten_output = o_proj(atten_output); return {atten_output}; } private: int hidden_size; int num_heads; int head_dim; int num_key_value_heads; int num_key_value_groups; Layer q_proj; Layer k_proj; Layer v_proj; Layer o_proj; Layer q_rope; Layer k_rope; Layer k_cache; Layer v_cache; Layer mask; Layer softmax; }; // Copied from GemmaDecoder with Gemma-\u003eQwen and set RmsNorm(without add_unit_offset) class QWenDecoder final : public Module { public: QWenDecoder() = default; QWenDecoder(const QWenConfig \u0026config, const QWenNameConfig \u0026names, const string \u0026base_name) { self_atten = QWenAttention(config, names, base_name + names._attn_base_name); mlp = QWenMLP(config.hidden_size, config.intermediate_size, names, base_name + names._ffn_base_name); input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps, base_name + names._attn_norm_name); post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps, base_name + names._ffn_norm_name); } std::vector\u003cTensor\u003e Forward(std::vector\u003cTensor\u003e inputs, std::vector\u003cstd::any\u003e args) override { auto x = input_layernorm(inputs[0]); x = self_atten({x, x, x})[0]; auto tmp = x + inputs[0]; x = post_attention_layernorm(tmp); x = mlp({x})[0]; x = x + tmp; return {x}; } private: QWenAttention self_atten; QWenMLP mlp; Layer input_layernorm; Layer post_attention_layernorm; }; // Copied from GemmaModel with Gemma-\u003eQwen and set RmsNorm(without add_unit_offset) class QWenModel final : public Module { public: QWenModel() = default; QWenModel(const QWenConfig \u0026config, const QWenNameConfig \u0026names, const string \u0026base_name) { blocks = List\u003cQWenDecoder\u003e(config.num_hidden_layers, config, names, base_name); norm = RMSNorm(config.hidden_size, config.rms_norm_eps, names.post_norm_name); } std::vector\u003cTensor\u003e Forward(std::vector\u003cTensor\u003e inputs, std::vector\u003cstd::any\u003e args) override { auto x = inputs[0]; for (auto \u0026block : blocks) { x = block({x})[0]; } x = norm(x); return {x}; } private: std::vector\u003cQWenDecoder\u003e blocks; Layer norm; }; class QWenForCausalLM final : public Module { public: QWenForCausalLM(QWenConfig \u0026config) { auto names = config.names_config; hidden_size = config.hidden_size; tie_embedding_words = config.tie_embedding_words; embedding = Embedding(config.vocab_size, config.hidden_size, names.token_embd_name); model = QWenModel(config, names, names.blk_name); // FIXME Qwen-0.5 use tied embedding // Others use nn.Linear() if (tie_embedding_words) { lm_head = Parameter(1, config.vocab_size, 1, config.hidden_size, names.token_embd_name + \".weight\"); } } std::vector\u003cTensor\u003e Forward(std::vector\u003cTensor\u003e inputs, std::vector\u003cstd::any\u003e args) override { auto x = embedding(inputs[0]); // go through model auto outputs = model({x})[0]; if (tie_embedding_words) { outputs = Tensor::mm(outputs, lm_head().transpose(Chl::SEQUENCE, Chl::DIMENSION)); } return {outputs}; } private: int hidden_size; bool tie_embedding_words; Layer embedding; Parameter lm_head; QWenModel model; }; #endif //! MODELING_QWEN_HPP ",
  "wordCount" : "1623",
  "inLanguage": "en",
  "datePublished": "2024-06-28T00:00:00Z",
  "dateModified": "2024-06-28T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "chenghua.Wang"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://chenghuawang.github.io/keep-moving-forward/tech/mllm-qwen/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Ubios Home",
    "logo": {
      "@type": "ImageObject",
      "url": "https://chenghuawang.github.io/keep-moving-forward/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://chenghuawang.github.io/keep-moving-forward/" accesskey="h" title="Ubios Home (Alt + H)">Ubios Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://chenghuawang.github.io/keep-moving-forward/about/about/" title="关于我">
                    <span>关于我</span>
                </a>
            </li>
            <li>
                <a href="https://chenghuawang.github.io/keep-moving-forward/about/tech_posts/" title="技术相关">
                    <span>技术相关</span>
                </a>
            </li>
            <li>
                <a href="https://chenghuawang.github.io/keep-moving-forward/about/paper_posts/" title="论文解析">
                    <span>论文解析</span>
                </a>
            </li>
            <li>
                <a href="https://chenghuawang.github.io/keep-moving-forward/about/news/" title="🎉News🎉">
                    <span>🎉News🎉</span>
                </a>
            </li>
            <li>
                <a href="https://chenghuawang.github.io/keep-moving-forward/about/thingking/" title="思考">
                    <span>思考</span>
                </a>
            </li>
            <li>
                <a href="https://chenghuawang.github.io/keep-moving-forward/about/hpc_ai/" title="AI&amp;Sys 入门">
                    <span>AI&amp;Sys 入门</span>
                </a>
            </li>
            <li>
                <a href="https://chenghuawang.github.io/keep-moving-forward/tags/" title="标签">
                    <span>标签</span>
                </a>
            </li>
            <li>
                <a href="https://chenghuawang.github.io/keep-moving-forward/series" title="系列文章">
                    <span>系列文章</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://chenghuawang.github.io/keep-moving-forward/">Home</a>&nbsp;»&nbsp;<a href="https://chenghuawang.github.io/keep-moving-forward/tech/">Technique</a></div>
    <h1 class="post-title entry-hint-parent">
      mllm框架浅析(一)-以QWen0.5B为例
    </h1>
    <div class="post-description">
      以Qwen0.5B为例解析mllm的基本实现，CPU Backend
    </div>
    <div class="post-meta"><span title='2024-06-28 00:00:00 +0000 UTC'>June 28, 2024</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;chenghua.Wang

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#1-%e7%ae%80%e4%bb%8b" aria-label="1. 简介">1. 简介</a></li>
                <li>
                    <a href="#2-%e6%a1%86%e6%9e%b6%e6%89%a7%e8%a1%8c%e6%b5%81%e7%a8%8b" aria-label="2. 框架执行流程">2. 框架执行流程</a><ul>
                        
                <li>
                    <a href="#21-%e4%bb%a5%e4%b8%a4%e5%b1%82linear%e5%b1%82%e8%bf%90%e8%a1%8c%e4%b8%ba%e4%be%8b" aria-label="2.1 以两层Linear层运行为例">2.1 以两层Linear层运行为例</a><ul>
                        
                <li>
                    <a href="#211-%e5%8a%a0%e8%bd%bd%e5%8f%82%e6%95%b0" aria-label="2.1.1 加载参数">2.1.1 加载参数</a></li>
                <li>
                    <a href="#212module%e7%9a%84operator%e6%98%af%e5%a6%82%e4%bd%95%e8%b0%83%e7%94%a8forward%e5%87%bd%e6%95%b0%e7%9a%84" aria-label="2.1.2Module的Operator()是如何调用Forward函数的？">2.1.2Module的Operator()是如何调用Forward函数的？</a></li>
                <li>
                    <a href="#213-linear%e5%b1%82%e7%9a%84%e6%89%a7%e8%a1%8c" aria-label="2.1.3 Linear层的执行">2.1.3 Linear层的执行</a></li></ul>
                </li>
                <li>
                    <a href="#22-%e6%80%bb%e7%bb%93" aria-label="2.2 总结">2.2 总结</a></li></ul>
                </li>
                <li>
                    <a href="#3-%e5%a6%82%e4%bd%95%e7%bc%96%e5%86%99op%e4%b8%8e%e8%87%aa%e5%ae%9a%e4%b9%89layer" aria-label="3. 如何编写Op与自定义Layer">3. 如何编写Op与自定义Layer</a><ul>
                        
                <li>
                    <a href="#31-%e6%96%b0%e5%a2%9e%e5%af%b9%e5%ba%94backend%e7%9a%84op%e6%96%87%e4%bb%b6" aria-label="3.1 新增对应Backend的Op文件">3.1 新增对应Backend的Op文件</a></li>
                <li>
                    <a href="#32-op%e5%8f%82%e6%95%b0%e8%87%aa%e5%ae%9a%e4%b9%89" aria-label="3.2 Op参数自定义">3.2 Op参数自定义</a></li>
                <li>
                    <a href="#33-%e9%87%8d%e8%bd%bd%e5%87%bd%e6%95%b0" aria-label="3.3 重载函数">3.3 重载函数</a></li>
                <li>
                    <a href="#34-op%e6%98%af%e5%a6%82%e4%bd%95%e8%a2%ab%e6%b3%a8%e5%86%8c%e5%92%8c%e5%88%9b%e5%bb%ba%e7%9a%84" aria-label="3.4 Op是如何被注册和创建的？">3.4 Op是如何被注册和创建的？</a><ul>
                        
                <li>
                    <a href="#341-%e5%9c%a8backend%e4%b8%ad%e6%b3%a8%e5%86%8cop" aria-label="3.4.1 在Backend中注册Op">3.4.1 在Backend中注册Op</a></li>
                <li>
                    <a href="#342-%e5%9c%a8layerhpp%e4%b8%ad%e5%8a%a0%e5%85%a5%e5%af%b9%e5%ba%94%e7%9a%84op-layer" aria-label="3.4.2 在Layer.hpp中加入对应的Op Layer">3.4.2 在Layer.hpp中加入对应的Op Layer</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#4-tokenizer" aria-label="4. Tokenizer">4. Tokenizer</a></li>
                <li>
                    <a href="#5-%e5%a6%82%e4%bd%95%e5%af%b9%e6%96%b0%e6%a8%a1%e5%9e%8b%e8%bf%9b%e8%a1%8c%e6%94%af%e6%8c%81" aria-label="5. 如何对新模型进行支持">5. 如何对新模型进行支持</a><ul>
                        
                <li>
                    <a href="#51-%e7%94%9f%e6%88%90mllm%e6%94%af%e6%8c%81%e7%9a%84vocab%e5%92%8c%e6%a8%a1%e5%9e%8b%e5%8f%82%e6%95%b0" aria-label="5.1 生成mllm支持的vocab和模型参数">5.1 生成mllm支持的vocab和模型参数</a><ul>
                        
                <li>
                    <a href="#511-%e6%a8%a1%e5%9e%8b%e8%bd%ac%e6%8d%a2" aria-label="5.1.1 模型转换">5.1.1 模型转换</a></li>
                <li>
                    <a href="#512-vocab%e8%bd%ac%e6%8d%a2" aria-label="5.1.2 Vocab转换">5.1.2 Vocab转换</a></li>
                <li>
                    <a href="#513-%e9%87%8f%e5%8c%96" aria-label="5.1.3 量化">5.1.3 量化</a></li></ul>
                </li>
                <li>
                    <a href="#52-configuration" aria-label="5.2 Configuration">5.2 Configuration</a></li>
                <li>
                    <a href="#53-tokenization" aria-label="5.3 Tokenization">5.3 Tokenization</a></li>
                <li>
                    <a href="#54-modeling" aria-label="5.4 Modeling">5.4 Modeling</a><ul>
                        
                <li>
                    <a href="#541-%e5%88%9b%e5%bb%ba%e8%af%a5module%e9%9c%80%e8%a6%81%e4%bd%bf%e7%94%a8%e7%9a%84layers" aria-label="5.4.1 创建该Module需要使用的Layers">5.4.1 创建该Module需要使用的Layers</a></li>
                <li>
                    <a href="#542-%e9%87%8d%e8%bd%bdforward%e5%89%8d%e5%90%91%e6%8e%a8%e7%90%86%e5%87%bd%e6%95%b0" aria-label="5.4.2 重载Forward前向推理函数">5.4.2 重载Forward前向推理函数</a></li></ul>
                </li>
                <li>
                    <a href="#55-%e8%bf%90%e8%a1%8c" aria-label="5.5 运行">5.5 运行</a></li></ul>
                </li>
                <li>
                    <a href="#6-mllm%e6%a1%86%e6%9e%b6%e7%9a%84%e4%b8%8d%e8%b6%b3" aria-label="6. mllm框架的不足">6. mllm框架的不足</a><ul>
                        
                <li>
                    <a href="#61-benchmark" aria-label="6.1 Benchmark">6.1 Benchmark</a></li>
                <li>
                    <a href="#62-%e5%af%b9%e4%ba%8e%e7%a7%bb%e5%8a%a8%e7%ab%afllm%e6%8e%a8%e7%90%86%e7%9a%84%e7%89%b9%e5%ae%9a%e4%bc%98%e5%8c%96" aria-label="6.2 对于移动端LLM推理的特定优化">6.2 对于移动端LLM推理的特定优化</a></li>
                <li>
                    <a href="#63-%e6%98%93%e7%94%a8%e6%80%a7" aria-label="6.3 易用性">6.3 易用性</a></li></ul>
                </li>
                <li>
                    <a href="#a1-qwen%e6%a8%a1%e5%9e%8b%e5%ae%9a%e4%b9%89" aria-label="A1. Qwen模型定义">A1. Qwen模型定义</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><hr>
<p>笔者最近在做一些mllm相关的工作，书写此文对mllm框架进行梳理总结，定有不少纰漏，请读者立即指出，谢谢。mllm目前在做一些其他工作，这篇文章的书写时间为发布时间。在mllm的其他工作合并进主仓库后，本文还会进一步的跟进。读者请注意本文的时效性。</p>
<hr>
<h1 id="1-简介">1. 简介<a hidden class="anchor" aria-hidden="true" href="#1-简介">#</a></h1>
<p><strong>mllm</strong>是一款适用于<strong>移动设备和边缘设备</strong>的快速、轻量的多模态LLM推理引擎。</p>
<ul>
<li>完全的C/C++实现，无第三方依赖</li>
<li>针对fuyu-8B等多模态LLM进行了优化</li>
<li>支持ARM NEON和X86 AVX2向量指令</li>
<li>支持4 bits和6 bits整数量化</li>
</ul>
<p>本文将更多的以工程的视角来解析mllm框架，在行文过程中，本文会将mllm与其他框架的设计方法做对比。接下来，本文将会用<strong>项目组织结构、框架执行流程、自定义Op/Layer、Tokenizer和如何支持新模型</strong>五个章节来详细描述mllm框架的各项特性和总体结构。读者可以把该文章做mllm的使用文档。<strong>在最后，本文将会指出mllm的不足之处和可以尝试跟进的工作</strong>。</p>
<hr>
<p>在开始正式解析mllm之前，读者可以先clone下mllm的代码库，以便于跟进分析流程。mllm不依赖于git submodule，项目配置起来很方便，目前mllm可以在linux上使用Clang/GCC编译器进行编译。目前mllm支持的目标设备体系结构是X86和Arm。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">git clone https://github.com/UbiquitousLearning/mllm
</span></span></code></pre></div><p>mllm团队将所有LLM相关的vocab文件都放在了git仓库中（这个其实可以移动到HuggingFace的仓库上），LLM量化后的模型文件都存储在HuggingFace上，读者可以在<a href="https://huggingface.co/mllmTeam">https://huggingface.co/mllmTeam</a>上找到mllm提供的模型文件。</p>
<h1 id="2-框架执行流程">2. 框架执行流程<a hidden class="anchor" aria-hidden="true" href="#2-框架执行流程">#</a></h1>
<h2 id="21-以两层linear层运行为例">2.1 以两层Linear层运行为例<a hidden class="anchor" aria-hidden="true" href="#21-以两层linear层运行为例">#</a></h2>
<p>首先，考虑下面的代码，定义了两个Linear Layers，并且输入$X$通过两个Linear Layers来得到输出：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">TwoLinear</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Module</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">TwoLinear</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">TwoLinear</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">linear1</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_f</span><span class="p">,</span> <span class="n">out_f</span><span class="p">,</span> <span class="cm">/*bias*/</span><span class="nb">true</span><span class="p">,</span> <span class="s">&#34;linear1&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">linear2</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">out_f</span><span class="p">,</span> <span class="n">out_f</span><span class="p">,</span> <span class="cm">/*bias*/</span><span class="nb">true</span><span class="p">,</span> <span class="s">&#34;linear2&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">Forward</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">any</span><span class="o">&gt;</span> <span class="n">args</span><span class="p">)</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">linear1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">linear2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">TwoLinear</span> <span class="n">tl</span><span class="p">;</span>
</span></span></code></pre></div><h3 id="211-加载参数">2.1.1 加载参数<a hidden class="anchor" aria-hidden="true" href="#211-加载参数">#</a></h3>
<p>读者可以使用 <code>tl.load(path)</code>来加载参数。那么mllm是如何实现参数加载的呢？在load函数中，mllm会创建一个ParamLoader，这个ParamLoader是Static的，在全局可以访问。然后mllm会设置另一个全局参数doLoad为True，进而进入推理流程<code>operator()(tmps, tmpt);</code>。在推理流程中，要是执行层发现doLoad为True，那么就执行每个算子内定义好的load指令，而不是执行每个算子的原本逻辑。
load的执行在<code>Layer.hpp</code>文件的<code>INIT_OP()</code>中。</p>
<h3 id="212module的operator是如何调用forward函数的">2.1.2Module的Operator()是如何调用Forward函数的？<a hidden class="anchor" aria-hidden="true" href="#212module的operator是如何调用forward函数的">#</a></h3>
<p>对于常见的<code>INPUT_TENSOR</code>类型的Tensor，mllm首先会设置这个Tensor的类型为<code>TENSOR_STATIC_INIT</code>，进行一遍Forward推理；第一遍Forward推理完毕以后再把Tensor的类型设为<code>TENSOR_STATIC_READY</code>，然后进行第二遍Forward推理。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">ttype</span><span class="p">()</span> <span class="o">==</span> <span class="n">TensorType</span><span class="o">::</span><span class="n">INPUT_TENSOR</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="o">&amp;</span><span class="nl">input</span> <span class="p">:</span> <span class="n">inputs</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">input</span><span class="p">.</span><span class="n">setTtype</span><span class="p">(</span><span class="n">TensorType</span><span class="o">::</span><span class="n">NORMAL_TENSOR</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">input</span><span class="p">.</span><span class="n">status</span><span class="p">()</span> <span class="o">=</span> <span class="n">TENSOR_STATIC_INIT</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span><span class="p">(</span><span class="n">input</span><span class="p">.</span><span class="n">batch</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">            <span class="n">Tensor</span><span class="o">::</span><span class="n">gph_</span><span class="p">[</span><span class="n">input</span><span class="p">.</span><span class="n">name</span><span class="p">()]</span> <span class="o">=</span> <span class="n">input</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor_status</span> <span class="o">=</span> <span class="n">TENSOR_STATIC_INIT</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">Forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">anyArgs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="o">&amp;</span><span class="nl">input</span> <span class="p">:</span> <span class="n">inputs</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">input</span><span class="p">.</span><span class="n">status</span><span class="p">()</span> <span class="o">=</span> <span class="n">TENSOR_STATIC_READY</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor_status</span> <span class="o">=</span> <span class="n">TENSOR_STATIC_READY</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nf">Forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">anyArgs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>第一次Forward推理的目的是调用Op定义的Reshape和SetUp函数，Reshape函数会推理出这一次模型推理的过程中每个Tensor的形状大小。SetUp函数会对Op需要输出的Tensor做内存的申请。
第二次Forward推理才是真正的计算。</p>
<h3 id="213-linear层的执行">2.1.3 Linear层的执行<a hidden class="anchor" aria-hidden="true" href="#213-linear层的执行">#</a></h3>
<p>每个Layer在实现的时候都会重载<code>operator()</code>，比如linear layer的<code>operator()</code>函数如下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">Tensor</span> <span class="o">&amp;</span><span class="k">operator</span><span class="p">()(</span><span class="n">Tensor</span> <span class="o">&amp;</span><span class="n">input</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nf">_1I1O_OP</span><span class="p">(</span><span class="n">input</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>其中，<code>_1I1O_OP</code>表示的意思是，这是需要使用1个输入和1个输出的函数来处理这个算子。mllm还提供了许多类似于<code>_1I1O_OP</code>的函数来处理不同的算子。</p>
<h2 id="22-总结">2.2 总结<a hidden class="anchor" aria-hidden="true" href="#22-总结">#</a></h2>
<p>大体来说，mllm使用了类似于状态机的参数来设置了当前推理过程的运行状态。每一次都是通过Forward函数来进行全模型的遍历，在Op的执行过程中，用这些设定的参数来区分每次Op需要表现的行为。</p>
<h1 id="3-如何编写op与自定义layer">3. 如何编写Op与自定义Layer<a hidden class="anchor" aria-hidden="true" href="#3-如何编写op与自定义layer">#</a></h1>
<h2 id="31-新增对应backend的op文件">3.1 新增对应Backend的Op文件<a hidden class="anchor" aria-hidden="true" href="#31-新增对应backend的op文件">#</a></h2>
<p>mllm提供了<code>src/backends/new_op.py</code>实用工具来帮助创建Op Class。该文件会帮助读者创建下述基本函数：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">ErrorCode</span> <span class="nf">reshape</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span> <span class="n">outputs</span><span class="p">)</span> <span class="k">override</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">ErrorCode</span> <span class="nf">execute</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span> <span class="n">outputs</span><span class="p">)</span> <span class="k">override</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">ErrorCode</span> <span class="nf">load</span><span class="p">(</span><span class="n">AbstructLoader</span> <span class="o">&amp;</span><span class="n">loader</span><span class="p">)</span> <span class="k">override</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">ErrorCode</span> <span class="nf">free</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span> <span class="n">outputs</span><span class="p">)</span> <span class="k">override</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">ErrorCode</span> <span class="nf">setUp</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span> <span class="n">outputs</span><span class="p">)</span> <span class="k">override</span><span class="p">;</span>
</span></span></code></pre></div><h2 id="32-op参数自定义">3.2 Op参数自定义<a hidden class="anchor" aria-hidden="true" href="#32-op参数自定义">#</a></h2>
<p>比如对于CPU上的LinearOp，需要<code>in_features</code>、<code>out_features</code>和<code>has_bias</code>三个参数。那么可以在3.1自动生成的class中加入：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">CPULinear</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Op</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="p">...</span>
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">in_features_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">out_features_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">bool</span> <span class="n">support_bias_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">thread_count</span> <span class="o">=</span> <span class="mi">4</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Tensor</span> <span class="n">weight_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Tensor</span> <span class="n">bias_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div><p>在CPULinearCreator中加入：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">CPULinearCreator</span> <span class="o">:</span> <span class="k">public</span> <span class="n">CPUBackend</span><span class="o">::</span><span class="n">Creator</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">virtual</span> <span class="n">Op</span> <span class="o">*</span><span class="n">create</span><span class="p">(</span><span class="n">OpParam</span> <span class="n">op_param</span><span class="p">,</span> <span class="n">Backend</span> <span class="o">*</span><span class="n">bn</span><span class="p">,</span> <span class="n">string</span> <span class="n">name</span><span class="p">,</span> <span class="kt">int</span> <span class="n">threadCount</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="kt">int</span> <span class="n">in_features</span> <span class="o">=</span> <span class="n">op_param</span><span class="p">[</span><span class="s">&#34;in_features&#34;</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="kt">int</span> <span class="n">out_features</span> <span class="o">=</span> <span class="n">op_param</span><span class="p">[</span><span class="s">&#34;out_features&#34;</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="kt">int</span> <span class="n">bias</span> <span class="o">=</span> <span class="n">op_param</span><span class="p">[</span><span class="s">&#34;bias&#34;</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="k">new</span> <span class="nf">CPULinear</span><span class="p">(</span><span class="n">bn</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="p">(</span><span class="kt">bool</span><span class="p">)</span><span class="n">bias</span><span class="p">,</span> <span class="n">threadCount</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div><p>请注意，OpParam是一个string-float map。</p>
<h2 id="33-重载函数">3.3 重载函数<a hidden class="anchor" aria-hidden="true" href="#33-重载函数">#</a></h2>
<p>读者需要自行实现reshape，execute，load，free函数，视情况重载setUp函数。
以Linear Op为例，reshape函数就会通过<code>in_features_</code>变量来检查输入的Tensor的维度是否正确，然后对output Tensor做<code>outputs[0]-&gt;reshape(inputs[0]-&gt;batch(), inputs[0]-&gt;head(), inputs[0]-&gt;sequence(), out_features_)</code></p>
<p>在load函数中，实现Weight和Bias的加载。</p>
<p>在execute函数中，具体实现矩阵乘法等计算操作。</p>
<p>在free函数中释放Weight和Bias。</p>
<h2 id="34-op是如何被注册和创建的">3.4 Op是如何被注册和创建的？<a hidden class="anchor" aria-hidden="true" href="#34-op是如何被注册和创建的">#</a></h2>
<p>在定义完成Op后，读者还需要把该Op注册到相应的Backend中，以及将Op抽象成Layer。</p>
<h3 id="341-在backend中注册op">3.4.1 在Backend中注册Op<a hidden class="anchor" aria-hidden="true" href="#341-在backend中注册op">#</a></h3>
<p>以CPU Backend为例，读者需要再CPUBackend文件中加入<code>addCreator(LINEAR, (CPUBackend::Creator *)(new CPULinearCreator()));</code></p>
<p>如果这是一个新的算子，读者还需要在<code>OpDefined</code>文件中加入新Op的Enum项。</p>
<h3 id="342-在layerhpp中加入对应的op-layer">3.4.2 在Layer.hpp中加入对应的Op Layer<a hidden class="anchor" aria-hidden="true" href="#342-在layerhpp中加入对应的op-layer">#</a></h3>
<p>如Linear Layer:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Linear</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Layer</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">explicit</span> <span class="n">Linear</span><span class="p">(</span><span class="kt">int</span> <span class="n">in_features</span><span class="p">,</span> <span class="kt">int</span> <span class="n">out_features</span><span class="p">,</span> <span class="kt">bool</span> <span class="n">bias</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">name</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">param_</span><span class="p">[</span><span class="s">&#34;in_features&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">in_features</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">param_</span><span class="p">[</span><span class="s">&#34;out_features&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">out_features</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">param_</span><span class="p">[</span><span class="s">&#34;bias&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="n">bias</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">init</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">OpType</span><span class="o">::</span><span class="n">LINEAR</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">Tensor</span> <span class="o">&amp;</span><span class="k">operator</span><span class="p">()(</span><span class="n">Tensor</span> <span class="o">&amp;</span><span class="n">input</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="nf">_1I1O_OP</span><span class="p">(</span><span class="n">input</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div><p><strong>其中，在构造函数中的</strong><code>**init()**</code><strong>函数并没有创建这个Linear算子。它只是负责给这个Linear指派了Backend。</strong>
真正的算子创建还是在<code>INIT_OP()</code>函数中。在这个函数中，它会通过<code>backend_-&gt;opCreate(param_, name_);</code>来创建算子。</p>
<h1 id="4-tokenizer">4. Tokenizer<a hidden class="anchor" aria-hidden="true" href="#4-tokenizer">#</a></h1>
<p>mllm提供了基础的Tokenizer支持，目前支持BPE和Unigram两种分词算法。</p>
<h1 id="5-如何对新模型进行支持">5. 如何对新模型进行支持<a hidden class="anchor" aria-hidden="true" href="#5-如何对新模型进行支持">#</a></h1>
<p>在mllm中，对模型组件（model、Tokenizer、Configuration）的定义和HuggingFace Transformer库中的定义方法基本一致。以支持QWen0.5B模型为例，需要编写三个文件：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">configuration_qwen.cpp
</span></span><span class="line"><span class="cl">modeling_qwen.cpp
</span></span><span class="line"><span class="cl">tokenization_qwen.cpp
</span></span></code></pre></div><p>其中<code>configuration_qwen.cpp</code>定义了Qwen LLM的各类参数，如Head数量，hidden dim等。<code>modeling_qwen.cpp</code>定义了Qwen LLM网络。<code>tokenization_qwen.cpp</code>包含了将句子转化为Token的预处理行为。</p>
<h2 id="51-生成mllm支持的vocab和模型参数">5.1 生成mllm支持的vocab和模型参数<a hidden class="anchor" aria-hidden="true" href="#51-生成mllm支持的vocab和模型参数">#</a></h2>
<h3 id="511-模型转换">5.1.1 模型转换<a hidden class="anchor" aria-hidden="true" href="#511-模型转换">#</a></h3>
<p>使用mllm提供的Converter实用工具来进行转换：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">cd</span> tools/convertor
</span></span><span class="line"><span class="cl">pip install -r ./requirements.txt
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># for one file pytorch model</span>
</span></span><span class="line"><span class="cl">python convert.py --input_model<span class="o">=</span>model.pth --output_model<span class="o">=</span>model.mllm --type<span class="o">=</span>torch
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># for multi-file pytorch model</span>
</span></span><span class="line"><span class="cl">python convert.py --input_model<span class="o">=</span>pytorch_model.bin.index.json --output_model<span class="o">=</span>model.mllm --type<span class="o">=</span>torch
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># for one file safetensor model</span>
</span></span><span class="line"><span class="cl">python convert.py --input_model<span class="o">=</span>model.bin --output_model<span class="o">=</span>model.mllm --type<span class="o">=</span>safetensor
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># for multi-file safetensor model</span>
</span></span><span class="line"><span class="cl">python convert.py --input_model<span class="o">=</span>model.safetensors.index.json --output_model<span class="o">=</span>model.mllm --type<span class="o">=</span>safetensor
</span></span></code></pre></div><h3 id="512-vocab转换">5.1.2 Vocab转换<a hidden class="anchor" aria-hidden="true" href="#512-vocab转换">#</a></h3>
<p>使用mllm提供的Converter实用工具来进行转换：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">cd</span> tools/convertor
</span></span><span class="line"><span class="cl">python vocab.py --input_file<span class="o">=</span>tokenizer.json --output_file<span class="o">=</span>vocab.mllm --type<span class="o">=</span>Unigram
</span></span></code></pre></div><h3 id="513-量化">5.1.3 量化<a hidden class="anchor" aria-hidden="true" href="#513-量化">#</a></h3>
<p>mllm提供了量化工具，该工具支持4 bits和6 bits整数量化，你可以使用下述指令来对模型参数进行量化</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="nb">cd</span> bin
</span></span><span class="line"><span class="cl">./quantize model.mllm model_q4_0.mllm Q4_K
</span></span></code></pre></div><h2 id="52-configuration">5.2 Configuration<a hidden class="anchor" aria-hidden="true" href="#52-configuration">#</a></h2>
<p>设置文件里面主要实现两个类，一个是<code>QWenNameConfig</code>，一个是<code>QWenConfig</code>，其中<code>QWenNameConfig</code>包含<code>QWenConfig</code>。在一个mllm模型参数文件中，模型参数是以key-value对的形式统一起来的。<code>QWenNameConfig</code>的目的就是给出每个参数的名称，以便于mllm框架索引到正确的模型参数。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">QWenNameConfig</span> <span class="o">:</span> <span class="k">public</span> <span class="n">TransformerNameConfig</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="cm">/**
</span></span></span><span class="line"><span class="cl"><span class="cm">     * @brief QWen2 following the hugging face naming method
</span></span></span><span class="line"><span class="cl"><span class="cm">     *
</span></span></span><span class="line"><span class="cl"><span class="cm">     * @param type RoPEType
</span></span></span><span class="line"><span class="cl"><span class="cm">     */</span>
</span></span><span class="line"><span class="cl">    <span class="kt">void</span> <span class="n">init</span><span class="p">(</span><span class="n">RoPEType</span> <span class="n">type</span> <span class="o">=</span> <span class="n">RoPEType</span><span class="o">::</span><span class="n">HFHUBROPE</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">switch</span> <span class="p">(</span><span class="n">type</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">case</span> <span class="n">RoPEType</span><span class="o">::</span><span class="nl">HFHUBROPE</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">blk_name</span> <span class="o">=</span> <span class="s">&#34;model.layers.&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_attn_base_name</span> <span class="o">=</span> <span class="s">&#34;self_attn.&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_ffn_base_name</span> <span class="o">=</span> <span class="s">&#34;mlp.&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_q_proj_name</span> <span class="o">=</span> <span class="s">&#34;q_proj&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_k_proj_name</span> <span class="o">=</span> <span class="s">&#34;k_proj&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_v_proj_name</span> <span class="o">=</span> <span class="s">&#34;v_proj&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_o_proj_name</span> <span class="o">=</span> <span class="s">&#34;o_proj&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_gate_proj_name</span> <span class="o">=</span> <span class="s">&#34;gate_proj&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_up_proj_name</span> <span class="o">=</span> <span class="s">&#34;up_proj&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_down_proj_name</span> <span class="o">=</span> <span class="s">&#34;down_proj&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_attn_norm_name</span> <span class="o">=</span> <span class="s">&#34;input_layernorm&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">_ffn_norm_name</span> <span class="o">=</span> <span class="s">&#34;post_attention_layernorm&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">token_embd_name</span> <span class="o">=</span> <span class="s">&#34;model.embed_tokens&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">post_norm_name</span> <span class="o">=</span> <span class="s">&#34;model.norm&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">lm_head_name</span> <span class="o">=</span> <span class="s">&#34;lm_head&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="k">break</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">...</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">blk_name</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">token_embd_name</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">post_norm_name</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">lm_head_name</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">_gate_proj_name</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div><p>在<code>QWenConfig</code>中则主要定义各层的超参数，如rope的theta值、中间层维度大小等，如下面的代码所示：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">QWenConfig</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">explicit</span> <span class="nf">QWenConfig</span><span class="p">(</span><span class="kt">int</span> <span class="n">token_limit</span><span class="p">,</span> <span class="n">string</span> <span class="n">billions</span> <span class="o">=</span> <span class="s">&#34;0.5B&#34;</span><span class="p">,</span> <span class="n">RoPEType</span> <span class="n">type</span> <span class="o">=</span> <span class="n">RoPEType</span><span class="o">::</span><span class="n">HFHUBROPE</span><span class="p">)</span> <span class="o">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">cache_limit</span><span class="p">(</span><span class="n">token_limit</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="p">...</span>
</span></span><span class="line"><span class="cl">    <span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">attention_dropout</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">bos_token_id</span> <span class="o">=</span> <span class="mi">151643</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">eos_token_id</span> <span class="o">=</span> <span class="mi">151643</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">hidden_act</span> <span class="o">=</span> <span class="s">&#34;silu&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">initializer_range</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">intermediate_size</span> <span class="o">=</span> <span class="mi">2816</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="mi">32768</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">max_window_layers</span> <span class="o">=</span> <span class="mi">21</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">model_type</span> <span class="o">=</span> <span class="s">&#34;qwen2&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">num_attention_heads</span> <span class="o">=</span> <span class="mi">16</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="mi">24</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">num_key_value_heads</span> <span class="o">=</span> <span class="mi">16</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">double</span> <span class="n">rms_norm_eps</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">rope_theta</span> <span class="o">=</span> <span class="mf">1000000.0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">sliding_window</span> <span class="o">=</span> <span class="mi">32768</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">151936</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">bool</span> <span class="n">tie_embedding_words</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">cache_limit</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">RoPEType</span> <span class="n">RoPE_type</span> <span class="o">=</span> <span class="n">RoPEType</span><span class="o">::</span><span class="n">HFHUBROPE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenNameConfig</span> <span class="n">names_config</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div><h2 id="53-tokenization">5.3 Tokenization<a hidden class="anchor" aria-hidden="true" href="#53-tokenization">#</a></h2>
<p>Tokenization是一个非常客制化的步骤，每个LLM的Tokenization方法都不尽相同。以QWen为例子，QWen使用了BBPE方法，那么读者在支持QWen模型的时候，就要给出实现了BBPE的Tokenizer。mllm内部已经实现一个BPE算法，读者可以复用该实现来实现自己的Tokenizer。</p>
<h2 id="54-modeling">5.4 Modeling<a hidden class="anchor" aria-hidden="true" href="#54-modeling">#</a></h2>
<p>使用mllm框架提供的算子来实现模型是非常简单和便利的，熟悉Pytorch的读者可以快速的上手mllm。本文在这里默认读者对llama/qwen/mistral等常见LLM的模型有着基本的了解。在下文中，本文以Attention模块为例来演示如何使用mllm来搭建模型。
首先，所有的class需要继承<code>Module</code>父类。<code>Module</code>父类提供了<code>Forward</code>函数，读者需要重载该函数来实现相应的计算流程。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">QWenAttention</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Module</span> <span class="p">...</span>
</span></span></code></pre></div><h3 id="541-创建该module需要使用的layers">5.4.1 创建该Module需要使用的Layers<a hidden class="anchor" aria-hidden="true" href="#541-创建该module需要使用的layers">#</a></h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">QWenAttention</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Module</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="err">：</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenAttention</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenAttention</span><span class="p">(</span><span class="k">const</span> <span class="n">QWenConfig</span> <span class="o">&amp;</span><span class="n">config</span><span class="p">,</span> <span class="k">const</span> <span class="n">QWenNameConfig</span> <span class="o">&amp;</span><span class="n">names</span><span class="p">,</span> <span class="k">const</span> <span class="n">string</span> <span class="o">&amp;</span><span class="n">base_name</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_key_value_heads</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">num_key_value_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_key_value_groups</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">/</span> <span class="n">num_key_value_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// init layers
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">q_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">,</span> <span class="nb">true</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_q_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">,</span> <span class="nb">true</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_k_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">,</span> <span class="nb">true</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_v_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">o_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="nb">false</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_o_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">q_rope</span> <span class="o">=</span> <span class="n">RoPE</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">RoPE_type</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">rope_theta</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">max_position_embeddings</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;q_rope&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_rope</span> <span class="o">=</span> <span class="n">RoPE</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">RoPE_type</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">rope_theta</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">max_position_embeddings</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;k_rope&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_cache</span> <span class="o">=</span> <span class="n">KVCache</span><span class="p">(</span><span class="n">num_key_value_groups</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">cache_limit</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;k_cache&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_cache</span> <span class="o">=</span> <span class="n">KVCache</span><span class="p">(</span><span class="n">num_key_value_groups</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">cache_limit</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;v_cache&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask</span> <span class="o">=</span> <span class="n">Causalmask</span><span class="p">(</span><span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;mask&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">softmax</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">DIMENSION</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;softmax&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">hidden_size</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">num_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">head_dim</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">num_key_value_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">num_key_value_groups</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">q_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">k_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">v_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">o_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">q_rope</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">k_rope</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">k_cache</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">v_cache</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">mask</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">softmax</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>细心的读者可能已经发现了，在<code>QWenAttention</code>的构造函数中，创建每个Layer的时候都在最后一个参数上传递了Layer名称（std::string type），这是因为mllm依赖于Layer的名称来寻找该Layer所需要的参数。</p>
<h3 id="542-重载forward前向推理函数">5.4.2 重载Forward前向推理函数<a hidden class="anchor" aria-hidden="true" href="#542-重载forward前向推理函数">#</a></h3>
<p>创建完了所有我们需要的Layers以后，就可以编写Forward函数来定义Attention模块的计算流程，Forward函数接收一个Tensor Array和一个std::any Array，返回Tensor Array：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">Forward</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">any</span><span class="o">&gt;</span> <span class="n">args</span><span class="p">)</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">query_states</span> <span class="o">=</span> <span class="n">q_proj</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">key_states</span> <span class="o">=</span> <span class="n">k_proj</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">value_states</span> <span class="o">=</span> <span class="n">v_proj</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// [batch, heads, sequence, dims]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">query_states</span> <span class="o">=</span> <span class="n">query_states</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// embedding
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">query_states</span> <span class="o">=</span> <span class="n">q_rope</span><span class="p">(</span><span class="n">query_states</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">key_states</span> <span class="o">=</span> <span class="n">k_rope</span><span class="p">(</span><span class="n">key_states</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// kv cache
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">key_states</span> <span class="o">=</span> <span class="n">k_cache</span><span class="p">(</span><span class="n">key_states</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">value_states</span> <span class="o">=</span> <span class="n">v_cache</span><span class="p">(</span><span class="n">value_states</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// attention weight
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">atten_weight</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">::</span><span class="n">mm</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Chl</span><span class="o">::</span><span class="n">SEQUENCE</span><span class="p">,</span> <span class="n">Chl</span><span class="o">::</span><span class="n">DIMENSION</span><span class="p">))</span> <span class="o">/</span> <span class="n">std</span><span class="o">::</span><span class="n">sqrt</span><span class="p">(</span><span class="n">head_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">atten_weight</span> <span class="o">=</span> <span class="n">mask</span><span class="p">(</span><span class="n">atten_weight</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">atten_weight</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">atten_weight</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// attention output
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">atten_output</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">::</span><span class="n">mm</span><span class="p">(</span><span class="n">atten_weight</span><span class="p">,</span> <span class="n">value_states</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">atten_output</span> <span class="o">=</span> <span class="n">atten_output</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">atten_output</span> <span class="o">=</span> <span class="n">o_proj</span><span class="p">(</span><span class="n">atten_output</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">{</span><span class="n">atten_output</span><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h2 id="55-运行">5.5 运行<a hidden class="anchor" aria-hidden="true" href="#55-运行">#</a></h2>
<p>完整的Qwen模型定义代码可以在附录1中找到。读者可以像Torch一样调用定义好的模型：首先，创建模型：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">QWenConfig</span> <span class="nf">config</span><span class="p">(</span><span class="n">tokens_limit</span><span class="p">,</span> <span class="s">&#34;0.5B&#34;</span><span class="p">,</span> <span class="n">RoPEType</span><span class="o">::</span><span class="n">HFHUBROPE</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="k">auto</span> <span class="n">model</span> <span class="o">=</span> <span class="n">QWenForCausalLM</span><span class="p">(</span><span class="n">config</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">);</span>
</span></span></code></pre></div><p><code>module</code>class重载了()operator，读者可以使用<code>model({input_tensor})</code>来进行推理。</p>
<h1 id="6-mllm框架的不足">6. mllm框架的不足<a hidden class="anchor" aria-hidden="true" href="#6-mllm框架的不足">#</a></h1>
<p>这里写的有点mean，本人专业知识浅薄，在学术上是依托答辩，对mllm的理解更是不到位，大家轻喷。</p>
<h2 id="61-benchmark">6.1 Benchmark<a hidden class="anchor" aria-hidden="true" href="#61-benchmark">#</a></h2>
<ol>
<li>缺少算子的Benchmark</li>
</ol>
<p>本文认为，mllm在实现的时候极力的避免使用第三方的库，因为mllm需要迁移到移动设备上，一些三方库可能不能正常工作。但是手工实现的Kernel还是需要一个Benchmark来和目标平台上提供的算子库来进行性能比较的。就mllm目前提供的MatMul Kernel来看，似乎缺少Pack优化和/micro Kernel的优化？</p>
<ol start="2">
<li>缺少prefill/decode的Benchmark</li>
</ol>
<p>mllm的issues中也有人提到过这个问题。作为具有LLM推理能力的引擎，应当测一下这两个基本能力。</p>
<h2 id="62-对于移动端llm推理的特定优化">6.2 对于移动端LLM推理的特定优化<a hidden class="anchor" aria-hidden="true" href="#62-对于移动端llm推理的特定优化">#</a></h2>
<ol>
<li>KV Cache量化</li>
</ol>
<p>IIRC，在OPPO的Transformer-Lite[2]中，用到了KV Cache量化的小技巧。这对移动设备有限的内存来说可能会更加友好，当然还需要考量量化带来的CPU负载问题。</p>
<ol start="2">
<li>动态形状推理/内存复用/KV Cache搬移优化</li>
</ol>
<p>目前mllm是没有做内存复用的，可以考虑使用符号推理方法来做动态形状的支持进而便于求解下一轮的内存使用情况。或许可以考虑一下PageAttention[3]的Tensor管理方法或者[2]中的KV Cache规划方法来进一步减少内存的搬移。</p>
<ol start="3">
<li>异构算力</li>
</ol>
<p>可以考虑把形状推理（CPU）和计算（GPU/NPU）并行执行起来。或者是6.2.4中提到的内容与计算并行起来。</p>
<ol start="4">
<li>对模型参数的Lazy Fetch和Pre Fetch</li>
</ol>
<p>目前，mllm会把参数一次性的读入内存？考虑到移动设备的内存有限，可以在合适的时机提前从外存上预取而不是全数载入。</p>
<h2 id="63-易用性">6.3 易用性<a hidden class="anchor" aria-hidden="true" href="#63-易用性">#</a></h2>
<ol>
<li>模型结构需要手动编写且无法保存</li>
</ol>
<p>目前，mllm的模型结构还是需要在C++文件中进行显示的手动定义。或许可以考虑创建自己的计算图和算子描述方式，使用flatbuffers来存储计算图。</p>
<ol start="2">
<li>如果要很好的使用所有的算力，可能还是需要完善的计算图机制，这样便于优化分析。</li>
<li>尝试引入三方易用的库如icu等来弥补C++ utf-8处理能力的不足。</li>
</ol>
<hr>
<p><strong>Ref：</strong></p>
<p>[1] mllm, <a href="https://github.com/UbiquitousLearning/mllm">https://github.com/UbiquitousLearning/mllm</a></p>
<p>[2] transformer-lite, <a href="https://arxiv.org/abs/2403.20041">https://arxiv.org/abs/2403.20041</a></p>
<p>[3] PageAttention, <a href="https://arxiv.org/abs/2309.06180">https://arxiv.org/abs/2309.06180</a></p>
<h1 id="a1-qwen模型定义">A1. Qwen模型定义<a hidden class="anchor" aria-hidden="true" href="#a1-qwen模型定义">#</a></h1>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#ifndef MODELING_QWEN_HPP
</span></span></span><span class="line"><span class="cl"><span class="cp">#define MODELING_QWEN_HPP
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;Backend.hpp&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;Layer.hpp&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;Module.hpp&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;Tensor.hpp&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;configuration_qwen.hpp&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;cmath&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="k">using</span> <span class="k">namespace</span> <span class="n">mllm</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Copied from GemmaMLP with Gemma-&gt;Qwen and using silu
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">class</span> <span class="nc">QWenMLP</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Module</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenMLP</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenMLP</span><span class="p">(</span><span class="kt">int</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="kt">int</span> <span class="n">intermediate_size</span><span class="p">,</span> <span class="k">const</span> <span class="n">QWenNameConfig</span> <span class="o">&amp;</span><span class="n">names</span><span class="p">,</span> <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="o">&amp;</span><span class="n">base_name</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">gate_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">intermediate_size</span><span class="p">,</span> <span class="nb">false</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_gate_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">silu</span> <span class="o">=</span> <span class="n">SiLU</span><span class="p">(</span><span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;act&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">up_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">intermediate_size</span><span class="p">,</span> <span class="nb">false</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_up_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">down_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="nb">false</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_down_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">Forward</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">any</span><span class="o">&gt;</span> <span class="n">args</span><span class="p">)</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">x</span> <span class="o">=</span> <span class="n">gate_proj</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">silu</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">y</span> <span class="o">=</span> <span class="n">up_proj</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">down_proj</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">{</span><span class="n">x</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">gate_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">up_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">down_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">silu</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Copied from GemmaAttention with Gemma-&gt;Qwen and using SWA
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">class</span> <span class="nc">QWenAttention</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Module</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenAttention</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenAttention</span><span class="p">(</span><span class="k">const</span> <span class="n">QWenConfig</span> <span class="o">&amp;</span><span class="n">config</span><span class="p">,</span> <span class="k">const</span> <span class="n">QWenNameConfig</span> <span class="o">&amp;</span><span class="n">names</span><span class="p">,</span> <span class="k">const</span> <span class="n">string</span> <span class="o">&amp;</span><span class="n">base_name</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_key_value_heads</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">num_key_value_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_key_value_groups</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">/</span> <span class="n">num_key_value_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// init layers
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">q_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">,</span> <span class="nb">true</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_q_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">,</span> <span class="nb">true</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_k_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">,</span> <span class="nb">true</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_v_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">o_proj</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="nb">false</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_o_proj_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">q_rope</span> <span class="o">=</span> <span class="n">RoPE</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">RoPE_type</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">rope_theta</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">max_position_embeddings</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;q_rope&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_rope</span> <span class="o">=</span> <span class="n">RoPE</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">RoPE_type</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">rope_theta</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">max_position_embeddings</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;k_rope&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_cache</span> <span class="o">=</span> <span class="n">KVCache</span><span class="p">(</span><span class="n">num_key_value_groups</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">cache_limit</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;k_cache&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_cache</span> <span class="o">=</span> <span class="n">KVCache</span><span class="p">(</span><span class="n">num_key_value_groups</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">cache_limit</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;v_cache&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// mask = SlidingWindowMask(config.sliding_window, base_name + &#34;mask&#34;);
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">mask</span> <span class="o">=</span> <span class="n">Causalmask</span><span class="p">(</span><span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;mask&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">softmax</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">DIMENSION</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="s">&#34;softmax&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">Forward</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">any</span><span class="o">&gt;</span> <span class="n">args</span><span class="p">)</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">query_states</span> <span class="o">=</span> <span class="n">q_proj</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">key_states</span> <span class="o">=</span> <span class="n">k_proj</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">value_states</span> <span class="o">=</span> <span class="n">v_proj</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// [batch, heads, sequence, dims]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">query_states</span> <span class="o">=</span> <span class="n">query_states</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// embedding
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">query_states</span> <span class="o">=</span> <span class="n">q_rope</span><span class="p">(</span><span class="n">query_states</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">key_states</span> <span class="o">=</span> <span class="n">k_rope</span><span class="p">(</span><span class="n">key_states</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// kv cache
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">key_states</span> <span class="o">=</span> <span class="n">k_cache</span><span class="p">(</span><span class="n">key_states</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">value_states</span> <span class="o">=</span> <span class="n">v_cache</span><span class="p">(</span><span class="n">value_states</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// attention weight
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">atten_weight</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">::</span><span class="n">mm</span><span class="p">(</span><span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Chl</span><span class="o">::</span><span class="n">SEQUENCE</span><span class="p">,</span> <span class="n">Chl</span><span class="o">::</span><span class="n">DIMENSION</span><span class="p">))</span> <span class="o">/</span> <span class="n">std</span><span class="o">::</span><span class="n">sqrt</span><span class="p">(</span><span class="n">head_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">atten_weight</span> <span class="o">=</span> <span class="n">mask</span><span class="p">(</span><span class="n">atten_weight</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">atten_weight</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">atten_weight</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// attention output
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">atten_output</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">::</span><span class="n">mm</span><span class="p">(</span><span class="n">atten_weight</span><span class="p">,</span> <span class="n">value_states</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">atten_output</span> <span class="o">=</span> <span class="n">atten_output</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_dim</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">atten_output</span> <span class="o">=</span> <span class="n">o_proj</span><span class="p">(</span><span class="n">atten_output</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">{</span><span class="n">atten_output</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">hidden_size</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">num_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">head_dim</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">num_key_value_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">num_key_value_groups</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">q_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">k_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">v_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">o_proj</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">q_rope</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">k_rope</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">k_cache</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">v_cache</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">mask</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">softmax</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Copied from GemmaDecoder with Gemma-&gt;Qwen and set RmsNorm(without add_unit_offset)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">class</span> <span class="nc">QWenDecoder</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Module</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenDecoder</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenDecoder</span><span class="p">(</span><span class="k">const</span> <span class="n">QWenConfig</span> <span class="o">&amp;</span><span class="n">config</span><span class="p">,</span> <span class="k">const</span> <span class="n">QWenNameConfig</span> <span class="o">&amp;</span><span class="n">names</span><span class="p">,</span> <span class="k">const</span> <span class="n">string</span> <span class="o">&amp;</span><span class="n">base_name</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">self_atten</span> <span class="o">=</span> <span class="n">QWenAttention</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_attn_base_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">mlp</span> <span class="o">=</span> <span class="n">QWenMLP</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_ffn_base_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">input_layernorm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">rms_norm_eps</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_attn_norm_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">post_attention_layernorm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">rms_norm_eps</span><span class="p">,</span> <span class="n">base_name</span> <span class="o">+</span> <span class="n">names</span><span class="p">.</span><span class="n">_ffn_norm_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">Forward</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">any</span><span class="o">&gt;</span> <span class="n">args</span><span class="p">)</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">x</span> <span class="o">=</span> <span class="n">input_layernorm</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">self_atten</span><span class="p">({</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">})[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">tmp</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">post_attention_layernorm</span><span class="p">(</span><span class="n">tmp</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">({</span><span class="n">x</span><span class="p">})[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">tmp</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">{</span><span class="n">x</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenAttention</span> <span class="n">self_atten</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenMLP</span> <span class="n">mlp</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">input_layernorm</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">post_attention_layernorm</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Copied from GemmaModel with Gemma-&gt;Qwen and set RmsNorm(without add_unit_offset)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">class</span> <span class="nc">QWenModel</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Module</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenModel</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenModel</span><span class="p">(</span><span class="k">const</span> <span class="n">QWenConfig</span> <span class="o">&amp;</span><span class="n">config</span><span class="p">,</span> <span class="k">const</span> <span class="n">QWenNameConfig</span> <span class="o">&amp;</span><span class="n">names</span><span class="p">,</span> <span class="k">const</span> <span class="n">string</span> <span class="o">&amp;</span><span class="n">base_name</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">blocks</span> <span class="o">=</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">QWenDecoder</span><span class="o">&gt;</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">num_hidden_layers</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">base_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">rms_norm_eps</span><span class="p">,</span> <span class="n">names</span><span class="p">.</span><span class="n">post_norm_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">Forward</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">any</span><span class="o">&gt;</span> <span class="n">args</span><span class="p">)</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="o">&amp;</span><span class="nl">block</span> <span class="p">:</span> <span class="n">blocks</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">block</span><span class="p">({</span><span class="n">x</span><span class="p">})[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">{</span><span class="n">x</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">QWenDecoder</span><span class="o">&gt;</span> <span class="n">blocks</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">norm</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">QWenForCausalLM</span> <span class="k">final</span> <span class="o">:</span> <span class="k">public</span> <span class="n">Module</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenForCausalLM</span><span class="p">(</span><span class="n">QWenConfig</span> <span class="o">&amp;</span><span class="n">config</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">names</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">names_config</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">tie_embedding_words</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">tie_embedding_words</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">embedding</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">names</span><span class="p">.</span><span class="n">token_embd_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span> <span class="o">=</span> <span class="n">QWenModel</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">names</span><span class="p">.</span><span class="n">blk_name</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// FIXME Qwen-0.5 use tied embedding
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// Others use nn.Linear()
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="n">tie_embedding_words</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">lm_head</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">names</span><span class="p">.</span><span class="n">token_embd_name</span> <span class="o">+</span> <span class="s">&#34;.weight&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">Forward</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">any</span><span class="o">&gt;</span> <span class="n">args</span><span class="p">)</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">x</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// go through model
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">({</span><span class="n">x</span><span class="p">})[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">tie_embedding_words</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">outputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">::</span><span class="n">mm</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">lm_head</span><span class="p">().</span><span class="n">transpose</span><span class="p">(</span><span class="n">Chl</span><span class="o">::</span><span class="n">SEQUENCE</span><span class="p">,</span> <span class="n">Chl</span><span class="o">::</span><span class="n">DIMENSION</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">{</span><span class="n">outputs</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">hidden_size</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">bool</span> <span class="n">tie_embedding_words</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Layer</span> <span class="n">embedding</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Parameter</span> <span class="n">lm_head</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">QWenModel</span> <span class="n">model</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#endif </span><span class="c1">//! MODELING_QWEN_HPP
</span></span></span></code></pre></div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://chenghuawang.github.io/keep-moving-forward/tags/llm-server/">LLM Server</a></li>
      <li><a href="https://chenghuawang.github.io/keep-moving-forward/tags/llm/">LLM</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://chenghuawang.github.io/keep-moving-forward/tech/fundamental_from_online_softmax_to_flash_attentionv3/">
    <span class="title">« Prev</span>
    <br>
    <span>[Fundamental] From Online Softmax to Flash Attention V3</span>
  </a>
  <a class="next" href="https://chenghuawang.github.io/keep-moving-forward/papers/mlsys2024-qmoe/">
    <span class="title">Next »</span>
    <br>
    <span>✅[Oct 2023] QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share mllm框架浅析(一)-以QWen0.5B为例 on x"
            href="https://x.com/intent/tweet/?text=mllm%e6%a1%86%e6%9e%b6%e6%b5%85%e6%9e%90%28%e4%b8%80%29-%e4%bb%a5QWen0.5B%e4%b8%ba%e4%be%8b&amp;url=https%3a%2f%2fchenghuawang.github.io%2fkeep-moving-forward%2ftech%2fmllm-qwen%2f&amp;hashtags=LLMServer%2cLLM">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share mllm框架浅析(一)-以QWen0.5B为例 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fchenghuawang.github.io%2fkeep-moving-forward%2ftech%2fmllm-qwen%2f&amp;title=mllm%e6%a1%86%e6%9e%b6%e6%b5%85%e6%9e%90%28%e4%b8%80%29-%e4%bb%a5QWen0.5B%e4%b8%ba%e4%be%8b&amp;summary=mllm%e6%a1%86%e6%9e%b6%e6%b5%85%e6%9e%90%28%e4%b8%80%29-%e4%bb%a5QWen0.5B%e4%b8%ba%e4%be%8b&amp;source=https%3a%2f%2fchenghuawang.github.io%2fkeep-moving-forward%2ftech%2fmllm-qwen%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share mllm框架浅析(一)-以QWen0.5B为例 on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fchenghuawang.github.io%2fkeep-moving-forward%2ftech%2fmllm-qwen%2f&title=mllm%e6%a1%86%e6%9e%b6%e6%b5%85%e6%9e%90%28%e4%b8%80%29-%e4%bb%a5QWen0.5B%e4%b8%ba%e4%be%8b">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share mllm框架浅析(一)-以QWen0.5B为例 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fchenghuawang.github.io%2fkeep-moving-forward%2ftech%2fmllm-qwen%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share mllm框架浅析(一)-以QWen0.5B为例 on whatsapp"
            href="https://api.whatsapp.com/send?text=mllm%e6%a1%86%e6%9e%b6%e6%b5%85%e6%9e%90%28%e4%b8%80%29-%e4%bb%a5QWen0.5B%e4%b8%ba%e4%be%8b%20-%20https%3a%2f%2fchenghuawang.github.io%2fkeep-moving-forward%2ftech%2fmllm-qwen%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share mllm框架浅析(一)-以QWen0.5B为例 on telegram"
            href="https://telegram.me/share/url?text=mllm%e6%a1%86%e6%9e%b6%e6%b5%85%e6%9e%90%28%e4%b8%80%29-%e4%bb%a5QWen0.5B%e4%b8%ba%e4%be%8b&amp;url=https%3a%2f%2fchenghuawang.github.io%2fkeep-moving-forward%2ftech%2fmllm-qwen%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share mllm框架浅析(一)-以QWen0.5B为例 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=mllm%e6%a1%86%e6%9e%b6%e6%b5%85%e6%9e%90%28%e4%b8%80%29-%e4%bb%a5QWen0.5B%e4%b8%ba%e4%be%8b&u=https%3a%2f%2fchenghuawang.github.io%2fkeep-moving-forward%2ftech%2fmllm-qwen%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>© <a href="https://github.com/chenghuaWang">chenghua.wang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a><script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js" integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<script>
const images = Array.from(document.querySelectorAll(".post-content img"));
images.forEach(img => {
  mediumZoom(img, {
    margin: 0,  
    scrollOffset: 40,  
    container: null,  
    template: null,  
    background: 'rgba(0, 0, 0, 0.8)'
  });
});
</script>
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5j20jf9ml5x&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>


<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
